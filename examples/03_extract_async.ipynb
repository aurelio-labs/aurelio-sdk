{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aurelio_sdk import AsyncAurelioClient\n",
    "import os\n",
    "\n",
    "client = AsyncAurelioClient(api_key=os.environ[\"AURELIO_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract text from PDFs and video MP4 files and urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtractResponse(status=<TaskStatus.completed: 'completed'>, usage=Usage(tokens=23071, pages=25, seconds=None), message=None, processing_options=ExtractProcessingOptions(chunk=True, quality=<ProcessingQuality.low: 'low'>), document=ResponseDocument(id='doc_65e7b298-0fe1-4f63-b444-05de227e1025', content=\"3 2 0 2\\nt c O 8 1\\nG L . s c [\\n2 v 5 3 4 9 1 . 5 0 3 2 : v i X r a\\nAdANNS: A Framework for Adaptive Semantic Search\\nAniket Rege    Aditya Kusupati    Sharan Ranjit S  Alan Fan  Qingqing Cao , Sham Kakade  Prateek Jain  Ali Farhadi   University of Washington,  Google Research,  Harvard University {kusupati,ali}@cs.washington.edu, prajain@google.com\\nAbstract\\nWeb-scale search systems learn an encoder to embed a given query which is then hooked into an approximate nearest neighbor search (ANNS) pipeline to retrieve similar data points. To accurately capture tail queries and data points, learned representations typically are rigid, high-dimensional vectors that are generally used as-is in the entire ANNS pipeline and can lead to computationally expensive retrieval. In this paper, we argue that instead of rigid representations, different stages of ANNS can leverage adaptive representations of varying capacities to achieve significantly better accuracy-compute trade-offs, i.e., stages of ANNS that can get away with more approximate computation should use a lower-capacity representation of the same data point. To this end, we introduce AdANNS , a novel ANNS design framework that explicitly leverages the flexibility of Ma- tryoshka Representations [31]. We demonstrate state-of-the-art accuracy-compute trade-offs using novel AdANNS-based key ANNS building blocks like search data structures (AdANNS-IVF) and quantization (AdANNS-OPQ). For exam- ple on ImageNet retrieval, AdANNS-IVF is up to 1.5% more accurate than the rigid representations-based IVF [48] at the same compute budget; and matches accuracy while being up to 90  faster in wall-clock time. For Natural Questions, 32-byte AdANNS-OPQ matches the accuracy of the 64-byte OPQ baseline [13] constructed using rigid representations   same accuracy at half the cost! We further show that the gains from AdANNS translate to modern-day composite ANNS indices that combine search structures and quantization. Finally, we demonstrate that AdANNS can enable inference-time adaptivity for compute-aware search on ANNS indices built non-adaptively on matryoshka representations. Code is open-sourced at https://github.com/RAIVNLab/AdANNS.\\nIntroduction\\nSemantic search [24] on learned representations [40, 41, 50] is a major component in retrieval pipelines [4, 9]. In its simplest form, semantic search methods learn a neural network to embed queries as well as a large number (N ) of data points in a d-dimensional vector space. For a given query, the nearest (in embedding space) point is retrieved using either an exact search or using approximate nearest neighbor search (ANNS) [21] which is now indispensable for real-time large-scale retrieval.\\nExisting semantic search methods learn fixed or rigid representations (RRs) which are used as is in all the stages of ANNS (data structures for data pruning and quantization for cheaper distance computation; see Section 2). That is, while ANNS indices allow a variety of parameters for searching the design space to optimize the accuracy-compute trade-off, the provided data dimensionality is typically assumed to be an immutable parameter. To make it concrete, let us consider inverted file index (IVF) [48], a popular web-scale ANNS technique [16]. IVF has two stages (Section 3) during inference: (a) cluster mapping: mapping the query to a cluster of data points [36], and (b) linear\\n Equal contribution.\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\\n        \\n     6 H D U F K   / D W H Q F \\\\   4 X H U \\\\     P V \\n 5 L J L G   , 9 )\\n a           J D L Q a        U H D O   Z R U O G   V S H H G   X S\\n         7 R S       $ F F X U D F \\\\       \\n $ G $ 1 1 6   , 9 )\\n        \\n      \\n        \\n        \\n    \\n    \\n     7 R S       $ F F X U D F \\\\       \\n    \\n 5 L J L G   2 3 4\\n $ G $ 1 1 6   2 3 4\\n    \\n    \\n    \\n a       J D L Q      F K H D S H U\\n     & R P S X W H   % X G J H W     % \\\\ W H V \\n(a) Image retrieval on ImageNet-1K.\\n(b) Passage retrieval on Natural Questions.\\nFigure 1: AdANNS helps design search data structures and quantization methods with better accuracy-compute trade-offs than the existing solutions. In particular, (a) AdANNS-IVF improves on standard IVF by up to 1.5% in accuracy while being 90  faster in deployment and (b) AdANNS-OPQ is as accurate as the baseline at half the cost! Rigid-IVF and Rigid-OPQ are standard techniques that are built on rigid representations (RRs) while AdANNS uses matryoshka representations (MRs) [31].\\nscan: distance computation w.r.t all points in the retrieved cluster to find the nearest neighbor (NN). Standard IVF utilizes the same high-dimensional RR for both phases, which can be sub-optimal.\\nWhy the sub-optimality? Imagine one needs to partition a dataset into k clusters for IVF and the dimensionality of the data is d   IVF uses full d representation to partition into k clusters. However, suppose we have an alternate approach that somehow projects the data in d/2 dimensions and learns 2k clusters. Note that the storage and computation to find the nearest cluster remains the same in both cases, i.e., when we have k clusters of d dimensions or 2k clusters of d/2 dimensions. 2k clusters can provide significantly more refined partitioning, but the distances computed between queries and clusters could be significantly more inaccurate after projection to d/2 dimensions.\\nSo, if we can find a mechanism to obtain a d/2-dimensional representation of points that can accurately approximate the topology/distances of d-dimensional representation, then we can potentially build significantly better ANNS structure that utilizes different capacity representations for the cluster mapping and linear scan phases of IVF. But how do we find such adaptive representations? These desired adaptive representations should be cheap to obtain and still ensure distance preservation across dimensionality. Post-hoc dimensionality reduction techniques like SVD [14] and random projections [25] on high-dimensional RRs are potential candidates, but our experiments indicate that in practice they are highly inaccurate and do not preserve distances well enough (Figure 2).\\nInstead, we identify that the recently proposed Matryoshka Representations (MRs) [31] satisfy the specifications for adaptive representations. Matryoshka representations pack information in a hierarchical nested manner, i.e., the first m-dimensions of the d-dimensional MR form an accurate low-dimensional representation while being aware of the information in the higher dimensions. This allows us to deploy MRs in two major and novel ways as part of ANNS: (a) low-dimensional representations for accuracy-compute optimal clustering and quantization, and (b) high-dimensional representations for precise re-ranking when feasible.\\nTo this effort, we introduce AdANNS , a novel design framework for semantic search that uses matryoshka representation-based adaptive representations across different stages of ANNS to ensure significantly better accuracy-compute trade-off than the state-of-the-art baselines.\\nTypical ANNS systems have two key components: (a) search data structure to store datapoints, (b) distance computation to map a given query to points in the data structure. Through AdANNS, we address both these components and significantly improve their performance. In particular, we first propose AdANNS-IVF (Section 4.1) which tackles the first component of ANNS systems. AdANNS- IVF uses standard full-precision computations but uses adaptive representations for different IVF stages. On ImageNet 1-NN image retrieval (Figure 1a), AdANNS-IVF is up to 1.5% more accurate for the compute budget and 90  cheaper in deployment for the same accuracy as IVF.\\nWe then propose AdANNS-OPQ (Section 4.2) which addresses the second component by using AdANNS-based quantization (OPQ [13])   here we use exhaustive search overall points. AdANNS- OPQ is as accurate as the baseline OPQ on RRs while being at least 2  faster on Natural Ques- tions [32] 1-NN passage retrieval (Figure 1b). Finally, we combine the two techniques to obtain AdANNS-IVFOPQ (Section 4.3) which is more accurate while being much cheaper   up to 8    than the traditional IVFOPQ [24] index. To demonstrate generality of our technique, we adapt AdANNS to DiskANN [22] which provides interesting accuracy-compute tradeoff; see Table 1.\\nWhile MR already has multi-granular representations, careful integration with ANNS building blocks is critical to obtain a practical method and is our main contribution. In fact, Kusupati et al. [31] proposed a simple adaptive retrieval setup that uses smaller-dimensional MR for shortlisting in re- trieval followed by precise re-ranking with a higher-dimensional MR. Such techniques, unfortunately, cannot be scaled to industrial systems as they require forming a new index for every shortlisting provided by low-dimensional MR. Ensuring that the method aligns well with the modern-day ANNS pipelines is important as they already have mechanisms to handle real-world constraints like load-balancing [16] and random access from disk [22]. So, AdANNS is a step towards making the abstraction of adaptive search and retrieval feasible at the web-scale.\\nThrough extensive experimentation, we also show that AdANNS generalizes across search data structures, distance approximations, modalities (text & image), and encoders (CNNs & Transformers) while still translating the theoretical gains to latency reductions in deployment. While we have mainly focused on IVF and OPQ-based ANNS in this work, AdANNS also blends well with other ANNS pipelines. We also show that AdANNS can enable compute-aware elastic search on prebuilt indices without making any modifications (Section 5.1); note that this is in contrast to AdANNS-IVF that builds the index explicitly utilizing  adaptivity  in representations. Finally, we provide an extensive analysis on the alignment of matryoshka representation for better semantic search (Section 5.2).\\nWe make the following key contributions:\\nWe introduce AdANNS\\n, a novel framework for semantic search that leverages matryoshka\\nrepresentations for designing ANNS systems with better accuracy-compute trade-offs.\\nAdANNS powered search data structure (AdANNS-IVF) and quantization (AdANNS-OPQ) show a significant improvement in accuracy-compute tradeoff compared to existing solutions.\\nAdANNS generalizes to modern-day composite ANNS indices and can also enable compute-aware elastic search during inference with no modifications.\\n2 Related Work\\nApproximate nearest neighbour search (ANNS) is a paradigm to come as close as possible [7] to retrieving the  true  nearest neighbor (NN) without the exorbitant search costs associated with exhaustive search [21, 52]. The  approximate  nature comes from data pruning as well as the cheaper distance computation that enable real-time web-scale search. In its naive form, NN-search has a complexity of O(dN ); d is the data dimensionality used for distance computation and N is the size of the database. ANNS employs each of these approximations to reduce the linear dependence on the dimensionality (cheaper distance computation) and data points visited during search (data pruning).\\nCheaper distance computation. From a bird s eye view, cheaper distance computation is always obtained through dimensionality reduction (quantization included). PCA and SVD [14, 26] can reduce dimensionality and preserve distances only to a limited extent without sacrificing accuracy. On the other hand, quantization-based techniques [6, 15] like (optimized) product quantization ((O)PQ) [13, 23] have proved extremely crucial for relatively accurate yet cheap distance computation and simultaneously reduce the memory overhead significantly. Another naive solution is to indepen- dently train the representation function with varying low-dimensional information bottlenecks [31] which is rarely used due to the costs of maintaining multiple models and databases.\\nData pruning. Enabled by various data structures, data pruning reduces the number of data points visited as part of the search. This is often achieved through hashing [8, 46], trees [3, 12, 16, 48] and graphs [22, 38]. More recently there have been efforts towards end-to-end learning of the search data structures [17, 29, 30]. However, web-scale ANNS indices are often constructed on rigid d-dimensional real vectors using the aforementioned data structures that assist with the real-time search. For a more comprehensive review of ANNS structures please refer to [5, 34, 51].\\nComposite indices. ANNS pipelines often benefit from the complementary nature of various building blocks [24, 42]. In practice, often the data structures (coarse-quantizer) like IVF [48] and HNSW [37] are combined with cheaper distance alternatives like PQ [23] (fine-quantizer) for massive speed-ups in web-scale search. While the data structures are built on d-dimensional real vectors, past works consistently show that PQ can be safely used for distance computation during search time. As evident in modern web-scale ANNS systems like DiskANN [22], the data structures are built on d-dimensional real vectors but work with PQ vectors (32   64-byte) for fast distance computations.\\nANNS benchmark datasets. Despite the Herculean advances in representation learning [19, 42], ANNS progress is often only benchmarked on fixed representation vectors provided for about a dozen million to billion scale datasets [1, 47] with limited access to the raw data. This resulted in the improvement of algorithmic design for rigid representations (RRs) that are often not specifically designed for search. All the existing ANNS methods work with the assumption of using the provided d-dimensional representation which might not be Pareto-optimal for the accuracy-compute trade- off in the first place. Note that the lack of raw-image and text-based benchmarks led us to using ImageNet-1K [45] (1.3M images, 50K queries) and Natural Questions [32] (21M passages, 3.6K queries) for experimentation. While not billion-scale, the results observed on ImageNet often translate to real-world progress [28], and Natural Questions is one of the largest question answering datasets benchmarked for dense passage retrieval [27], making our results generalizable and widely applicable.\\nIn this paper, we investigate the utility of adaptive representations   embeddings of different dimen- sionalities having similar semantic information   in improving the design of ANNS algorithms. This helps in transitioning out of restricted construction and inference on rigid representations for ANNS. To this end, we extensively use Matryoshka Representations (MRs) [31] which have desired adaptive properties in-built. To the best of our knowledge, this is the first work that improves accuracy-compute trade-off in ANNS by leveraging adaptive representations on different phases of construction and inference for ANNS data structures.\\n3 Problem Setup, Notation, and Preliminaries\\nThe problem setup of approximate nearest neighbor search (ANNS) [21] consists of a database of N data points, [x1, x2, . . . , xN ], and a query, q, where the goal is to  approximately  retrieve the nearest data point to the query. Both the database and query are embedded to Rd using a representation function   : X   Rd, often a neural network that can be learned through various representation learning paradigms [2, 19, 20, 40, 42].\\nMatryoshka Representations (MRs). The d-dimensional representations from   can have a nested structure like Matryoshka Representations (MRs) [31] in-built    MR(d). Matryoshka Representation Learning (MRL) learns these nested representations with a simple strategy of optimizing the same training objective at varying dimensionalities. These granularities are ordered such that the lowest representation size forms a prefix for the higher-dimensional representations. So, high-dimensional MR inherently contains low-dimensional representations of varying granularities that can be accessed for free   first m-dimensions (m   [d]) ie.,  MR(d)[1 : m] from the d-dimensional MR form an m-dimensional representation which is as accurate as its independently trained rigid representation (RR) counterpart    RR(m). Training an encoder with MRL does not involve any overhead or hyperparameter tuning and works seamlessly across modalities, training objectives, and architectures.\\nInverted File Index (IVF). IVF [48] is an ANNS data structure used in web-scale search sys- tems [16] owing to its simplicity, minimal compute overhead, and high accuracy. IVF construction involves clustering (coarse quantization through k-means) [36] on d-dimensional representation that results in an inverted file list [53] of all the data points in each cluster. During search, d-dimensional query representation is assigned to the most relevant cluster (Ci; i   [k]) by finding the closest cen- troid ( i) using an appropriate distance metric (L2 or cosine). This is followed by an exhaustive linear search across all data points in the cluster which gives the closest NN (see Figure 5 in Appendix A for IVF overview). Lastly, IVF can scale to web-scale by utilizing a hierarchical IVF structure within each cluster [16]. Table 2 in Appendix A describes the retrieval formula for multiple variants of IVF.\\nOptimized Product Quantization (OPQ). Product Quantization (PQ) [23] works by splitting a d-dimensional real vector into m sub-vectors and quantizing each sub-vector with an independent 2b\\nlength codebook across the database. After PQ, each d-dimensional vector can be represented by a compact m   b bit vector; we make each vector m bytes long by fixing b = 8. During search time, distance computation between the query vector and PQ database is extremely efficient with only m codebook lookups. The generality of PQ encompasses scalar/vector quantization [15, 36] as special cases. However, PQ can be further improved by rotating the d-dimensional space appropriately to maximize distance preservation after PQ. Optimized Product Quantization (OPQ) [13] achieves this by learning an orthonormal projection matrix R that rotates the d-dimensional space to be more amenable to PQ. OPQ shows consistent gains over PQ across a variety of ANNS tasks and has become the default choice in standard composite indices [22, 24].\\nDatasets. We evaluate the ANNS algorithms while changing the representations used for the search thus making it impossible to evaluate on the usual benchmarks [1]. Hence we experiment with two public datasets: (a) ImageNet-1K [45] dataset on the task of image retrieval   where the goal is to retrieve images from a database (1.3M image train set) belonging to the same class as the query image (50K image validation set) and (b) Natural Questions (NQ) [32] dataset on the task of question answering through dense passage retrieval   where the goal is to retrieve the relevant passage from a database (21M Wikipedia passages) for a query (3.6K questions).\\nMetrics Performance of ANNS is often measured using recall score [22], k-recall@N   recall of the exact NN across search complexities which denotes the recall of k  true  NN when N data points are retrieved. However, the presence of labels allows us to compute 1-NN (top-1) accuracy. Top-1 accuracy is a harder and more fine-grained metric that correlates well with typical retrieval metrics like recall and mean average precision (mAP@k). Even though we report top-1 accuracy by default during experimentation, we discuss other metrics in Appendix C. Finally, we measure the compute overhead of ANNS using MFLOPS/query and also provide wall-clock times (see Appendix B.1).\\nEncoders. For ImageNet, we encode both the database and query set using a ResNet50 ( I ) [19] trained on ImageNet-1K. For NQ, we encode both the passages in the database and the questions in the query set using a BERT-Base ( N ) [10] model fine-tuned on NQ for dense passage retrieval [27].\\nWe use the trained ResNet50 models with varying representation sizes (d = [8, 16, . . . , 2048]; default being 2048) as suggested by Kusupati et al. [31] alongside the MRL-ResNet50 models trained with MRL for the same dimensionalities. The RR and MR models are trained to ensure the supervised one-vs-all classification accuracy across all data dimensionalities is nearly the same   1-NN accuracy of 2048-d RR and MR models are 71.19% and 70.97% respectively on ImageNet-1K. Independently trained models,  RR(d) , output d = [8, 16 . . . , 2048] dimensional RRs while a single MRL-ResNet50 model,  MR(d) We also train BERT-Base models in a similar vein as the aforementioned ResNet50 models. The key difference is that we take a pre-trained BERT-Base model and fine-tune on NQ as suggested by Karpukhin et al. [27] with varying (5) representation sizes (bottlenecks) (d = [48, 96, . . . , 768]; default being 768) to obtain  RR(d) that creates RRs for the NQ dataset. To get the MRL-BERT- Base model, we fine-tune a pre-trained BERT-Base encoder on the NQ train dataset using the MRL objective with the same granularities as RRs to obtain  MR(d) which contains all five granularities. Akin to ResNet50 models, the RR and MR BERT-Base models on NQ are built to have similar 1-NN accuracy for 768-d of 52.2% and 51.5% respectively. More implementation details can be found in Appendix B and additional experiment-specific information is provided at the appropriate places.\\n, outputs a d = 2048-dimensional MR that contains all the 9 granularities.\\n4 AdANNS   Adaptive ANNS\\nIn this section, we present our proposed AdANNS framework that exploits the inherent flexibility of matryoshka representations to improve the accuracy-compute trade-off for semantic search com- ponents. Standard ANNS pipeline can be split into two key components: (a) search data structure that indexes and stores data points, (b) query-point computation method that outputs (approximate) distance between a given query and data point. For example, standard IVFOPQ [24] method uses an IVF structure to index points on full-precision vectors and then relies on OPQ for more efficient distance computation between the query and the data points during the linear scan.\\nBelow, we show that AdANNS can be applied to both the above-mentioned ANNS components and provides significant gains on the computation-accuracy tradeoff curve. In particular, we present AdANNS-IVF which is AdANNS version of the standard IVF index structure [48], and the closely related ScaNN structure [16]. We also present AdANNS-OPQ which introduces representation adap- tivity in the OPQ, an industry-default quantization. Then, in Section 4.3 we further demonstrate the combination of the two techniques to get AdANNS-IVFOPQ   an AdANNS version of IVFOPQ [24]   and AdANNS-DiskANN, a similar variant of DiskANN [22]. Overall, our experiments show that AdANNS-IVF is significantly more accuracy-compute optimal compared to the IVF indices built on RRs and AdANNS-OPQ is as accurate as the OPQ on RRs while being significantly cheaper.\\n4.1 AdANNS-IVF\\n         7 R S       $ F F X U D F \\\\       \\n , 9 )   0 5\\n        \\n $ G $ 1 1 6   , 9 )\\n        \\n   0 ) / 2 3 6   4 X H U \\\\\\n 0 *   , 9 )   5 5\\n      \\n $ G $ 1 1 6   , 9 )   '\\n      \\n        \\n , 9 )   5 5\\n 0 *   , 9 )   6 9 '\\nRecall from Section 1 that IVF has a clustering and a linear scan phase, where both phase use same dimen- sional rigid representation. Now, AdANNS-IVF allows the clustering phase to use the first dc dimensions of the given matryoshka represen- tation (MR). Similarly, the linear scan within each cluster uses ds di- mensions, where again ds represents top ds coordinates from MR. Note that setting dc = ds results in non- adaptive regular IVF. Intuitively, we would set dc   ds, so that instead of clustering with a high-dimensional representation, we can approximate it accurately with a low-dimensional em- bedding of size dc followed by a lin- ear scan with a higher ds-dimensional representation. Intuitively, this helps in the smooth search of design space for state-of-the-art accuracy-compute trade-off. Furthermore, this can provide a precise operating point on accuracy-compute tradeoff curve which is critical in several practical settings.\\nFigure 2: 1-NN accuracy on ImageNet retrieval shows that AdANNS-IVF achieves near-optimal accuracy-compute trade-off compared across various rigid and adaptive base- lines. Both adaptive variants of MR and RR significantly outperform their rigid counterparts (IVF-XX) while post-hoc compression on RR using SVD for adaptivity falls short.\\nOur experiments on regular IVF with MRs and RRs (IVF-MR & IVF-RR) of varying dimensionali- ties and IVF configurations (# clusters, # probes) show that (Figure 2) matryoshka representations result in a significantly better accuracy-compute trade-off. We further studied and found that learned lower-dimensional representations offer better accuracy-compute trade-offs for IVF than higher- dimensional embeddings (see Appendix E for more results).\\nAdANNS utilizes d-dimensional matryoshka representation to get accurate dc and ds dimensional vectors at no extra compute cost. The resulting AdANNS-IVF provides a much better accuracy- compute trade-off (Figure 2) on ImageNet-1K retrieval compared to IVF-MR, IVF-RR, and MG- IVF-RR   multi-granular IVF with rigid representations (akin to AdANNS without MR)   a strong baseline that uses dc and ds dimensional RRs. Finally, we exhaustively search the design space of IVF by varying dc, ds   [8, 16, . . . , 2048] and the number of clusters k   [8, 16, . . . , 2048]. Please see Appendix E for more details. For IVF experiments on the NQ dataset, please refer to Appendix G.\\nEmpirical results. Figure 2 shows that AdANNS-IVF outperforms the baselines across all accuracy-compute settings for ImageNet-1K retrieval. AdANNS-IVF results in 10  lower compute for the best accuracy of the extremely expensive MG-IVF-RR and non-adaptive IVF-MR. Specifi- cally, as shown in Figure 1a, AdANNS-IVF is up to 1.5% more accurate for the same compute and has up to 100  lesser FLOPS/query (90  real-world speed-up!) than the status quo ANNS on rigid representations (IVF-RR). We filter out points for the sake of presentation and encourage the reader to check out Figure 8 in Appendix E for an expansive plot of all the configurations searched.\\nThe advantage of AdANNS for construction of search structures is evident from the improvements in IVF (AdANNS-IVF) and can be easily extended to other ANNS structures like ScaNN [16] and\\nHNSW [38]. For example, HNSW consists of multiple layers with graphs of NSW graphs [37] of increasing complexity. AdANNS can be adopted to HNSW, where the construction of each level can be powered by appropriate dimensionalities for an optimal accuracy-compute trade-off. In general, AdANNS provides fine-grained control over compute overhead (storage, working memory, inference, and construction cost) during construction and inference while providing the best possible accuracy.\\n4.2 AdANNS-OPQ\\nStandard Product Quantization (PQ) essentially performs block-wise vector quantization via cluster- ing. For example, suppose we need 32-byte PQ compressed vectors from the given 2048 dimensional representations. Then, we can chunk the representations in m = 32 equal blocks/sub-vectors of 64-d each, and each sub-vector space is clustered into 28 = 256 partitions. That is, the representation of each point is essentially cluster-id for each block. Optimized PQ (OPQ) [13] further refines this idea, by first rotating the representations using a learned orthogonal matrix, and then applying PQ on top of the rotated representations. In ANNS, OPQ is used extensively to compress vectors and improves approximate distance computation primarily due to significantly lower memory overhead than storing full-precision data points IVF.\\nAdANNS-OPQ utilizes MR representations to apply OPQ on lower-dimensional representations. That is, for a given quantization budget, AdANNS allows using top ds   d dimensions from MR and then computing clusters with ds/m-dimensional blocks where m is the number of blocks. Depending on ds and m, we have further flexibility of trading-off dimensionality/capacity for increasing the number of clusters to meet the given quantization budget. AdANNS-OPQ tries multiple ds, m, and number of clusters for a fixed quantization budget to obtain the best performing configuration.\\nWe experimented with 8   128 byte OPQ budgets for both ImageNet and Natural Questions retrieval with an exhaustive search on the quantized vectors. We compare AdANNS-OPQ which uses MRs of varying granularities to the baseline OPQ built on the highest dimensional RRs. We also evaluate OPQ vectors obtained projection using SVD [14] on top of the highest-dimensional RRs.\\nEmpirical results. Figures 3 and 1b show that AdANNS-OPQ significantly outperforms   up to 4% accuracy gain   the baselines (OPQ on RRs) across compute budgets on both ImageNet and NQ. In particular, AdANNS-OPQ tends to match the accuracy of a 64-byte (a typical choice in ANNS) OPQ baseline with only a 32-byte budget. This results in a 2  reduction in both storage and compute FLOPS which translates to significant gains in real-world web-scale deployment (see Appendix D).\\nWe only report the best AdANNS-OPQ for each budget typically obtained through a much lower- dimensional MR (128 & 192; much faster to build as well) than the highest-dimensional MR (2048 & 768) for ImageNet and NQ respectively (see Appendix G for more details). At the same time, we\\n 2 3 4   5 5\\n    \\n     & R P S X W H   % X G J H W     % \\\\ W H V \\n     7 R S       $ F F X U D F \\\\       \\n    \\n    \\n    \\n    \\n 2 3 4   5 5   6 9 '\\n   \\n $ G $ 1 1 6   2 3 4\\n   \\n    \\n    \\n    \\n    \\n    \\n     7 R S       $ F F X U D F \\\\       \\n 5 L J L G   , 9 ) 2 3 4\\n $ G $ 1 1 6   , 9 ) 2 3 4\\n    \\n     & R P S X W H   % X G J H W     % \\\\ W H V \\nFigure 3: AdANNS-OPQ matches the accuracy of 64-byte OPQ on RR using only 32-bytes for ImageNet retrieval. AdANNS provides large gains at lower compute budgets and saturates to baseline performance for larger budgets.\\nFigure 4: Combining the gains of AdANNS for IVF and OPQ leads to better IVFOPQ compos- ite indices. On ImageNet retrieval, AdANNS- IVFOPQ is 8  cheaper for the same accuracy and provides 1 - 4% gains over IVFOPQ on RRs.\\nnote that building compressed OPQ vectors on projected RRs using SVD to the smaller dimensions (or using low-dimensional RRs, see Appendix D) as the optimal AdANNS-OPQ does not help in improving the accuracy. The significant gains we observe in AdANNS-OPQ are purely due to better information packing in MRs   we hypothesize that packing the most important information in the initial coordinates results in a better PQ quantization than RRs where the information is uniformly distributed across all the dimensions [31, 49]. See Appendix D for more details and experiments.\\n4.3 AdANNS for Composite Indices\\nWe now extend AdANNS to composite indices [24] which put together two main ANNS building blocks   search structures and quantization   together to obtain efficient web-scale ANNS indices used in practice. A simple instantiation of a composite index would be the combination of IVF and OPQ   IVFOPQ   where the clustering in IVF happens with full-precision real vectors but the linear scan within each cluster is approximated using OPQ-compressed variants of the representation   since often the full-precision vectors of the database cannot fit in RAM. Contemporary ANNS indices like DiskANN [22] make this a default choice where they build the search graph with a full-precision vector and approximate the distance computations during search with an OPQ-compressed vector to obtain a very small shortlist of retrieved datapoints. In DiskANN, the shortlist of data points is then re-ranked to form the final list using their full-precision vectors fetched from the disk. AdANNS is naturally suited to this shortlist-rerank framework: we use a low-d MR for forming index, where we could tune AdANNS parameters according to the accuracy-compute trade-off of the graph and OPQ vectors. We then use a high-d MR for re-ranking.\\nEmpirical results. Figure 4 shows that AdANNS-IVFOPQ is 1   4% better than the baseline at all the PQ compute budgets. Furthermore, AdANNS-IVFOPQ has the same ac- curacy as the baselines at 8  lower overhead. With DiskANN, AdANNS accelerates shortlist generation by us- ing low-dimensional representations and recoups the accuracy by re- ranking with the highest-dimensional MR at negligible cost. Table 1 shows that AdANNS-DiskANN is more accurate than the baseline for both 1-NN and ranking performance at only half the cost. Using low-dimensional representations further speeds up inference in AdANNS-DiskANN (see Appendix F).\\nTable 1: AdANNS-DiskANN using a 16-d MR + re-ranking with the 2048-d MR outperforms DiskANN built on 2048-d RR at half the compute cost on ImageNet retrieval.\\nRR-2048 AdANNS\\n16 70.56 64.70 68.25\\nPQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)\\n32 70.37 62.46 65.65\\nThese results show the generality of AdANNS and its broad applicability across a variety of ANNS indices built on top of the base building blocks. Currently, AdANNS piggybacks on typical ANNS pipelines for their inherent accounting of the real-world system constraints [16, 22, 25]. However, we believe that AdANNS s flexibility and significantly better accuracy-compute trade-off can be further informed by real-world deployment constraints. We leave this high-potential line of work that requires extensive study to future research.\\n5 Further Analysis and Discussion\\n5.1 Compute-aware Elastic Search During Inference\\nAdANNS search structures cater to many specific large-scale use scenarios that need to satisfy precise resource constraints during construction as well as inference. However, in many cases, construction and storage of the indices are not the bottlenecks or the user is unable to search the design space. In these settings, AdANNS-D enables adaptive inference through accurate yet cheaper distance computation using the low-dimensional prefix of matryoshka representation. Akin to composite indices (Section 4.3) that use PQ vectors for cheaper distance computation, we can use the low- dimensional MR for faster distance computation on ANNS structure built non-adaptively with a high-dimensional MR without any modifications to the existing index.\\nEmpirical results. Figure 2 shows that for a given compute budget using IVF on ImageNet-1K retrieval, AdANNS-IVF is better than AdANNS-IVF-D due to the explicit control during the building\\nof the ANNS structure which is expected. However, the interesting observation is that AdANNS-D matches or outperforms the IVF indices built with MRs of varying capacities for ImageNet retrieval.\\nHowever, these methods are applicable in specific scenarios of deployment. Obtaining optimal AdANNS search structure (highly accurate) or even the best IVF-MR index relies on a relatively expensive design search but delivers indices that fit the storage, memory, compute, and accuracy constraints all at once. On the other hand AdANNS-D does not require a precisely built ANNS index but can enable compute-aware search during inference. AdANNS-D is a great choice for setups that can afford only one single database/index but need to cater to varying deployment constraints, e.g., one task requires 70% accuracy while another task has a compute budget of 1 MFLOPS/query.\\n5.2 Why MRs over RRs?\\nQuite a few of the gains from AdANNS are owing to the quality and capabilities of matryoshka representations. So, we conducted extensive analysis to understand why matryoshka representations seem to be more aligned for semantic search than the status-quo rigid representations.\\nDifficulty of NN search. Relative contrast (Cr) [18] is inversely proportional to the difficulty of nearest neighbor search on a given database. On ImageNet-1K, Figure 14 shows that MRs have better Cr than RRs across dimensionalities, further supporting that matryoshka representations are more aligned (easier) for NN search than existing rigid representations for the same accuracy. More details and analysis about this experiment can be found in Appendix H.2.\\nClustering distributions. We also investigate the potential deviation in clustering distributions for MRs across dimensionalities compared to RRs. Unlike the RRs where the information is uniformly diffused across dimensions [49], MRs have hierarchical information packing. Figure 11 in Appendix E.3 shows that matryoshka representations result in clusters similar (measured by total variation distance [33]) to that of rigid representations and do not result in any unusual artifacts.\\nRobustness. Figure 9 in Appendix E shows that MRs continue to be better than RRs even for out-of- distribution (OOD) image queries (ImageNetV2 [44]) using ANNS. It also shows that the highest data dimensionality need not always be the most robust which is further supported by the higher recall using lower dimensions. Further details about this experiment can be found in Appendix E.1.\\nGenerality across encoders. IVF-MR consistently has higher accuracy than IVF-RR across dimen- sionalities despite having similar accuracies with exact NN search (for ResNet50 on ImageNet and BERT-Base on NQ). We find that our observations on better alignment of MRs for NN search hold across neural network architectures, ResNet18/34/101 [19] and ConvNeXt-Tiny [35]. Appendix H.3 delves deep into the experimentation done using various neural architectures on ImageNet-1K.\\nRecall score analysis. Analysis of recall score (see Appendix C) in Appendix H.1 shows that for a similar top-1 accuracy, lower-dimensional representations have better 1-Recall@1 across search complexities for IVF and HNSW on ImageNet-1K. Across the board, MRs have higher recall scores and top-1 accuracy pointing to easier  searchability  and thus suitability of matryoshka representations for ANNS. Larger-scale experiments and further analysis can be found in Appendix H.\\nThrough these analyses, we argue that matryoshka representations are better suited for semantic search than rigid representations, thus making them an ideal choice for AdANNS.\\n5.3 Search for AdANNS Hyperparameters\\nChoosing the optimal hyperparameters for AdANNS, such as dc, ds, m, # clusters, # probes, is an interesting and open problem that requires more rigorous examination. As the ANNS index is formed once and used for potentially billions of queries with massive implications for cost, latency and queries-per-second, a hyperparameter search for the best index is generally an acceptable industry practice [22, 38]. The Faiss library [24] provides guidelines2 to choose the appropriate index for a specific problem, including memory constraints, database size, and the need for exact results. There have been efforts at automating the search for optimal indexing parameters, such as Autofaiss3, which maximizes recall given compute constraints.\\n2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss\\nIn case of AdANNS, we suggest starting at the best configurations of MRs followed by a local design space search to lead to near-optimal AdANNS configurations (e.g. use IVF-MR to bootstrap AdANNS-IVF). We also share some observations during the course of our experiments:\\n1. AdANNS-IVF: Top-1 accuracy generally improves (with diminishing returns after a point) with increasing dimensionality of clustering (dc) and search (ds), as we show on ImageNet variants and with multiple encoders in the Appendix (Figures 9 and 15). Clustering with low-d MRs matches the performance of high-d MRs as they likely contain similar amounts of useful information, making the increased compute cost not worth the marginal gains. Increasing # probes naturally boosts performance (Appendix, Figure 10a). Lastly, it is generally accepted that a good starting point for the # clusters k is (cid:112)ND/2, where ND is the number of indexable items [39]. k = ND is the optimal choice of k from a FLOPS computation perspective as can be seen in Appendix B.1.  \\n2. AdANNS-OPQ: we observe that for a fixed compute budget in bytes (m), the top-1 accuracy reaches a peak at d < dmax (Appendix, Table 4). We hypothesize that the better performance of AdANNS-OPQ at d < dmax is due to the curse of dimensionality, i.e. it is easier to learn PQ codebooks on smaller embeddings with similar amounts of information. We find that using an MR with d = 4   m is a good starting point on ImageNet and NQ. We also suggest using an 8-bit (256-length) codebook for OPQ as the default for each of the sub-block quantizer.\\n3. AdANNS-DiskANN: Our observations with DiskANN are consistent with other indexing struc- tures, i.e. the optimal graph construction dimensionality d < dmax (Appendix, Figure 12). A careful study of DiskANN on different datasets is required for more general guidelines to choose graph construction and OPQ dimensionality d.\\n5.4 Limitations\\nAdANNS s core focus is to improve the design of the existing ANNS pipelines. To use AdANNS on a corpus, we need to back-fill [43] the MRs of the data   a significant yet a one-time overhead. We also notice that high-dimensional MRs start to degrade in performance when optimizing also for an extremely low-dimensional granularity (e.g., < 24-d for NQ)   otherwise is it quite easy to have comparable accuracies with both RRs and MRs. Lastly, the existing dense representations can only in theory be converted to MRs with an auto-encoder-style non-linear transformation. We believe most of these limitations form excellent future work to improve AdANNS further.\\n6 Conclusions\\nWe proposed a novel framework, AdANNS , that leverages adaptive representations for different phases of ANNS pipelines to improve the accuracy-compute tradeoff. AdANNS utilizes the inherent flexibility of matryoshka representations [31] to design better ANNS building blocks than the standard ones which use the rigid representation in each phase. AdANNS achieves SOTA accuracy-compute trade-off for the two main ANNS building blocks: search data structures (AdANNS-IVF) and quantization (AdANNS-OPQ). The combination of AdANNS-based building blocks leads to the construction of better real-world composite ANNS indices   with as much as 8  reduction in cost at the same accuracy as strong baselines   while also enabling compute-aware elastic search. Finally, we note that combining AdANNS with elastic encoders [11] enables truly adaptive large-scale retrieval.\\nAcknowledgments\\nWe are grateful to Kaifeng Chen, Venkata Sailesh Sanampudi, Sanjiv Kumar, Harsha Vardhan Simhadri, Gantavya Bhatt, Matthijs Douze and Matthew Wallingford for helpful discussions and feedback. Aditya Kusupati also thanks Tom Duerig and Rahul Sukthankar for their support. Part of the paper s large-scale experimentation is supported through a research GCP credit award from Google Cloud and Google Research. Sham Kakade acknowledges funding from the ONR award N00014-22-1-2377 and NSF award CCF-2212841. Ali Farhadi acknowledges funding from the NSF awards IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543, and gifts from Allen Institute for Artificial Intelligence and Google.\\nReferences\\n[1] M. Aum ller, E. Bernhardsson, and A. Faithfull. Ann-benchmarks: A benchmarking tool for\\napproximate nearest neighbor algorithms. Information Systems, 87:101374, 2020.\\n[2] Y. Bengio. Deep learning of representations for unsupervised and transfer learning. In Proceed- ings of ICML workshop on unsupervised and transfer learning, pages 17 36. JMLR Workshop and Conference Proceedings, 2012.\\n[3] E. Bernhardsson. Annoy: Approximate Nearest Neighbors in C++/Python, 2018. URL\\nhttps://pypi.org/project/annoy/. Python package version 1.13.0.\\n[4] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. Computer\\nnetworks and ISDN systems, 30(1-7):107 117, 1998.\\n[5] D. Cai. A revisit of hashing algorithms for approximate nearest neighbor search.\\nIEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.\\n[6] T. Chen, L. Li, and Y. Sun. Differentiable product quantization for end-to-end embedding compression. In International Conference on Machine Learning, pages 1617 1626. PMLR, 2020.\\n[7] K. L. Clarkson. An algorithm for approximate closest-point queries. In Proceedings of the tenth\\nannual symposium on Computational geometry, pages 160 164, 1994.\\n[8] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational geometry, pages 253 262, 2004.\\n[9] J. Dean. Challenges in building large-scale information retrieval systems. In Keynote of the 2nd ACM International Conference on Web Search and Data Mining (WSDM), volume 10, 2009.\\n[10] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional\\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n[11] Devvrit, S. Kudugunta, A. Kusupati, T. Dettmers, K. Chen, I. Dhillon, Y. Tsvetkov, H. Hannaneh, S. Kakade, A. Farhadi, and P. Jain. Matformer: Nested transformer for elastic inference. arXiv preprint arxiv:2310.07707, 2023.\\n[12] J. H. Friedman, J. L. Bentley, and R. A. Finkel. An algorithm for finding best matches in logarithmic expected time. ACM Transactions on Mathematical Software (TOMS), 3(3):209  226, 1977.\\n[13] T. Ge, K. He, Q. Ke, and J. Sun. Optimized product quantization for approximate nearest neighbor search. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2946 2953, 2013.\\n[14] G. Golub and W. Kahan. Calculating the singular values and pseudo-inverse of a matrix. Journal of the Society for Industrial and Applied Mathematics, Series B: Numerical Analysis, 2(2): 205 224, 1965.\\n[15] R. Gray. Vector quantization. IEEE Assp Magazine, 1(2):4 29, 1984.\\n[16] R. Guo, P. Sun, E. Lindgren, Q. Geng, D. Simcha, F. Chern, and S. Kumar. Accelerating large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning, pages 3887 3896. PMLR, 2020.\\n[17] N. Gupta, P. H. Chen, H.-F. Yu, C.-J. Hsieh, and I. S. Dhillon. End-to-end learning to index and\\nsearch in large output spaces. arXiv preprint arXiv:2210.08410, 2022.\\n[18] J. He, S. Kumar, and S.-F. Chang. On the difficulty of nearest neighbor search. In International\\nConference on Machine Learning (ICML), 2012.\\n[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition.\\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770  778, 2016.\\n[20] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729 9738, 2020.\\n[21] P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 604 613, 1998.\\n[22] S. Jayaram Subramanya, F. Devvrit, H. V. Simhadri, R. Krishnawamy, and R. Kadekodi. Diskann: Fast accurate billion-point nearest neighbor search on a single node. Advances in Neural Information Processing Systems, 32, 2019.\\n[23] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. IEEE\\ntransactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.\\n[24] J. Johnson, M. Douze, and H. J gou. Billion-scale similarity search with GPUs.\\nTransactions on Big Data, 7(3):535 547, 2019.\\n[25] W. B. Johnson. Extensions of lipschitz mappings into a hilbert space. Contemp. Math., 26:\\n189 206, 1984.\\n[26] I. T. Jolliffe and J. Cadima. Principal component analysis: a review and recent developments. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065):20150202, 2016.\\n[27] V. Karpukhin, B. O guz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020.\\n[28] S. Kornblith, J. Shlens, and Q. V. Le. Do better imagenet models transfer better? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2661 2671, 2019.\\n[29] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned index structures. In Proceedings of the 2018 international conference on management of data, pages 489 504, 2018.\\n[30] A. Kusupati, M. Wallingford, V. Ramanujan, R. Somani, J. S. Park, K. Pillutla, P. Jain, S. Kakade, and A. Farhadi. Llc: Accurate, multi-purpose learnt low-dimensional binary codes. Advances in Neural Information Processing Systems, 34:23900 23913, 2021.\\n[31] A. Kusupati, G. Bhatt, A. Rege, M. Wallingford, A. Sinha, V. Ramanujan, W. Howard-Snyder, K. Chen, S. Kakade, P. Jain, and A. Farhadi. Matryoshka representation learning. In Advances in Neural Information Processing Systems, December 2022.\\n[32] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453 466, 2019.\\n[33] D. A. Levin and Y. Peres. Markov chains and mixing times, volume 107. American Mathematical\\nSoc., 2017.\\n[34] W. Li, Y. Zhang, Y. Sun, W. Wang, W. Zhang, and X. Lin. Approximate nearest neighbor search on high dimensional data experiments, analyses, and improvement. IEEE Transactions on Knowledge and Data Engineering, 2020.\\n[35] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11976 11986, 2022.\\n[36] S. Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):\\n129 137, 1982.\\nIEEE\\n[37] Y. Malkov, A. Ponomarenko, A. Logvinov, and V. Krylov. Approximate nearest neighbor algorithm based on navigable small world graphs. Information Systems, 45:61 68, 2014.\\n[38] Y. A. Malkov and D. Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis & Machine Intelligence, 42(04):824 836, 2020.\\n[39] K. Mardia, J. Kent, and J. Bibby. Multivariate analysis. Probability and Mathematical Statistics,\\n1979.\\n[40] P. Nayak. Understanding searches better than ever before. Google AI Blog, 2019. URL https:\\n//blog.google/products/search/search-language-understanding-bert/.\\n[41] A. Neelakantan, T. Xu, R. Puri, A. Radford, J. M. Han, J. Tworek, Q. Yuan, N. Tezak, J. W. Kim, C. Hallacy, et al. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005, 2022.\\n[42] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748 8763. PMLR, 2021.\\n[43] V. Ramanujan, P. K. A. Vasu, A. Farhadi, O. Tuzel, and H. Pouransari. Forward compatible\\ntraining for representation learning. arXiv preprint arXiv:2112.02805, 2021.\\n[44] B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning, pages 5389 5400. PMLR, 2019.\\n[45] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211 252, 2015.\\n[46] R. Salakhutdinov and G. Hinton. Semantic hashing. International Journal of Approximate\\nReasoning, 50(7):969 978, 2009.\\n[47] H. V. Simhadri, G. Williams, M. Aum ller, M. Douze, A. Babenko, D. Baranchuk, Q. Chen, L. Hosseini, R. Krishnaswamy, G. Srinivasa, et al. Results of the neurips 21 challenge on billion-scale approximate nearest neighbor search. arXiv preprint arXiv:2205.03763, 2022.\\n[48] J. Sivic and A. Zisserman. Video google: A text retrieval approach to object matching in videos. In Computer Vision, IEEE International Conference on, volume 3, pages 1470 1470. IEEE Computer Society, 2003.\\n[49] D. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N. Srebro. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):2822 2878, 2018.\\n[50] C. Waldburger. As search needs evolve, microsoft makes ai tools for better search available to researchers and developers. Microsoft AI Blog, 2019. URL https://blogs.microsoft. com/ai/bing-vector-search/.\\n[51] M. Wang, X. Xu, Q. Yue, and Y. Wang. A comprehensive survey and experimental comparison of graph-based approximate nearest neighbor search. Proceedings of the VLDB Endowment, 14 (11):1964 1978, 2021.\\n[52] R. Weber, H.-J. Schek, and S. Blott. A quantitative analysis and performance study for similarity- search methods in high-dimensional spaces. In VLDB, volume 98, pages 194 205, 1998.\\n[53] I. H. Witten, I. H. Witten, A. Moffat, T. C. Bell, T. C. Bell, E. Fox, and T. C. Bell. Managing gigabytes: compressing and indexing documents and images. Morgan Kaufmann, 1999.\\nA AdANNS Framework\\nAlgorithm 1 AdANNS-IVF Psuedocode\\n# Index database to construct clusters and build inverted file system\\ndef adannsConstruction(database, d_cluster, num_clusters):\\n# Slice database with cluster construction dim (d_cluster) xb = database[:d_cluster] cluster_centroids = constructClusters(xb, num_clusters)\\nreturn cluster_centroids\\ndef adannsInference(queries, centroids, d_shortlist, d_search, num_probes,\\nk): # Slice queries and centroids with cluster shortlist dim (d_shortlist) xq = queries[:d_shortlist] xc = centroids[:d_shortlist]\\nfor q in queries:\\n# compute distance of query from each cluster centroid candidate_distances = computeDistances(q, xc) # sort cluster candidates by distance and choose small number to\\nprobe\\ncluster_candidates = sortAscending(candidate_distances)[:num_probes] database_candidates = getClusterMembers(cluster_candidates) # Linear Scan all shortlisted clusters with search dim (d_search) k_nearest_neighbors[q] = linearScan(q, database_candidates, d_search,\\nreturn k_nearest_neighbors\\nDatabaseConstruct Clusters with  dc\\nSelect Closest Cluster\\n :   !\\n k  Ck\\nANNS ConstructionANNS Inference\\nLinear Scan with  ds\\n 1  C1\\nQuery\\nTop k Relevant Data Points\\n i  Ci\\nFigure 5: The schematic of inverted file index (IVF) outlaying the construction and inference phases. Adaptive representations can be utilized effectively in the decoupled components of clustering and searching for a better accuracy-compute trade-off (AdANNS-IVF).\\nTable 2: Mathematical formulae of the retrieval phase across various methods built on IVF. See Section 3 for notations.\\nMethod\\nRetrieval Formula during Inference\\nIVF-RR\\nIVF-MR\\nAdANNS-IVF\\nMG-IVF-RR\\n  RR(d)(q)    RR(d)(xj) , s.t. h(q) = arg minh   RR(d)(q)    RR(d)     MR(d)(q)    MR(d)(xj) , s.t. h(q) = arg minh   MR(d)(q)    MR(d)   MR(ds)(q)    MR(ds)(xj) , s.t. h(q) = arg minh   MR(dc)(q)    MR(dc)   RR(ds)(q)    RR(ds)(xj) , s.t. h(q) = arg minh   RR(dc)(q)    RR(dc)  \\narg minj Ch(q) arg minj Ch(q)\\narg minj Ch(q) arg minj Ch(q)\\nAdANNS-IVF-D arg minj Ch(q)\\nIVFOPQ\\n  MR(d)(q)[1 :  d]    MR(d)(xj)[1 :  d] , s.t. h(q) = arg minh   MR(d)(q)[1 :  d]    MR(d) arg minj Ch(q)\\n  PQ(m,b)(q)    PQ(m,b)(xj) , s.t. h(q) = arg minh   (q)    h \\nB Training and Compute Costs\\nA bulk of our ANNS experimentation was written with Faiss [24], a library for efficient similarity search and clustering. AdANNS was implemented from scratch (Algorithm 1) due to difficulty in decoupling clustering and linear scan with Faiss, with code available at https://github.com/ RAIVNLab/AdANNS. We also provide a version of AdANNS with Faiss optimizations with the restriction that Dc   Ds as a limitation of the current implementation, which can be further optimized. All ANNS experiments (AdANNS-IVF, MG-IVF-RR, IVF-MR, IVF-RR, HNSW, HNSWOPQ, IVFOPQ) were run on an Intel Xeon 2.20GHz CPU with 12 cores. Exact Search (Flat L2, PQ, OPQ) and DiskANN experiments were run with CUDA 11.0 on a A100-SXM4 NVIDIA GPU with 40G RAM. The wall-clock inference times quoted in Figure 1a and Table 3 are reported on CPU with Faiss optimizations, and are averaged over three inference runs for ImageNet-1K retrieval.\\nTable 3: Comparison of AdANNS-IVF and Rigid-IVF wall-clock inference times for ImageNet-1K retrieval. AdANNS-IVF has up to   1.5% gain over Rigid-IVF for a fixed search latency per query.\\nAdANNS-IVF\\nRigid-IVF\\nTop-1\\nSearch Latency/Query (ms)\\nTop-1\\nSearch Latency/Query (ms)\\n70.02 70.08 70.19 70.36 70.60\\n0.03 0.06 0.06 0.88 5.57\\n68.51 68.54 68.74 69.20 70.13\\n0.02 0.05 0.08 0.86 5.67\\nDPR [27] on NQ [32]. We follow the setup on the DPR repo4: the Wikipedia corpus has 21 million passages and Natural Questions dataset for open-domain QA settings. The training set contains 79,168 question and answer pairs, the dev set has 8,757 pairs and the test set has 3,610 pairs.\\nB.1 Inference Compute Cost\\nWe evaluate inference compute costs for IVF in MegaFLOPS per query (MFLOPS/query) as shown in Figures 2, 10a, and 8 as follows:\\nC = dsk +\\nnpdsND k\\nwhere dc is the cluster construction embedding dimensionality, ds is the embedding dim used for linear scan within each probed cluster, which is controlled by # of search probes np. Finally, k is the number of clusters |Ci| indexed over database of size ND. The default setting in this work, unless otherwise stated, is np = 1, k = 1024, ND = 1281167 (ImageNet-1K trainset). Vanilla IVF supports only dc = ds, while AdANNS-IVF provides flexibility via decoupling clustering and search (Section 4). AdANNS-IVF-D is a special case of AdANNS-IVF with the flexibility restricted to inference, i.e., dc is a fixed high-dimensional MR.\\n4https://github.com/facebookresearch/DPR\\n[1 :  d] \\nC Evaluation Metrics\\nIn this work, we primarily use top-1 accuracy (i.e. 1-Nearest Neighbor), recall@k, corrected mean average precision (mAP@k) [30] and k-Recall@N (recall score), which are defined over all queries Q over indexed database of size ND as:\\ntop-1 =\\n(cid:80)\\nQ correct_pred@1 |Q|\\nRecall@k =\\n(cid:80)\\nQ correct_pred@k |Q|\\nnum_classes |ND|\\nwhere correct_pred@k is the number of k-NN with correctly predicted labels for a given query. As noted in Section 3, k-Recall@N is the overlap between k exact search nearest neighbors (considered as ground truth) and the top N retrieved documents. As Faiss [24] supports a maximum of 2048- NN while searching the indexed database, we report 40-Recall@2048 in Figure 13. Also note that for ImageNet-1K, which constitutes a bulk of the experimentation in this work, |Q| = 50000, |ND| = 1281167 and num_classes = 1000. For ImageNetv2 [44], |Q| = 10000 and num_classes = 1000, and for ImageNet-4K [31], |Q| = 210100, |ND| = 4202000 and num_classes = 4202. For NQ [32], |Q| = 3610 and |ND| = 21015324. As NQ consists of question-answer pairs (instance- level), num_classes = 3610 for the test set.\\n 5 L J L G   2 3 4\\n    \\n 2 3 4   0 5          \\n     7 R S       $ F F X U D F \\\\       \\n $ G $ 1 1 6   2 3 4\\n    \\n    \\n     & R P S X W H   % X G J H W     % \\\\ W H V \\n    \\n    \\n $ G $ 1 1 6   , 9 )   2 3 4\\n    \\n , 9 )   2 3 4   0 5          \\n     & R P S X W H   % X G J H W     % \\\\ W H V \\n    \\n     7 R S       $ F F X U D F \\\\       \\n    \\n 5 L J L G   , 9 )   2 3 4\\n    \\n    \\n(a) Exact Search + OPQ on ImageNet-1K\\n(b) IVF + OPQ on ImageNet-1K\\n         7 R S       $ F F X U D F \\\\       \\n 5 L J L G   ' L V N $ 1 1   2 3 4\\n    \\n        \\n        \\n ' L V N $ 1 1   2 3 4   0 5          \\n        \\n $ G $ 1 1 6   ' L V N $ 1 1   2 3 4\\n     & R P S X W H   % X G J H W     % \\\\ W H V \\n    \\n     7 R S       $ F F X U D F \\\\       \\n    \\n    \\n    \\n    \\n     & R P S X W H   % X G J H W     % \\\\ W H V \\n    \\n $ G $ 1 1 6   + 1 6 :   2 3 4\\n + 1 6 :   2 3 4   0 5          \\n 5 L J L G   + 1 6 :   2 3 4\\n(c) DiskANN + OPQ on ImageNet-1K\\n(d) HNSW + OPQ on ImageNet-1K\\nFigure 6: Top-1 Accuracy of AdANNS composite indices with OPQ distance computation compared to MR and Rigid baselines models on ImageNet-1K and Natural Questions.\\nD AdANNS-OPQ\\nIn this section, we take a deeper dive into the quantization characteristics of MR. In this work, we restrict our focus to optimized product quantization (OPQ) [13], which adds a learned space rotation and dimensionality permutation to PQ s sub-vector quantization to learn more optimal PQ codes. We compare OPQ to vanilla PQ on ImageNet in Table 4, and observe large gains at larger embedding dimensionalities, which agrees with the findings of Jayaram Subramanya et al. [22].\\nWe perform a study of composite OPQ m   b indices on ImageNet-1K across compression compute budgets m (where b = 8, i.e. 1 Byte), i.e. Exact Search with OPQ, IVF+OPQ, HNSW+OPQ, and DiskANN+OPQ, as seen in Figure 6. It is evident from these results:\\n1. Learning OPQ codebooks with AdANNS (Figure 6a) provides a 1-5% gain in top-1 accuracy over rigid representations at low compute budgets (  32 Bytes). AdANNS-OPQ saturates to Rigid-OPQ performance at low compression (  64 Bytes).\\n2. For IVF, learning clusters with MRs instead of RRs (Figure 6b) provides substantial gains (1- 4%). In contrast to Exact-OPQ, using AdANNS for learning OPQ codebooks does not provide substantial top-1 accuracy gains over MR with d = 2048 (highest), though it is still slightly better or equal to MR-2048 at all compute budgets. This further supports that IVF performance generally scales with embedding dimensionality, which is consistent with our findings on ImageNet across robustness variants and encoders (See Figures 9 and 15 respectively).\\n3. Note that in contrast to Exact, IVF, and HNSW coarse quantizers, DiskANN inherently re-ranks the retrieved shortlist with high-precision embeddings (d = 2048), which is reflected in its high top-1 accuracy. We find that AdANNS with 8-byte OPQ (Figure 6c) matches the top-1 accuracy of rigid representations using 32-byte OPQ, for a 4  cost reduction for the same accuracy. Also note that using AdANNS provides large gains over using MR-2048 at high compression (1.5%), highlighting the necessity of AdANNS s flexibility for high-precision retrieval at low compute budgets.\\n4. Our findings on the HNSW-OPQ composite index (Figure 6d) are consistent with all other indices, i.e. HNSW graphs constructed with AdANNS OPQ codebooks provide significant gains over RR and MR, especially at high compression (  32 Bytes).\\nOPQ on NQ dataset\\n    \\n    \\n    \\n    \\n    \\n    \\n 2 3 4   0 5        \\n    \\n     7 R S       $ F F X U D F \\\\       \\n    \\n    \\n $ G $ 1 1 6   2 3 4\\n     & R P S X W H   % X G J H W     % \\\\ W H V \\n 5 L J L G   2 3 4\\n 5 L J L G   , 9 )   2 3 4\\n     & R P S X W H   % X G J H W     % \\\\ W H V \\n    \\n    \\n , 9 )   2 3 4   0 5        \\n    \\n    \\n $ G $ 1 1 6   , 9 )   2 3 4\\n    \\n    \\n    \\n     7 R S       $ F F X U D F \\\\       \\n    \\n    \\n(a) Exact Search + OPQ on Natural Questions\\n(b) IVF + OPQ on Natural Questions\\nFigure 7: Top-1 Accuracy of AdANNS composite indices with OPQ distance computation compared to MR and Rigid baselines models on Natural Questions.\\nOur observations on ImageNet with ResNet-50 MR across search structures also extend to the Natural Questions dataset with Dense Passage Retriever (DPR with BERT-Base MR embeddings). We note that AdANNS provides gains over RR-768 embeddings for both Exact Search and IVF with OPQ (Figure 7a and 7b). We find that similar to ImageNet (Figure 15) IVF performance on Natural Questions generally scales with dimensionality. AdANNS thus reduces to MR-768 performance for M   16. See Appendix G for a more in-depth discussion of AdANNS with DPR on Natural Questions.\\nTable 4: Comparison of PQ-MR with OPQ-MR for exact search on ImageNet-1K across embedding dimensionality d   {8, 16, ..., 2048} quantized to m   {8, 16, 32, 64} bytes. OPQ shows large gains over vanilla PQ at larger embedding dimensionalities d   128. Entries with the highest top-1 accuracy for a given (d, m) tuple are bolded.\\nConfig\\nOPQ\\nm Top-1 mAP@10\\nP@100\\nTop-1 mAP@10\\nP@100\\n62.18\\n56.71\\n61.23\\n62.22\\n56.70\\n61.23\\n8 16\\n67.91 67.85\\n62.85 62.95\\n67.21 67.21\\n67.88 67.96\\n62.96 62.94\\n67.21 67.21\\n8 16 32\\n68.80 69.57 69.44\\n63.62 64.22 64.20\\n67.86 68.12 68.12\\n68.91 69.47 69.47\\n63.63 64.20 64.23\\n67.86 68.12 68.12\\n8 16 32 64\\n68.39 69.77 70.13 70.12\\n63.40 64.43 64.67 64.69\\n67.47 68.25 68.38 68.42\\n68.38 69.95 70.05 70.18\\n63.42 64.55 64.65 64.70\\n67.60 68.38 68.38 68.38\\n128\\n8 16 32 64\\n67.27 69.51 70.27 70.61\\n61.99 64.32 64.72 64.93\\n65.78 68.12 68.51 68.49\\n68.40 69.78 70.60 70.65\\n63.11 64.56 64.97 64.98\\n67.34 68.38 68.51 68.51\\n256\\n8 16 32 64\\n66.06 68.56 70.08 70.48\\n60.44 63.33 64.83 64.98\\n64.09 66.95 68.38 68.55\\n67.90 69.92 70.59 70.69\\n62.69 64.71 65.15 65.09\\n66.95 68.51 68.64 68.64\\n512\\n8 16 32 64\\n65.09 67.68 69.51 70.53\\n59.03 62.11 64.01 65.02\\n62.53 65.39 67.34 68.52\\n67.51 69.67 70.44 70.72\\n62.12 64.53 65.11 65.17\\n66.56 68.38 68.64 68.64\\n1024\\n8 16 32 64\\n64.58 66.84 68.71 69.88\\n58.26 61.07 62.92 64.35\\n61.75 64.09 66.04 67.68\\n67.26 69.34 70.43 70.81\\n62.07 64.23 65.03 65.19\\n66.56 68.12 68.64 68.64\\n2048\\n8 16 32 64\\n62.19 65.99 67.99 69.20\\n56.11 60.27 62.04 63.46\\n59.80 63.18 64.74 66.40\\n66.89 69.25 70.39 70.57\\n61.69 64.09 64.97 65.15\\n66.30 67.99 68.51 68.51\\nE AdANNS-IVF\\nInverted file index (IVF) [48] is a simple yet powerful ANNS data structure used in web-scale search systems [16]. IVF construction involves clustering (coarse quantization often through k-means) [36] on d-dimensional representation that results in an inverted file list [53] of all the data points in each cluster. During search, the d-dimensional query representation is first assigned to the closest clusters (# probes, typically set to 1) and then an exhaustive linear scan happens within each cluster to obtain the nearest neighbors. As seen in Figure 9, IVF top-1 accuracy scales logarithmically with increasing representation dimensionality d on ImageNet-1K/V2/4K. The learned low-d representations thus provide better accuracy-compute trade-offs compared to high-d representations, thus furthering the case for usage of AdANNS with IVF.\\nOur proposed adaptive variant of IVF, AdANNS-IVF, decouples the clustering, with dc dimensions, and the linear scan within each cluster, with ds dimensions   setting dc = ds results in non- adaptive vanilla IVF. This helps in the smooth search of design space for the optimal accuracy- compute trade-off. A naive instantiation yet strong baseline would be to use explicitly trained\\n    \\n $ G $ 1 1 6   , 9 )   '\\n    \\n 0 *   , 9 )   5 5\\n 0 *   , 9 )   6 9 '\\n    \\n    \\n    \\n     7 R S       $ F F X U D F \\\\     \\n    \\n , 9 )   0 5\\n $ G $ 1 1 6   , 9 )\\n    \\n , 9 )   5 5\\n   \\n      \\n       0 ) / 2 3 6   4 X H U \\\\\\nFigure 8: Top-1 accuracy vs. compute cost per query of AdANNS-IVF compared to IVF-MR, IVF-RR and MG-IVF-RR baselines on ImageNet-1K.\\ndc and ds dimensional rigid representations (called MG-IVF-RR, for multi-granular IVF with rigid representations). We also examine the setting of adaptively choosing low-dimensional MR to linear scan the shortlisted clusters built with high-dimensional MR, i.e. AdANNS-IVF-D, as seen in Table 5. As seen in Figure 8, AdANNS-IVF provides pareto-optimal accuracy-compute tradeoff across inference compute. This figure is a more exhaustive indication of AdANNS-IVF behavior compared to baselines than Figures 1a and 2. AdANNS-IVF is evaluated for all possible tuples of dc, ds, k = |C|   {8, 16, . . . , 2048}. AdANNS-IVF-D is evaluated for a pre-built IVF index with dc = 2048 and ds   {8, . . . , 2048}. MG-IVF-RR configurations are evaluated for dc   {8, . . . , ds}, ds   {32, . . . , 2048} and k = 1024 clusters. A study over additional k values is omitted due to high compute cost. Finally, IVF-MR and IVF-RR configurations are evaluated for dc = ds   {8, 16, . . . , 2048} and k   {256, . . . , 8192}. Note that for a fair comparison, we use np = 1 across all configurations. We discuss the inference compute for these settings in Appendix B.1.\\nE.1 Robustness\\n    \\n        \\n      \\n    \\n    \\n , 9 )   0 5\\n    \\n    \\n      \\n      \\n    \\n , 9 )   5 5\\n    \\n         5 H S U H V H Q W D W L R Q   6 L ] H\\n ( [ D F W   5 5\\n    \\n     7 R S       $ F F X U D F \\\\     \\n ( [ D F W   0 5\\n    \\n ( [ D F W   5 5\\n      \\n    \\n    \\n ( [ D F W   0 5\\n         5 H S U H V H Q W D W L R Q   6 L ] H\\n      \\n    \\n    \\n    \\n , 9 )   0 5\\n     7 R S       $ F F X U D F \\\\     \\n        \\n    \\n      \\n    \\n , 9 )   5 5\\n    \\n         5 H S U H V H Q W D W L R Q   6 L ] H\\n    \\n    \\n      \\n    \\n ( [ D F W   0 5\\n    \\n    \\n , 9 )   0 5\\n      \\n     7 R S       $ F F X U D F \\\\     \\n      \\n        \\n    \\n(a) ImageNet-1K\\n(b) ImageNetV2\\n(c) ImageNet-4K\\nFigure 9: Top-1 Accuracy variation of IVF-MR of ImageNet 1K, ImageNetV2 and ImageNet-4K. RR baselines are omitted on ImageNet-4K due to high compute cost.\\nAs shown in Figure 9, we examined the clustering capabilities of MRs on both in-distribution (ID) queries via ImageNet-1K and out-of-distribution (OOD) queries via ImageNetV2 [44], as well as on larger-scale ImageNet-4K [31]. For ID queries on ImageNet-1K (Figure 9a), IVF-MR is at least as accurate as Exact-RR for d   256 with a single search probe, demonstrating the quality of in- distribution low-d clustering with MR. On OOD queries (Figure 9b), we observe that IVF-MR is on average 2% more robust than IVF-RR across all cluster construction and linear scan dimensionalities d. It is also notable that clustering with MRs followed by linear scan with # probes = 1 is more robust than exact search with RR embeddings across all d   2048, indicating the adaptability of MRs to distribution shifts during inference. As seen in Table 5, on ImageNetV2 AdANNS-IVF-D is the best\\nTable 5: Top-1 Accuracy of AdANNS-IVF-D on out-of-distribution queries from ImageNetV2 compared to both IVF and Exact Search with MR and RR embeddings. Note that for AdANNS- IVF-D, the dimensionality used to build clusters dc = 2048.\\nAdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR\\n8 16 32 64 128 256 512 1024 2048\\n53.51 57.32 57.32 57.85 58.02 58.01 58.03 57.66 58.04\\n50.44 56.35 57.64 58.01 58.09 58.33 57.84 57.58 58.04\\n50.41 56.64 57.96 58.94 59.13 59.18 59.40 59.11 59.63\\n49.03 55.04 56.06 56.84 56.14 55.60 55.46 54.80 56.17\\n48.79 55.08 56.69 57.37 57.17 57.09 57.12 57.53 57.84\\nconfiguration for d   16, and is similarly accurate to IVF-MR at all other d. AdANNS-IVF-D with d = 128 is able to match its own accuracy with d = 2048, a 16  compute gain during inference. This demonstrates the potential of AdANNS to adaptively search pre-indexed clustering structures.\\nOn 4-million scale ImageNet-4K (Figure 9c), we observe similar accuracy trends of IVF-MR compared to Exact-MR as in ImageNet-1K (Figure 9a) and ImageNetV2 (Figure 9b). We omit baseline IVF-RR and Exact-RR experiments due to high compute cost at larger scale.\\nE.2 IVF-MR Ablations\\n a          & R P S X W H\\n     7 R S           \\n         0 ) / 2 3 6   4 X H U \\\\\\n    \\n    \\n , 9 )       3 U R E H\\n , 9 )       3 U R E H\\n      \\n    \\n    \\n ( [ D F W\\n    \\n      \\n , 9 )       3 U R E H\\n , 9 )       3 U R E H\\n     5 H F D O O #  \\n         5 H S U H V H Q W D W L R Q   6 L ] H\\n    \\n               5 H F D O O # 1\\n        \\n    \\n     5 H F D O O #    \\n    \\n     5 H F D O O #  \\n        \\n        \\n     5 H F D O O #  \\n        \\n      \\n        \\n     5 H F D O O #  \\n        \\n      \\n        \\n      \\n        \\n        \\n(a) 4K Search Probes (np)\\n(b) Centroid Recall\\nFigure 10: Ablations on IVF-MR Clustering: a) Analysis of accuracy-compute tradeoff with in- creasing IVF-MR search probes np on ImageNet-4K compared to Exact-MR and b) k-Recall@N on ImageNet-1K cluster centroids across representation sizes d. Cluster centroids retrieved with highest embedding dim d = 2048 were considered ground-truth centroids.\\nAs seen in Figure 10a, IVF-MR can match the accuracy of Exact Search on ImageNet-4K with   100  less compute. We also explored the capability of MRs at retrieving cluster centroids with low-d compared to a ground truth of 2048-d with k-Recall@N, as seen in Figure 10b. MRs were able to saturate to near-perfect 1-Recall@N for d   32 and N   4, indicating the potential of AdANNS at matching exact search performance with less than 10 search probes np.\\nE.3 Clustering Distribution\\nWe examined the distribution of learnt clusters across embedding dimensionalities d for both MR and RR models, as seen in Figure 11. We observe IVF-MR to have less variance than IVF-RR at d   {8, 16}, and slightly higher variance for d   32, while IVF-MR outperforms IVF-RR in top-1 across all d (Figure 9a). This indicates that although MR learns clusters that are less uniformly distributed than RR at high d, the quality of learnt clustering is superior to RR across all d. Note that a uniform distribution is N/k data points per cluster, i.e.   1250 for ImageNet-1K with k = 1024. We quantitatively evaluate the proximity of the MR and RR clustering distributions with Total Variation\\n150\\n300\\n300\\n2500Number of data points per cell\\nRR-128\\n300\\n350Number of cells\\n1000\\n1000\\ndTV=0.00076dTV,2048=0.00065\\nMR-16\\n1000\\n1000\\n150\\nRR-256\\n200\\nMR-1024\\n150\\n500\\nMR-256\\n2500Number of data points per cell\\n1500\\n100\\n350Number of cells\\n150\\nRR-512\\n2000\\ndTV=0.00068dTV,2048=0.00064\\ndTV=0.00054dTV,2048=0.00054\\n250\\n200\\ndTV=0.00016dTV,2048=0.00095\\n1500\\nMR-512\\n300\\n200\\n1000\\nMR-32\\n200\\nRR-8\\n2500Number of data points per cell\\nMR-8\\n400Number of cells\\n150\\n200\\n300\\n500\\n300Number of cells\\n500\\nRR-16\\n100\\n2000\\n200\\nMR-64\\n2000\\n1000\\n250\\n250\\n250\\n1000\\n1500\\n2000\\n500\\n300Number of cells\\n250\\ndTV=0.00075dTV,2048=0.00059\\n1000\\n1500\\n2000\\ndTV=0.00061dTV,2048=0.00069\\nRR-1024\\n2500Number of data points per cell\\n1500\\n100\\n2500Number of data points per cell\\n100\\n2000\\n2500Number of data points per cell\\nRR-64\\n200\\n2000\\n2000\\nRR-32\\n100\\n300\\n2500Number of data points per cell\\n1000\\n350Number of cells\\n200\\n500\\n500\\n1500\\n2500Number of data points per cell\\nRR-2048\\n100\\n2000\\n300\\n350Number of cells\\n400Number of cells\\n100\\n500\\n500\\n200\\ndTV=0.00027dTV,2048=0.00121\\n500\\n1500\\ndTV=0.00037dTV,2048=0.00078\\n250\\nMR-128\\n150\\n400Number of cells\\n1500\\n100\\n100\\n1500\\nMR-2048\\ndTV=0.00078dTV,2048=0.00128\\n2500Number of data points per cell\\nFigure 11: Clustering distributions for IVF-MR and IVF-RR across embedding dimensionality d on ImageNet-1K. An IVF-MR and IVF-RR clustered with d = 16 embeddings is denoted by MR-16 and RR-16 respectively.\\nDistance [33], which is defined over two discrete probability distributions p, q over [n] as follows:\\ndT V (p, q) =\\n1 2\\n(cid:88)\\n|pi   qi|\\ni [n]\\nWe also compute dT V,2048(MR-d) = dT V (MR-d, RR-2048), which evaluates the total variation dis- tance of a given low-d MR from high-d RR-2048. We observe a monotonically decreasing dT V,2048 with increasing d, which demonstrates that MR clustering distributions get closer to RR-2048 as we increase the embedding dimensionality d. We observe in Figure 11 that dT V (MR-d, RR-d)   7e   4 for d   {8, 256, . . . , 2048} and   3e   4 for d   {16, 32, 64}. These findings agree with the top-1 improvement of MR over RR as shown in Figure 9a, where there are smaller improvements for d   {16, 32, 64} (smaller dT V ) and larger improvements for d   {8, 256, . . . , 2048} (larger dT V ). These results demonstrate a correlation between top-1 performance of IVF-MR and the quality of clusters learnt with MR.\\nF AdANNS-DiskANN\\nDiskANN is a state-of-the-art graph-based ANNS index capable of serving queries from both RAM and SSD. DiskANN builds a greedy best-first graph with OPQ distance computation, with compressed vectors stored in memory. The index and full-precision vectors are stored on the SSD. During search,\\nTable 6: Wall clock search latency ( s) of AdANNS-DiskANN across graph construction dimension- ality d   {8, 16, . . . , 2048} and compute budget in terms of OPQ budget M   {8, 16, 32, 48, 64}. Search latency is fairly consistent across fixed embedding dimensionality D.\\nM =8 M =16 M =32 M =48 M =64\\n8 16 32 64 128 256 512 1024 2048\\n495 555 669 864 1182 1923 2802 5127 9907\\n571 655 855 1311 1779 3272 5456 9833\\n- 653 843 1156 1744 3423 5724 10205\\n- - 844 1161 2849 2780 4683 10183\\n- - 848 2011 1818 3171 5087 9329\\nwhen a query s neighbor shortlist is fetched from the SSD, its full-precision vector is also fetched in a single disk read. This enables efficient and fast distance computation with PQ on a large initial shortlist of candidate nearest neighbors in RAM followed by a high-precision re-ranking with full- precision vectors fetched from the SSD on a much smaller shortlist. The experiments carried out in this work primarily utilize a DiskANN graph index built in-memory5 with OPQ distance computation.\\nAs with IVF, DiskANN is also well suited to the flexibility provided by AdANNS as we demonstrate on both ImageNet and NQ that the optimal PQ codebook for a given compute budget is learnt with a smaller em- bedding dimensionality d (see Fig- ures 6c and 7a). We demonstrate the capability of AdANNS-DiskANN with a compute budget of m   {32, 64} in Table 1. We tabulate the search time latency of AdANNS- DiskANN in microseconds ( s) in Table 6, which grows linearly with graph construction dimensionality d. We also examine DiskANN-MR with SSD graph indices on ImageNet-1K across OPQ budgets for distance com- putation mdc   {32, 48, 64}, as seen in Figure 12. With SSD indices, we store PQ-compressed vectors on disk with mdisk = mdc, which es- sentially disables DiskANN s implicit high-precision re-ranking. We ob- serve similar trends to other composite ANNS indices on ImageNet, where the optimal dim for fixed OPQ budget is not the highest dim (d = 1024 with fp32 embeddings is current highest dim supported by DiskANN which stores vectors in 4KB sectors on disk). This provides further motiva- tion for AdANNS-DiskANN, which leverages MRs to provide flexible access to the optimal dim for quantization and thus enables similar Top-1 accuracy to Rigid DiskANN for up to 1/4 the cost (Figure 6c).\\n    \\n      \\n         5 H S U H V H Q W D W L R Q   6 L ] H\\n         7 R S       $ F F X U D F \\\\       \\n        \\n        \\n      \\n       % \\\\ W H\\n       % \\\\ W H\\n       % \\\\ W H\\n        \\n        \\n    \\n        \\n        \\n      \\nFigure 12: DiskANN-MR with SSD indices for ImageNet- 1K retrieval, with compute budgets mdisk = mdc   {32, 48, 64} across graph and OPQ codebook construction dimensionalities d   {32, . . . , 1024}. Note that this does not use any re-ranking after obtaining OPQ based shortlist.\\nG AdANNS on Natural Questions\\nIn addition to image retrieval on ImageNet, we also experiment with dense passage retrieval (DPR) on Natural Questions. As shown in Figure 6, MR representations are 1   10% more accurate than their\\n5https://github.com/microsoft/DiskANN\\nRR counterparts across PQ compute budgets with Exact Search + OPQ on NQ. We also demonstrate that IVF-MR is 1   2.5% better than IVF-RR for Precision@k, k   {1, 5, 20, 100, 200}. Note that on NQ, IVF loses   10% accuracy compared to exact search, even with the RR-768 baseline. We hypothesize the weak performance of IVF owing to poor clusterability of the BERT-Base embeddings fine-tuned on the NQ dataset. A more thorough exploration of AdANNS-IVF on NQ is an immediate future work and is in progress.\\nH Ablations\\nH.1 Recall Score Analysis\\n    \\n        \\n        \\n        \\n        \\n        \\n      \\n        \\n        \\n    \\n      \\n                 5 H F D O O #               \\n        \\n    \\n     , 9 )   6 H D U F K   3 U R E H V    np \\n      \\n        \\n    \\n    \\n     , 9 )   6 H D U F K   3 U R E H V    np \\n    \\n        \\n      \\n      \\n    \\n    \\n        \\n    \\n      \\n    \\n    \\n    \\n    \\n           5 H F D O O #         \\n        \\n    \\n      \\n    \\n      \\n    \\n             5 H F D O O #               \\n     + 1 6 :   6 H D U F K   3 U R E H V     H I 6 H D U F K \\n      \\n    \\n    \\n    \\n    \\n    \\n        \\n    \\n    \\n     + 1 6 :   6 H D U F K   3 U R E H V     H I 6 H D U F K \\n           5 H F D O O #         \\n      \\n    \\n    \\n    \\n    \\n    \\n      \\n    \\n    \\n    \\n    \\n        \\n      \\n    \\n        \\n    \\n    \\n 0 5          \\n    \\n    \\n 0 5        \\n    \\n    \\n             5 H F D O O #               \\n 0 5      \\n     , 9 )   6 H D U F K   3 U R E H V    np \\n 0 5    \\n    \\n           5 H F D O O #         \\n     , 9 )   6 H D U F K   3 U R E H V    np \\n    \\n 0 5      \\n    \\n    \\n    \\n 0 5          \\n    \\n 0 5    \\n 0 5        \\n    \\nFigure 13: k-Recall@N of d-dimensional MR for IVF and HNSW with increasing search probes np on ImageNet-1K and ImageNet-4K. On ImageNet-4K, we restrict our study to IVF-MR with d   {8, 64, 256, 2048}. Other embedding dimensionalities, HNSW-MR and RR baselines are omitted due to high compute cost. We observe that trends from ImageNet-1K with increasing d and np extend to ImageNet-4K, which is 4  larger.\\n         5 H S U H V H Q W D W L R Q   6 L ] H\\n    \\n      \\n      \\n    \\n        \\n        \\n        \\n        \\n 0 5\\n      \\n         5 H O D W L Y H   & R Q W U D V W\\n      \\n      \\n        \\n      \\n 5 5\\n        \\n    \\nFigure 14: Relative contrast of varying capacity MRs and RRs on ImageNet-1K corroborating the findings of He et al. [18].\\nIn this section we also examine the variation of k-Recall@N with by probing a larger search space with IVF and HNSW indices. For IVF, search probes represent the number of clusters shortlisted for linear scan during inference. For HNSW, search quality is controlled by the ef Search parameter [38], which represents the closest neighbors to query q at level lc of the graph and is analogous to number of search probes in IVF. As seen in Figure 13, general trends show a) an intuitive increase in recall with increasing search probes np) for fixed search probes, b) a decrease in recall with increasing search dimensionality d c) similar trends in ImageNet-1K and 4  larger ImageNet-4K.\\nH.2 Relative Contrast\\nWe utilize Relative Contrast [18] to capture the difficulty of nearest neighbors search with IVF-MR compared to IVF-RR. For a given database X = {xi   Rd, i = 1, . . . , ND}, a query q   Rd, and a distance metric D(., .) we compute relative contrast Cr as a measure of the difficulty in finding the 1-nearest neighbor (1-NN) for a query q in database X as follows:\\n1. Compute Dq\\nmin = min i=1...n\\nD(q, xi), i.e. the distance of query q to its nearest neighbor xq\\nnn   X\\n2. Compute Dq x   X\\nmean = Ex[D(q, x)] as the average distance of query q from all database points\\nDq mean Dq\\n3. Relative Contrast of a given query C q r =\\n, which is a measure of how separable the\\nmin nn is from an average point in the database x\\nquery s nearest neighbor xq\\n4. Compute an expectation over all queries for Relative Contrast over the entire database as\\nCr =\\nEq[Dq Eq[Dq\\nmean] min]\\nIt is evident that Cr captures the difficulty of Nearest Neighbor Search in database X, as a Cr   1 indicates that for an average query, its nearest neighbor is almost equidistant from a random point in the database. As demonstrated in Figure 14, MRs have higher Rc than RR Embeddings for an Exact Search on ImageNet-1K for all d   16. This result implies that a portion of MR s improvement over RR for 1-NN retrieval across all embedding dimensionalities d [31] is due to a higher average separability of the MR 1-NN from a random database point.\\nH.3 Generality across Encoders\\nWe perform an ablation over the representation function   : X   Rd learnt via a backbone neural network (primarily ResNet50 in this work), as detailed in Section 3. We also train MRL models [31]  M R(d) on ResNet18/34/101 [19] that are as accurate as their independently trained RR baseline models  RR(d), where d is the default max representation size of each architecture. We also train\\nMRL with a ConvNeXt-Tiny backbone with [d] = {48, 96, 192, 384, 786}. MR-768 has a top-1 accuracy of 79.45% compared to independently trained publicly available RR-768 baseline with top-1 accuracy 82.1% (Code and RR model available on the official repo6). We note that this training had no hyperparameter tuning whatsoever, and this gap can be closed with additional model training effort. We then compare clustering the MRs via IVF-MR with k = 2048, np = 1 on ImageNet-1K to Exact-MR, which is shown in Figure 15. IVF-MR shows similar trends across backbones compared to Exact-MR, i.e. a maximum top-1 accuracy drop of   1.6% for a single search probe. This suggests the clustering capabilities of MR extend beyond an inductive bias of  M R(d)   ResNet50, though we leave a detailed exploration for future work.\\n 5 H V 1 H W    \\n     7 R S       $ F F X U D F \\\\       \\n    \\n 5 H V 1 H W    \\n    \\n ( [ D F W   0 5\\n    \\n 5 H V 1 H W    \\n    \\n    \\n    \\n    \\n      \\n         5 H S U H V H Q W D W L R Q   6 L ] H\\n 5 H V 1 H W      \\n      \\n & R Q Y 1 H ; W   7 L Q \\\\\\n , 9 )   0 5\\n    \\n        \\n      \\nFigure 15: Top-1 Accuracy variation of IVF-MR on ImageNet-1K with different embedding rep- resentation function  M R(d) (see Section 3), where     {ResNet18/34/101, ConvNeXt-Tiny}. We observe similar trends between IVF-MR and Exact-MR on ResNet18/34/101 when compared to ResNet50 (Figure 9a) which is the default in all experiments in this work.\\n6https://github.com/facebookresearch/ConvNeXt\\n\", source='data%2fpdf%2fadaptive_semantic_search.pdf', source_type=<SourceType.application_pdf: 'application/pdf'>, num_chunks=83, metadata={}, chunks=[ResponseChunk(id='chunk_ffe05b8a-1a09-45b7-9803-0ef99bf25d08', content='3 2 0 2 t c O 8 1 G L . s c [ 2 v 5 3 4 9 1 . 5 0 3 2 : v i X r a AdANNS: A Framework for Adaptive Semantic Search Aniket Rege    Aditya Kusupati    Sharan Ranjit S  Alan Fan  Qingqing Cao , Sham Kakade  Prateek Jain  Ali Farhadi   University of Washington,  Google Research,  Harvard University {kusupati,ali}@cs.washington.edu, prajain@google.com Abstract Web-scale search systems learn an encoder to embed a given query which is then hooked into an approximate nearest neighbor search (ANNS) pipeline to retrieve similar data points. To accurately capture tail queries and data points, learned representations typically are rigid, high-dimensional vectors that are generally used as-is in the entire ANNS pipeline and can lead to computationally expensive retrieval. In this paper, we argue that instead of rigid representations, different stages of ANNS can leverage adaptive representations of varying capacities to achieve significantly better accuracy-compute trade-offs, i.e., stages of ANNS that can get away with more approximate computation should use a lower-capacity representation of the same data point. To this end, we introduce AdANNS , a novel ANNS design framework that explicitly leverages the flexibility of Ma- tryoshka Representations [31].', chunk_index=1, num_tokens=297, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_7564b763-f711-4999-a135-3a0efba692d0', content='We demonstrate state-of-the-art accuracy-compute trade-offs using novel AdANNS-based key ANNS building blocks like search data structures (AdANNS-IVF) and quantization (AdANNS-OPQ). For exam- ple on ImageNet retrieval, AdANNS-IVF is up to 1.5% more accurate than the rigid representations-based IVF [48] at the same compute budget; and matches accuracy while being up to 90  faster in wall-clock time. For Natural Questions, 32-byte AdANNS-OPQ matches the accuracy of the 64-byte OPQ baseline [13] constructed using rigid representations   same accuracy at half the cost! We further show that the gains from AdANNS translate to modern-day composite ANNS indices that combine search structures and quantization. Finally, we demonstrate that AdANNS can enable inference-time adaptivity for compute-aware search on ANNS indices built non-adaptively on matryoshka representations. Code is open-sourced at https://github.com/RAIVNLab/AdANNS. Introduction Semantic search [24] on learned representations [40, 41, 50] is a major component in retrieval pipelines [4, 9]. In its simplest form, semantic search methods learn a neural network to embed queries as well as a large number (N ) of data points in a d-dimensional vector space.', chunk_index=2, num_tokens=286, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_123895cc-06cc-458f-9800-0e044d67068e', content='For a given query, the nearest (in embedding space) point is retrieved using either an exact search or using approximate nearest neighbor search (ANNS) [21] which is now indispensable for real-time large-scale retrieval. Existing semantic search methods learn fixed or rigid representations (RRs) which are used as is in all the stages of ANNS (data structures for data pruning and quantization for cheaper distance computation; see Section 2). That is, while ANNS indices allow a variety of parameters for searching the design space to optimize the accuracy-compute trade-off, the provided data dimensionality is typically assumed to be an immutable parameter. To make it concrete, let us consider inverted file index (IVF) [48], a popular web-scale ANNS technique [16].', chunk_index=3, num_tokens=156, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_f76d16c4-2d75-4f54-89df-5fed59ad307a', content='IVF has two stages (Section 3) during inference: (a) cluster mapping: mapping the query to a cluster of data points [36], and (b) linear Equal contribution. 37th Conference on Neural Information Processing Systems (NeurIPS 2023).  6 H D U F K   / D W H Q F \\\\   4 X H U \\\\     P V 5 L J L G   , 9 ) a           J D L Q a        U H D O   Z R U O G   V S H H G   X S 7 R S       $ F F X U D F \\\\ $ G $ 1 1 6   , 9 )       7 R S       $ F F X U D F \\\\  5 L J L G   2 3 4 $ G $ 1 1 6   2 3 4    a       J D L Q      F K H D S H U & R P S X W H   % X G J H W     % \\\\ W H V (a) Image retrieval on ImageNet-1K. (b) Passage retrieval on Natural Questions. Figure 1: AdANNS helps design search data structures and quantization methods with better accuracy-compute trade-offs than the existing solutions.', chunk_index=4, num_tokens=275, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_46c7ad50-ec2d-42b2-8a05-9315f7bf384b', content='In particular, (a) AdANNS-IVF improves on standard IVF by up to 1.5% in accuracy while being 90  faster in deployment and (b) AdANNS-OPQ is as accurate as the baseline at half the cost! Rigid-IVF and Rigid-OPQ are standard techniques that are built on rigid representations (RRs) while AdANNS uses matryoshka representations (MRs) [31]. scan: distance computation w.r.t all points in the retrieved cluster to find the nearest neighbor (NN). Standard IVF utilizes the same high-dimensional RR for both phases, which can be sub-optimal. Why the sub-optimality? Imagine one needs to partition a dataset into k clusters for IVF and the dimensionality of the data is d   IVF uses full d representation to partition into k clusters. However, suppose we have an alternate approach that somehow projects the data in d/2 dimensions and learns 2k clusters. Note that the storage and computation to find the nearest cluster remains the same in both cases, i.e., when we have k clusters of d dimensions or 2k clusters of d/2 dimensions. 2k clusters can provide significantly more refined partitioning, but the distances computed between queries and clusters could be significantly more inaccurate after projection to d/2 dimensions.', chunk_index=5, num_tokens=278, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_b01fb987-3d77-477c-951d-974d3e4a9561', content='So, if we can find a mechanism to obtain a d/2-dimensional representation of points that can accurately approximate the topology/distances of d-dimensional representation, then we can potentially build significantly better ANNS structure that utilizes different capacity representations for the cluster mapping and linear scan phases of IVF. But how do we find such adaptive representations? These desired adaptive representations should be cheap to obtain and still ensure distance preservation across dimensionality. Post-hoc dimensionality reduction techniques like SVD [14] and random projections [25] on high-dimensional RRs are potential candidates, but our experiments indicate that in practice they are highly inaccurate and do not preserve distances well enough (Figure 2). Instead, we identify that the recently proposed Matryoshka Representations (MRs) [31] satisfy the specifications for adaptive representations. Matryoshka representations pack information in a hierarchical nested manner, i.e., the first m-dimensions of the d-dimensional MR form an accurate low-dimensional representation while being aware of the information in the higher dimensions. This allows us to deploy MRs in two major and novel ways as part of ANNS: (a) low-dimensional representations for accuracy-compute optimal clustering and quantization, and (b) high-dimensional representations for precise re-ranking when feasible.', chunk_index=6, num_tokens=257, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_5bd17e73-15d1-4c30-87f0-4471c5f70368', content='To this effort, we introduce AdANNS , a novel design framework for semantic search that uses matryoshka representation-based adaptive representations across different stages of ANNS to ensure significantly better accuracy-compute trade-off than the state-of-the-art baselines. Typical ANNS systems have two key components: (a) search data structure to store datapoints, (b) distance computation to map a given query to points in the data structure. Through AdANNS, we address both these components and significantly improve their performance. In particular, we first propose AdANNS-IVF (Section 4.1) which tackles the first component of ANNS systems. AdANNS- IVF uses standard full-precision computations but uses adaptive representations for different IVF stages. On ImageNet 1-NN image retrieval (Figure 1a), AdANNS-IVF is up to 1.5% more accurate for the compute budget and 90  cheaper in deployment for the same accuracy as IVF. We then propose AdANNS-OPQ (Section 4.2) which addresses the second component by using AdANNS-based quantization (OPQ [13])   here we use exhaustive search overall points. AdANNS- OPQ is as accurate as the baseline OPQ on RRs while being at least 2  faster on Natural Ques- tions [32] 1-NN passage retrieval (Figure 1b).', chunk_index=7, num_tokens=296, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_0e6e2364-c841-4b13-bbeb-ccf79f7f3c20', content='Finally, we combine the two techniques to obtain AdANNS-IVFOPQ (Section 4.3) which is more accurate while being much cheaper   up to 8    than the traditional IVFOPQ [24] index. To demonstrate generality of our technique, we adapt AdANNS to DiskANN [22] which provides interesting accuracy-compute tradeoff; see Table 1. While MR already has multi-granular representations, careful integration with ANNS building blocks is critical to obtain a practical method and is our main contribution. In fact, Kusupati et al. [31] proposed a simple adaptive retrieval setup that uses smaller-dimensional MR for shortlisting in re- trieval followed by precise re-ranking with a higher-dimensional MR. Such techniques, unfortunately, cannot be scaled to industrial systems as they require forming a new index for every shortlisting provided by low-dimensional MR. Ensuring that the method aligns well with the modern-day ANNS pipelines is important as they already have mechanisms to handle real-world constraints like load-balancing [16] and random access from disk [22]. So, AdANNS is a step towards making the abstraction of adaptive search and retrieval feasible at the web-scale. Through extensive experimentation, we also show that AdANNS generalizes across search data structures, distance approximations, modalities (text & image), and encoders (CNNs & Transformers) while still translating the theoretical gains to latency reductions in deployment.', chunk_index=8, num_tokens=299, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_e42f305f-38b3-4e1b-80ad-0c6ac47af5a8', content='While we have mainly focused on IVF and OPQ-based ANNS in this work, AdANNS also blends well with other ANNS pipelines. We also show that AdANNS can enable compute-aware elastic search on prebuilt indices without making any modifications (Section 5.1); note that this is in contrast to AdANNS-IVF that builds the index explicitly utilizing  adaptivity  in representations. Finally, we provide an extensive analysis on the alignment of matryoshka representation for better semantic search (Section 5.2). We make the following key contributions: We introduce AdANNS , a novel framework for semantic search that leverages matryoshka representations for designing ANNS systems with better accuracy-compute trade-offs. AdANNS powered search data structure (AdANNS-IVF) and quantization (AdANNS-OPQ) show a significant improvement in accuracy-compute tradeoff compared to existing solutions. AdANNS generalizes to modern-day composite ANNS indices and can also enable compute-aware elastic search during inference with no modifications. 2 Related Work Approximate nearest neighbour search (ANNS) is a paradigm to come as close as possible [7] to retrieving the  true  nearest neighbor (NN) without the exorbitant search costs associated with exhaustive search [21, 52]. The  approximate  nature comes from data pruning as well as the cheaper distance computation that enable real-time web-scale search.', chunk_index=9, num_tokens=295, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_4f6a2347-e70d-4494-8d03-e9680f9b8fc1', content='In its naive form, NN-search has a complexity of O(dN ); d is the data dimensionality used for distance computation and N is the size of the database. ANNS employs each of these approximations to reduce the linear dependence on the dimensionality (cheaper distance computation) and data points visited during search (data pruning). Cheaper distance computation. From a bird s eye view, cheaper distance computation is always obtained through dimensionality reduction (quantization included). PCA and SVD [14, 26] can reduce dimensionality and preserve distances only to a limited extent without sacrificing accuracy. On the other hand, quantization-based techniques [6, 15] like (optimized) product quantization ((O)PQ) [13, 23] have proved extremely crucial for relatively accurate yet cheap distance computation and simultaneously reduce the memory overhead significantly. Another naive solution is to indepen- dently train the representation function with varying low-dimensional information bottlenecks [31] which is rarely used due to the costs of maintaining multiple models and databases. Data pruning. Enabled by various data structures, data pruning reduces the number of data points visited as part of the search. This is often achieved through hashing [8, 46], trees [3, 12, 16, 48] and graphs [22, 38]. More recently there have been efforts towards end-to-end learning of the search data structures [17, 29, 30].', chunk_index=10, num_tokens=295, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_7fba0ed4-ad1c-4bb1-95d1-2350b7e793af', content='However, web-scale ANNS indices are often constructed on rigid d-dimensional real vectors using the aforementioned data structures that assist with the real-time search. For a more comprehensive review of ANNS structures please refer to [5, 34, 51]. Composite indices. ANNS pipelines often benefit from the complementary nature of various building blocks [24, 42]. In practice, often the data structures (coarse-quantizer) like IVF [48] and HNSW [37] are combined with cheaper distance alternatives like PQ [23] (fine-quantizer) for massive speed-ups in web-scale search. While the data structures are built on d-dimensional real vectors, past works consistently show that PQ can be safely used for distance computation during search time. As evident in modern web-scale ANNS systems like DiskANN [22], the data structures are built on d-dimensional real vectors but work with PQ vectors (32   64-byte) for fast distance computations. ANNS benchmark datasets. Despite the Herculean advances in representation learning [19, 42], ANNS progress is often only benchmarked on fixed representation vectors provided for about a dozen million to billion scale datasets [1, 47] with limited access to the raw data. This resulted in the improvement of algorithmic design for rigid representations (RRs) that are often not specifically designed for search.', chunk_index=11, num_tokens=277, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_d7ca5211-7721-4967-8e09-510c5322bbe0', content='All the existing ANNS methods work with the assumption of using the provided d-dimensional representation which might not be Pareto-optimal for the accuracy-compute trade- off in the first place. Note that the lack of raw-image and text-based benchmarks led us to using ImageNet-1K [45] (1.3M images, 50K queries) and Natural Questions [32] (21M passages, 3.6K queries) for experimentation. While not billion-scale, the results observed on ImageNet often translate to real-world progress [28], and Natural Questions is one of the largest question answering datasets benchmarked for dense passage retrieval [27], making our results generalizable and widely applicable. In this paper, we investigate the utility of adaptive representations   embeddings of different dimen- sionalities having similar semantic information   in improving the design of ANNS algorithms. This helps in transitioning out of restricted construction and inference on rigid representations for ANNS. To this end, we extensively use Matryoshka Representations (MRs) [31] which have desired adaptive properties in-built.', chunk_index=12, num_tokens=223, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_98953442-17fd-4c78-8419-611169e4664b', content='To the best of our knowledge, this is the first work that improves accuracy-compute trade-off in ANNS by leveraging adaptive representations on different phases of construction and inference for ANNS data structures. 3 Problem Setup, Notation, and Preliminaries The problem setup of approximate nearest neighbor search (ANNS) [21] consists of a database of N data points, [x1, x2, . . . , xN ], and a query, q, where the goal is to  approximately  retrieve the nearest data point to the query. Both the database and query are embedded to Rd using a representation function   : X   Rd, often a neural network that can be learned through various representation learning paradigms [2, 19, 20, 40, 42]. Matryoshka Representations (MRs). The d-dimensional representations from   can have a nested structure like Matryoshka Representations (MRs) [31] in-built    MR(d). Matryoshka Representation Learning (MRL) learns these nested representations with a simple strategy of optimizing the same training objective at varying dimensionalities. These granularities are ordered such that the lowest representation size forms a prefix for the higher-dimensional representations.', chunk_index=13, num_tokens=252, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_ef124a0a-b5e4-4586-889d-7a6427d877b3', content='So, high-dimensional MR inherently contains low-dimensional representations of varying granularities that can be accessed for free   first m-dimensions (m   [d]) ie.,  MR(d)[1 : m] from the d-dimensional MR form an m-dimensional representation which is as accurate as its independently trained rigid representation (RR) counterpart    RR(m). Training an encoder with MRL does not involve any overhead or hyperparameter tuning and works seamlessly across modalities, training objectives, and architectures. Inverted File Index (IVF). IVF [48] is an ANNS data structure used in web-scale search sys- tems [16] owing to its simplicity, minimal compute overhead, and high accuracy. IVF construction involves clustering (coarse quantization through k-means) [36] on d-dimensional representation that results in an inverted file list [53] of all the data points in each cluster. During search, d-dimensional query representation is assigned to the most relevant cluster (Ci; i   [k]) by finding the closest cen- troid ( i) using an appropriate distance metric (L2 or cosine). This is followed by an exhaustive linear search across all data points in the cluster which gives the closest NN (see Figure 5 in Appendix A for IVF overview). Lastly, IVF can scale to web-scale by utilizing a hierarchical IVF structure within each cluster [16]. Table 2 in Appendix A describes the retrieval formula for multiple variants of IVF.', chunk_index=14, num_tokens=300, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_a1e35082-ee36-4c98-a7bc-8c6efd70f96b', content='Optimized Product Quantization (OPQ). Product Quantization (PQ) [23] works by splitting a d-dimensional real vector into m sub-vectors and quantizing each sub-vector with an independent 2b length codebook across the database. After PQ, each d-dimensional vector can be represented by a compact m   b bit vector; we make each vector m bytes long by fixing b = 8. During search time, distance computation between the query vector and PQ database is extremely efficient with only m codebook lookups. The generality of PQ encompasses scalar/vector quantization [15, 36] as special cases. However, PQ can be further improved by rotating the d-dimensional space appropriately to maximize distance preservation after PQ. Optimized Product Quantization (OPQ) [13] achieves this by learning an orthonormal projection matrix R that rotates the d-dimensional space to be more amenable to PQ. OPQ shows consistent gains over PQ across a variety of ANNS tasks and has become the default choice in standard composite indices [22, 24]. Datasets. We evaluate the ANNS algorithms while changing the representations used for the search thus making it impossible to evaluate on the usual benchmarks [1].', chunk_index=15, num_tokens=246, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_1c51f04a-b818-4d5e-8664-e33977f89594', content='Hence we experiment with two public datasets: (a) ImageNet-1K [45] dataset on the task of image retrieval   where the goal is to retrieve images from a database (1.3M image train set) belonging to the same class as the query image (50K image validation set) and (b) Natural Questions (NQ) [32] dataset on the task of question answering through dense passage retrieval   where the goal is to retrieve the relevant passage from a database (21M Wikipedia passages) for a query (3.6K questions). Metrics Performance of ANNS is often measured using recall score [22], k-recall@N   recall of the exact NN across search complexities which denotes the recall of k  true  NN when N data points are retrieved. However, the presence of labels allows us to compute 1-NN (top-1) accuracy. Top-1 accuracy is a harder and more fine-grained metric that correlates well with typical retrieval metrics like recall and mean average precision (mAP@k). Even though we report top-1 accuracy by default during experimentation, we discuss other metrics in Appendix C. Finally, we measure the compute overhead of ANNS using MFLOPS/query and also provide wall-clock times (see Appendix B.1). Encoders. For ImageNet, we encode both the database and query set using a ResNet50 ( I ) [19] trained on ImageNet-1K.', chunk_index=16, num_tokens=297, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_313cfb70-3a2e-487a-9c52-0f6ae87498e5', content='For NQ, we encode both the passages in the database and the questions in the query set using a BERT-Base ( N ) [10] model fine-tuned on NQ for dense passage retrieval [27]. We use the trained ResNet50 models with varying representation sizes (d = [8, 16, . . . , 2048]; default being 2048) as suggested by Kusupati et al. [31] alongside the MRL-ResNet50 models trained with MRL for the same dimensionalities. The RR and MR models are trained to ensure the supervised one-vs-all classification accuracy across all data dimensionalities is nearly the same   1-NN accuracy of 2048-d RR and MR models are 71.19% and 70.97% respectively on ImageNet-1K. Independently trained models,  RR(d) , output d = [8, 16 . . . , 2048] dimensional RRs while a single MRL-ResNet50 model,  MR(d) We also train BERT-Base models in a similar vein as the aforementioned ResNet50 models.', chunk_index=17, num_tokens=236, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_1dbeddc4-c06a-4378-ac28-943c7a5e9006', content='The key difference is that we take a pre-trained BERT-Base model and fine-tune on NQ as suggested by Karpukhin et al. [27] with varying (5) representation sizes (bottlenecks) (d = [48, 96, . . . , 768]; default being 768) to obtain  RR(d) that creates RRs for the NQ dataset. To get the MRL-BERT- Base model, we fine-tune a pre-trained BERT-Base encoder on the NQ train dataset using the MRL objective with the same granularities as RRs to obtain  MR(d) which contains all five granularities. Akin to ResNet50 models, the RR and MR BERT-Base models on NQ are built to have similar 1-NN accuracy for 768-d of 52.2% and 51.5% respectively. More implementation details can be found in Appendix B and additional experiment-specific information is provided at the appropriate places. , outputs a d = 2048-dimensional MR that contains all the 9 granularities. 4 AdANNS   Adaptive ANNS In this section, we present our proposed AdANNS framework that exploits the inherent flexibility of matryoshka representations to improve the accuracy-compute trade-off for semantic search com- ponents.', chunk_index=18, num_tokens=279, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_0aa92606-43ae-4155-a1d3-dfe1e4c46507', content='Standard ANNS pipeline can be split into two key components: (a) search data structure that indexes and stores data points, (b) query-point computation method that outputs (approximate) distance between a given query and data point. For example, standard IVFOPQ [24] method uses an IVF structure to index points on full-precision vectors and then relies on OPQ for more efficient distance computation between the query and the data points during the linear scan. Below, we show that AdANNS can be applied to both the above-mentioned ANNS components and provides significant gains on the computation-accuracy tradeoff curve. In particular, we present AdANNS-IVF which is AdANNS version of the standard IVF index structure [48], and the closely related ScaNN structure [16]. We also present AdANNS-OPQ which introduces representation adap- tivity in the OPQ, an industry-default quantization. Then, in Section 4.3 we further demonstrate the combination of the two techniques to get AdANNS-IVFOPQ   an AdANNS version of IVFOPQ [24]   and AdANNS-DiskANN, a similar variant of DiskANN [22].', chunk_index=19, num_tokens=252, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_b9986ff9-e5fa-4095-89a1-1a33929e66ee', content=\"Overall, our experiments show that AdANNS-IVF is significantly more accuracy-compute optimal compared to the IVF indices built on RRs and AdANNS-OPQ is as accurate as the OPQ on RRs while being significantly cheaper. 4.1 AdANNS-IVF 7 R S       $ F F X U D F \\\\ , 9 )   0 5  $ G $ 1 1 6   , 9 )  0 ) / 2 3 6   4 X H U \\\\ 0 *   , 9 )   5 5  $ G $ 1 1 6   , 9 )   '   , 9 )   5 5 0 *   , 9 )   6 9 ' Recall from Section 1 that IVF has a clustering and a linear scan phase, where both phase use same dimen- sional rigid representation. Now, AdANNS-IVF allows the clustering phase to use the first dc dimensions of the given matryoshka represen- tation (MR). Similarly, the linear scan within each cluster uses ds di- mensions, where again ds represents top ds coordinates from MR. Note that setting dc = ds results in non- adaptive regular IVF.\", chunk_index=20, num_tokens=274, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_3125d43d-6d34-4e7e-b856-b1c4d70ef33e', content='Intuitively, we would set dc   ds, so that instead of clustering with a high-dimensional representation, we can approximate it accurately with a low-dimensional em- bedding of size dc followed by a lin- ear scan with a higher ds-dimensional representation. Intuitively, this helps in the smooth search of design space for state-of-the-art accuracy-compute trade-off. Furthermore, this can provide a precise operating point on accuracy-compute tradeoff curve which is critical in several practical settings. Figure 2: 1-NN accuracy on ImageNet retrieval shows that AdANNS-IVF achieves near-optimal accuracy-compute trade-off compared across various rigid and adaptive base- lines. Both adaptive variants of MR and RR significantly outperform their rigid counterparts (IVF-XX) while post-hoc compression on RR using SVD for adaptivity falls short. Our experiments on regular IVF with MRs and RRs (IVF-MR & IVF-RR) of varying dimensionali- ties and IVF configurations (# clusters, # probes) show that (Figure 2) matryoshka representations result in a significantly better accuracy-compute trade-off. We further studied and found that learned lower-dimensional representations offer better accuracy-compute trade-offs for IVF than higher- dimensional embeddings (see Appendix E for more results). AdANNS utilizes d-dimensional matryoshka representation to get accurate dc and ds dimensional vectors at no extra compute cost.', chunk_index=21, num_tokens=294, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_ba8d9953-0af8-4606-b30e-a6106cae02b8', content='The resulting AdANNS-IVF provides a much better accuracy- compute trade-off (Figure 2) on ImageNet-1K retrieval compared to IVF-MR, IVF-RR, and MG- IVF-RR   multi-granular IVF with rigid representations (akin to AdANNS without MR)   a strong baseline that uses dc and ds dimensional RRs. Finally, we exhaustively search the design space of IVF by varying dc, ds   [8, 16, . . . , 2048] and the number of clusters k   [8, 16, . . . , 2048]. Please see Appendix E for more details. For IVF experiments on the NQ dataset, please refer to Appendix G. Empirical results. Figure 2 shows that AdANNS-IVF outperforms the baselines across all accuracy-compute settings for ImageNet-1K retrieval. AdANNS-IVF results in 10  lower compute for the best accuracy of the extremely expensive MG-IVF-RR and non-adaptive IVF-MR. Specifi- cally, as shown in Figure 1a, AdANNS-IVF is up to 1.5% more accurate for the same compute and has up to 100  lesser FLOPS/query (90  real-world speed-up!) than the status quo ANNS on rigid representations (IVF-RR).', chunk_index=22, num_tokens=297, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_b6dfb329-98db-4381-9f8b-c7ebaeb81e47', content='We filter out points for the sake of presentation and encourage the reader to check out Figure 8 in Appendix E for an expansive plot of all the configurations searched. The advantage of AdANNS for construction of search structures is evident from the improvements in IVF (AdANNS-IVF) and can be easily extended to other ANNS structures like ScaNN [16] and HNSW [38]. For example, HNSW consists of multiple layers with graphs of NSW graphs [37] of increasing complexity. AdANNS can be adopted to HNSW, where the construction of each level can be powered by appropriate dimensionalities for an optimal accuracy-compute trade-off. In general, AdANNS provides fine-grained control over compute overhead (storage, working memory, inference, and construction cost) during construction and inference while providing the best possible accuracy. 4.2 AdANNS-OPQ Standard Product Quantization (PQ) essentially performs block-wise vector quantization via cluster- ing. For example, suppose we need 32-byte PQ compressed vectors from the given 2048 dimensional representations. Then, we can chunk the representations in m = 32 equal blocks/sub-vectors of 64-d each, and each sub-vector space is clustered into 28 = 256 partitions. That is, the representation of each point is essentially cluster-id for each block.', chunk_index=23, num_tokens=280, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_22c81c95-29e6-4842-8556-b1f90ce71a1b', content='Optimized PQ (OPQ) [13] further refines this idea, by first rotating the representations using a learned orthogonal matrix, and then applying PQ on top of the rotated representations. In ANNS, OPQ is used extensively to compress vectors and improves approximate distance computation primarily due to significantly lower memory overhead than storing full-precision data points IVF. AdANNS-OPQ utilizes MR representations to apply OPQ on lower-dimensional representations. That is, for a given quantization budget, AdANNS allows using top ds   d dimensions from MR and then computing clusters with ds/m-dimensional blocks where m is the number of blocks. Depending on ds and m, we have further flexibility of trading-off dimensionality/capacity for increasing the number of clusters to meet the given quantization budget. AdANNS-OPQ tries multiple ds, m, and number of clusters for a fixed quantization budget to obtain the best performing configuration. We experimented with 8   128 byte OPQ budgets for both ImageNet and Natural Questions retrieval with an exhaustive search on the quantized vectors. We compare AdANNS-OPQ which uses MRs of varying granularities to the baseline OPQ built on the highest dimensional RRs. We also evaluate OPQ vectors obtained projection using SVD [14] on top of the highest-dimensional RRs. Empirical results.', chunk_index=24, num_tokens=277, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_dfedb996-f3fe-4f34-b213-161d0395ebac', content='Figures 3 and 1b show that AdANNS-OPQ significantly outperforms   up to 4% accuracy gain   the baselines (OPQ on RRs) across compute budgets on both ImageNet and NQ. In particular, AdANNS-OPQ tends to match the accuracy of a 64-byte (a typical choice in ANNS) OPQ baseline with only a 32-byte budget. This results in a 2  reduction in both storage and compute FLOPS which translates to significant gains in real-world web-scale deployment (see Appendix D). We only report the best AdANNS-OPQ for each budget typically obtained through a much lower- dimensional MR (128 & 192; much faster to build as well) than the highest-dimensional MR (2048 & 768) for ImageNet and NQ respectively (see Appendix G for more details).', chunk_index=25, num_tokens=183, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_d7062fdd-12dd-4af7-9883-c0abfc224923', content=\"At the same time, we 2 3 4   5 5  & R P S X W H   % X G J H W     % \\\\ W H V 7 R S       $ F F X U D F \\\\     2 3 4   5 5   6 9 '  $ G $ 1 1 6   2 3 4       7 R S       $ F F X U D F \\\\ 5 L J L G   , 9 ) 2 3 4 $ G $ 1 1 6   , 9 ) 2 3 4  & R P S X W H   % X G J H W     % \\\\ W H V Figure 3: AdANNS-OPQ matches the accuracy of 64-byte OPQ on RR using only 32-bytes for ImageNet retrieval. AdANNS provides large gains at lower compute budgets and saturates to baseline performance for larger budgets. Figure 4: Combining the gains of AdANNS for IVF and OPQ leads to better IVFOPQ compos- ite indices.\", chunk_index=26, num_tokens=241, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_c33a09d4-90f6-4c22-816b-a62c7051ce03', content='On ImageNet retrieval, AdANNS- IVFOPQ is 8  cheaper for the same accuracy and provides 1 - 4% gains over IVFOPQ on RRs. note that building compressed OPQ vectors on projected RRs using SVD to the smaller dimensions (or using low-dimensional RRs, see Appendix D) as the optimal AdANNS-OPQ does not help in improving the accuracy. The significant gains we observe in AdANNS-OPQ are purely due to better information packing in MRs   we hypothesize that packing the most important information in the initial coordinates results in a better PQ quantization than RRs where the information is uniformly distributed across all the dimensions [31, 49]. See Appendix D for more details and experiments. 4.3 AdANNS for Composite Indices We now extend AdANNS to composite indices [24] which put together two main ANNS building blocks   search structures and quantization   together to obtain efficient web-scale ANNS indices used in practice. A simple instantiation of a composite index would be the combination of IVF and OPQ   IVFOPQ   where the clustering in IVF happens with full-precision real vectors but the linear scan within each cluster is approximated using OPQ-compressed variants of the representation   since often the full-precision vectors of the database cannot fit in RAM.', chunk_index=27, num_tokens=280, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_e0102d16-597a-4d03-95cd-c667d4a0e0fc', content='Contemporary ANNS indices like DiskANN [22] make this a default choice where they build the search graph with a full-precision vector and approximate the distance computations during search with an OPQ-compressed vector to obtain a very small shortlist of retrieved datapoints. In DiskANN, the shortlist of data points is then re-ranked to form the final list using their full-precision vectors fetched from the disk. AdANNS is naturally suited to this shortlist-rerank framework: we use a low-d MR for forming index, where we could tune AdANNS parameters according to the accuracy-compute trade-off of the graph and OPQ vectors. We then use a high-d MR for re-ranking. Empirical results. Figure 4 shows that AdANNS-IVFOPQ is 1   4% better than the baseline at all the PQ compute budgets. Furthermore, AdANNS-IVFOPQ has the same ac- curacy as the baselines at 8  lower overhead. With DiskANN, AdANNS accelerates shortlist generation by us- ing low-dimensional representations and recoups the accuracy by re- ranking with the highest-dimensional MR at negligible cost. Table 1 shows that AdANNS-DiskANN is more accurate than the baseline for both 1-NN and ranking performance at only half the cost. Using low-dimensional representations further speeds up inference in AdANNS-DiskANN (see Appendix F).', chunk_index=28, num_tokens=297, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_44195435-d22a-46a6-ac29-3df28476ef10', content='Table 1: AdANNS-DiskANN using a 16-d MR + re-ranking with the 2048-d MR outperforms DiskANN built on 2048-d RR at half the compute cost on ImageNet retrieval. RR-2048 AdANNS 16 70.56 64.70 68.25 PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%) 32 70.37 62.46 65.65 These results show the generality of AdANNS and its broad applicability across a variety of ANNS indices built on top of the base building blocks. Currently, AdANNS piggybacks on typical ANNS pipelines for their inherent accounting of the real-world system constraints [16, 22, 25]. However, we believe that AdANNS s flexibility and significantly better accuracy-compute trade-off can be further informed by real-world deployment constraints. We leave this high-potential line of work that requires extensive study to future research. 5 Further Analysis and Discussion 5.1 Compute-aware Elastic Search During Inference AdANNS search structures cater to many specific large-scale use scenarios that need to satisfy precise resource constraints during construction as well as inference. However, in many cases, construction and storage of the indices are not the bottlenecks or the user is unable to search the design space.', chunk_index=29, num_tokens=284, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_7624b93e-b46c-442b-8040-526a825fd62c', content='In these settings, AdANNS-D enables adaptive inference through accurate yet cheaper distance computation using the low-dimensional prefix of matryoshka representation. Akin to composite indices (Section 4.3) that use PQ vectors for cheaper distance computation, we can use the low- dimensional MR for faster distance computation on ANNS structure built non-adaptively with a high-dimensional MR without any modifications to the existing index. Empirical results. Figure 2 shows that for a given compute budget using IVF on ImageNet-1K retrieval, AdANNS-IVF is better than AdANNS-IVF-D due to the explicit control during the building of the ANNS structure which is expected. However, the interesting observation is that AdANNS-D matches or outperforms the IVF indices built with MRs of varying capacities for ImageNet retrieval. However, these methods are applicable in specific scenarios of deployment. Obtaining optimal AdANNS search structure (highly accurate) or even the best IVF-MR index relies on a relatively expensive design search but delivers indices that fit the storage, memory, compute, and accuracy constraints all at once. On the other hand AdANNS-D does not require a precisely built ANNS index but can enable compute-aware search during inference.', chunk_index=30, num_tokens=260, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_b6e14c82-1b77-4adb-8b46-09feff7f2886', content='AdANNS-D is a great choice for setups that can afford only one single database/index but need to cater to varying deployment constraints, e.g., one task requires 70% accuracy while another task has a compute budget of 1 MFLOPS/query. 5.2 Why MRs over RRs? Quite a few of the gains from AdANNS are owing to the quality and capabilities of matryoshka representations. So, we conducted extensive analysis to understand why matryoshka representations seem to be more aligned for semantic search than the status-quo rigid representations. Difficulty of NN search. Relative contrast (Cr) [18] is inversely proportional to the difficulty of nearest neighbor search on a given database. On ImageNet-1K, Figure 14 shows that MRs have better Cr than RRs across dimensionalities, further supporting that matryoshka representations are more aligned (easier) for NN search than existing rigid representations for the same accuracy. More details and analysis about this experiment can be found in Appendix H.2. Clustering distributions. We also investigate the potential deviation in clustering distributions for MRs across dimensionalities compared to RRs. Unlike the RRs where the information is uniformly diffused across dimensions [49], MRs have hierarchical information packing.', chunk_index=31, num_tokens=260, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_e694774b-1a3a-43a5-bd9b-1c046ac40c41', content='Figure 11 in Appendix E.3 shows that matryoshka representations result in clusters similar (measured by total variation distance [33]) to that of rigid representations and do not result in any unusual artifacts. Robustness. Figure 9 in Appendix E shows that MRs continue to be better than RRs even for out-of- distribution (OOD) image queries (ImageNetV2 [44]) using ANNS. It also shows that the highest data dimensionality need not always be the most robust which is further supported by the higher recall using lower dimensions. Further details about this experiment can be found in Appendix E.1. Generality across encoders. IVF-MR consistently has higher accuracy than IVF-RR across dimen- sionalities despite having similar accuracies with exact NN search (for ResNet50 on ImageNet and BERT-Base on NQ). We find that our observations on better alignment of MRs for NN search hold across neural network architectures, ResNet18/34/101 [19] and ConvNeXt-Tiny [35]. Appendix H.3 delves deep into the experimentation done using various neural architectures on ImageNet-1K. Recall score analysis.', chunk_index=32, num_tokens=247, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_cf00c6f0-cd7f-4e13-ae77-fc3e952013d9', content='Analysis of recall score (see Appendix C) in Appendix H.1 shows that for a similar top-1 accuracy, lower-dimensional representations have better 1-Recall@1 across search complexities for IVF and HNSW on ImageNet-1K. Across the board, MRs have higher recall scores and top-1 accuracy pointing to easier  searchability  and thus suitability of matryoshka representations for ANNS. Larger-scale experiments and further analysis can be found in Appendix H. Through these analyses, we argue that matryoshka representations are better suited for semantic search than rigid representations, thus making them an ideal choice for AdANNS. 5.3 Search for AdANNS Hyperparameters Choosing the optimal hyperparameters for AdANNS, such as dc, ds, m, # clusters, # probes, is an interesting and open problem that requires more rigorous examination. As the ANNS index is formed once and used for potentially billions of queries with massive implications for cost, latency and queries-per-second, a hyperparameter search for the best index is generally an acceptable industry practice [22, 38]. The Faiss library [24] provides guidelines2 to choose the appropriate index for a specific problem, including memory constraints, database size, and the need for exact results.', chunk_index=33, num_tokens=262, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_7e857c6f-a759-42fd-979c-516c65380c79', content='There have been efforts at automating the search for optimal indexing parameters, such as Autofaiss3, which maximizes recall given compute constraints. 2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss In case of AdANNS, we suggest starting at the best configurations of MRs followed by a local design space search to lead to near-optimal AdANNS configurations (e.g. use IVF-MR to bootstrap AdANNS-IVF). We also share some observations during the course of our experiments: 1. AdANNS-IVF: Top-1 accuracy generally improves (with diminishing returns after a point) with increasing dimensionality of clustering (dc) and search (ds), as we show on ImageNet variants and with multiple encoders in the Appendix (Figures 9 and 15). Clustering with low-d MRs matches the performance of high-d MRs as they likely contain similar amounts of useful information, making the increased compute cost not worth the marginal gains. Increasing # probes naturally boosts performance (Appendix, Figure 10a).', chunk_index=34, num_tokens=239, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_99766a49-5db4-4b0f-87cb-93d176a5f39c', content='Lastly, it is generally accepted that a good starting point for the # clusters k is (cid:112)ND/2, where ND is the number of indexable items [39]. k = ND is the optimal choice of k from a FLOPS computation perspective as can be seen in Appendix B.1. 2. AdANNS-OPQ: we observe that for a fixed compute budget in bytes (m), the top-1 accuracy reaches a peak at d < dmax (Appendix, Table 4). We hypothesize that the better performance of AdANNS-OPQ at d < dmax is due to the curse of dimensionality, i.e. it is easier to learn PQ codebooks on smaller embeddings with similar amounts of information. We find that using an MR with d = 4   m is a good starting point on ImageNet and NQ. We also suggest using an 8-bit (256-length) codebook for OPQ as the default for each of the sub-block quantizer. 3. AdANNS-DiskANN: Our observations with DiskANN are consistent with other indexing struc- tures, i.e. the optimal graph construction dimensionality d < dmax (Appendix, Figure 12).', chunk_index=35, num_tokens=257, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_a951ab05-735a-4a76-a9c8-110c4bd1500b', content='A careful study of DiskANN on different datasets is required for more general guidelines to choose graph construction and OPQ dimensionality d. 5.4 Limitations AdANNS s core focus is to improve the design of the existing ANNS pipelines. To use AdANNS on a corpus, we need to back-fill [43] the MRs of the data   a significant yet a one-time overhead. We also notice that high-dimensional MRs start to degrade in performance when optimizing also for an extremely low-dimensional granularity (e.g., < 24-d for NQ)   otherwise is it quite easy to have comparable accuracies with both RRs and MRs. Lastly, the existing dense representations can only in theory be converted to MRs with an auto-encoder-style non-linear transformation. We believe most of these limitations form excellent future work to improve AdANNS further. 6 Conclusions We proposed a novel framework, AdANNS , that leverages adaptive representations for different phases of ANNS pipelines to improve the accuracy-compute tradeoff. AdANNS utilizes the inherent flexibility of matryoshka representations [31] to design better ANNS building blocks than the standard ones which use the rigid representation in each phase. AdANNS achieves SOTA accuracy-compute trade-off for the two main ANNS building blocks: search data structures (AdANNS-IVF) and quantization (AdANNS-OPQ).', chunk_index=36, num_tokens=290, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_525bfe25-e81b-488c-b982-1a4fe79481fe', content='The combination of AdANNS-based building blocks leads to the construction of better real-world composite ANNS indices   with as much as 8  reduction in cost at the same accuracy as strong baselines   while also enabling compute-aware elastic search. Finally, we note that combining AdANNS with elastic encoders [11] enables truly adaptive large-scale retrieval. Acknowledgments We are grateful to Kaifeng Chen, Venkata Sailesh Sanampudi, Sanjiv Kumar, Harsha Vardhan Simhadri, Gantavya Bhatt, Matthijs Douze and Matthew Wallingford for helpful discussions and feedback. Aditya Kusupati also thanks Tom Duerig and Rahul Sukthankar for their support. Part of the paper s large-scale experimentation is supported through a research GCP credit award from Google Cloud and Google Research. Sham Kakade acknowledges funding from the ONR award N00014-22-1-2377 and NSF award CCF-2212841. Ali Farhadi acknowledges funding from the NSF awards IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543, and gifts from Allen Institute for Artificial Intelligence and Google. References [1] M. Aum ller, E. Bernhardsson, and A. Faithfull. Ann-benchmarks:', chunk_index=37, num_tokens=296, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_35b3b98b-a54f-418f-90cb-f06b93a55d58', content='A benchmarking tool for approximate nearest neighbor algorithms. Information Systems, 87:101374, 2020. [2] Y. Bengio. Deep learning of representations for unsupervised and transfer learning. In Proceed- ings of ICML workshop on unsupervised and transfer learning, pages 17 36. JMLR Workshop and Conference Proceedings, 2012. [3] E. Bernhardsson. Annoy: Approximate Nearest Neighbors in C++/Python, 2018. URL https://pypi.org/project/annoy/. Python package version 1.13.0. [4] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. Computer networks and ISDN systems, 30(1-7):107 117, 1998. [5] D. Cai. A revisit of hashing algorithms for approximate nearest neighbor search. IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897. [6] T. Chen, L. Li, and Y. Sun. Differentiable product quantization for end-to-end embedding compression. In International Conference on Machine Learning, pages 1617 1626. PMLR, 2020. [7] K. L. Clarkson.', chunk_index=38, num_tokens=295, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_7ddd8ee8-3e80-48d2-9bc4-f25fde0bb5c9', content='An algorithm for approximate closest-point queries. In Proceedings of the tenth annual symposium on Computational geometry, pages 160 164, 1994. [8] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational geometry, pages 253 262, 2004. [9] J. Dean. Challenges in building large-scale information retrieval systems. In Keynote of the 2nd ACM International Conference on Web Search and Data Mining (WSDM), volume 10, 2009. [10] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [11] Devvrit, S. Kudugunta, A. Kusupati, T. Dettmers, K. Chen, I. Dhillon, Y. Tsvetkov, H. Hannaneh, S. Kakade, A. Farhadi, and P. Jain. Matformer:', chunk_index=39, num_tokens=264, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_a938fd4c-f047-402f-91d6-25b769c9c9a9', content='Nested transformer for elastic inference. arXiv preprint arxiv:2310.07707, 2023. [12] J. H. Friedman, J. L. Bentley, and R. A. Finkel. An algorithm for finding best matches in logarithmic expected time. ACM Transactions on Mathematical Software (TOMS), 3(3):209  226, 1977. [13] T. Ge, K. He, Q. Ke, and J. Sun. Optimized product quantization for approximate nearest neighbor search. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2946 2953, 2013. [14] G. Golub and W. Kahan. Calculating the singular values and pseudo-inverse of a matrix. Journal of the Society for Industrial and Applied Mathematics, Series B: Numerical Analysis, 2(2): 205 224, 1965. [15] R. Gray. Vector quantization. IEEE Assp Magazine, 1(2):4 29, 1984. [16] R. Guo, P. Sun, E. Lindgren, Q. Geng, D. Simcha, F. Chern, and S. Kumar. Accelerating large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning, pages 3887 3896.', chunk_index=40, num_tokens=288, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_5878c9fe-4261-4fa0-95d6-44ebec919d3a', content='PMLR, 2020. [17] N. Gupta, P. H. Chen, H.-F. Yu, C.-J. Hsieh, and I. S. Dhillon. End-to-end learning to index and search in large output spaces. arXiv preprint arXiv:2210.08410, 2022. [18] J. He, S. Kumar, and S.-F. Chang. On the difficulty of nearest neighbor search. In International Conference on Machine Learning (ICML), 2012. [19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770  778, 2016. [20] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729 9738, 2020. [21] P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 604 613, 1998. [22] S.', chunk_index=41, num_tokens=290, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_576360e5-34f0-4335-8cac-edb724efa997', content='Jayaram Subramanya, F. Devvrit, H. V. Simhadri, R. Krishnawamy, and R. Kadekodi. Diskann: Fast accurate billion-point nearest neighbor search on a single node. Advances in Neural Information Processing Systems, 32, 2019. [23] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010. [24] J. Johnson, M. Douze, and H. J gou. Billion-scale similarity search with GPUs. Transactions on Big Data, 7(3):535 547, 2019. [25] W. B. Johnson. Extensions of lipschitz mappings into a hilbert space. Contemp. Math., 26: 189 206, 1984. [26] I. T. Jolliffe and J. Cadima. Principal component analysis: a review and recent developments. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065):20150202, 2016. [27] V. Karpukhin, B. O guz, S. Min, P. Lewis, L. Wu, S. Edunov, D.', chunk_index=42, num_tokens=286, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_f1f9543c-8801-4fe5-b6e5-a51f14cfaefb', content='Chen, and W.-t. Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. [28] S. Kornblith, J. Shlens, and Q. V. Le. Do better imagenet models transfer better? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2661 2671, 2019. [29] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned index structures. In Proceedings of the 2018 international conference on management of data, pages 489 504, 2018. [30] A. Kusupati, M. Wallingford, V. Ramanujan, R. Somani, J. S. Park, K. Pillutla, P. Jain, S. Kakade, and A. Farhadi. Llc: Accurate, multi-purpose learnt low-dimensional binary codes. Advances in Neural Information Processing Systems, 34:23900 23913, 2021. [31] A. Kusupati, G. Bhatt, A. Rege, M. Wallingford, A. Sinha, V. Ramanujan, W.', chunk_index=43, num_tokens=290, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_1e3ed774-e946-45c4-9fe3-b6615fadae2f', content='Howard-Snyder, K. Chen, S. Kakade, P. Jain, and A. Farhadi. Matryoshka representation learning. In Advances in Neural Information Processing Systems, December 2022. [32] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453 466, 2019. [33] D. A. Levin and Y. Peres. Markov chains and mixing times, volume 107. American Mathematical Soc., 2017. [34] W. Li, Y. Zhang, Y. Sun, W. Wang, W. Zhang, and X. Lin. Approximate nearest neighbor search on high dimensional data experiments, analyses, and improvement. IEEE Transactions on Knowledge and Data Engineering, 2020. [35] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie. A convnet for the 2020s.', chunk_index=44, num_tokens=265, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_b62bdd98-0090-4f44-bb58-6be5b28825e9', content='In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11976 11986, 2022. [36] S. Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2): 129 137, 1982. IEEE [37] Y. Malkov, A. Ponomarenko, A. Logvinov, and V. Krylov. Approximate nearest neighbor algorithm based on navigable small world graphs. Information Systems, 45:61 68, 2014. [38] Y. A. Malkov and D. Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis & Machine Intelligence, 42(04):824 836, 2020. [39] K. Mardia, J. Kent, and J. Bibby. Multivariate analysis. Probability and Mathematical Statistics, 1979. [40] P. Nayak. Understanding searches better than ever before. Google AI Blog, 2019. URL https: //blog.google/products/search/search-language-understanding-bert/. [41] A. Neelakantan, T. Xu, R. Puri, A. Radford, J. M. Han, J. Tworek, Q. Yuan, N. Tezak, J. W.', chunk_index=45, num_tokens=291, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_3b604f9a-f9a8-4601-99e4-b7926c01e1b9', content='Kim, C. Hallacy, et al. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005, 2022. [42] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748 8763. PMLR, 2021. [43] V. Ramanujan, P. K. A. Vasu, A. Farhadi, O. Tuzel, and H. Pouransari. Forward compatible training for representation learning. arXiv preprint arXiv:2112.02805, 2021. [44] B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning, pages 5389 5400. PMLR, 2019. [45] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A.', chunk_index=46, num_tokens=294, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_de34f950-2509-4d4e-80d2-ad1a8145d5c1', content='Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211 252, 2015. [46] R. Salakhutdinov and G. Hinton. Semantic hashing. International Journal of Approximate Reasoning, 50(7):969 978, 2009. [47] H. V. Simhadri, G. Williams, M. Aum ller, M. Douze, A. Babenko, D. Baranchuk, Q. Chen, L. Hosseini, R. Krishnaswamy, G. Srinivasa, et al. Results of the neurips 21 challenge on billion-scale approximate nearest neighbor search. arXiv preprint arXiv:2205.03763, 2022. [48] J. Sivic and A. Zisserman. Video google: A text retrieval approach to object matching in videos. In Computer Vision, IEEE International Conference on, volume 3, pages 1470 1470. IEEE Computer Society, 2003. [49] D. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N. Srebro. The implicit bias of gradient descent on separable data.', chunk_index=47, num_tokens=281, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_b184a1ac-9477-41bd-8491-abc7f1c38c60', content='The Journal of Machine Learning Research, 19(1):2822 2878, 2018. [50] C. Waldburger. As search needs evolve, microsoft makes ai tools for better search available to researchers and developers. Microsoft AI Blog, 2019. URL https://blogs.microsoft. com/ai/bing-vector-search/. [51] M. Wang, X. Xu, Q. Yue, and Y. Wang. A comprehensive survey and experimental comparison of graph-based approximate nearest neighbor search. Proceedings of the VLDB Endowment, 14 (11):1964 1978, 2021. [52] R. Weber, H.-J. Schek, and S. Blott. A quantitative analysis and performance study for similarity- search methods in high-dimensional spaces. In VLDB, volume 98, pages 194 205, 1998. [53] I. H. Witten, I. H. Witten, A. Moffat, T. C. Bell, T. C. Bell, E. Fox, and T. C. Bell. Managing gigabytes: compressing and indexing documents and images. Morgan Kaufmann, 1999.', chunk_index=48, num_tokens=247, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_56a6b69d-a1a2-4359-bfcc-6e178b11cbb6', content='A AdANNS Framework Algorithm 1 AdANNS-IVF Psuedocode # Index database to construct clusters and build inverted file system def adannsConstruction(database, d_cluster, num_clusters): # Slice database with cluster construction dim (d_cluster) xb = database[:d_cluster] cluster_centroids = constructClusters(xb, num_clusters) return cluster_centroids def adannsInference(queries, centroids, d_shortlist, d_search, num_probes, k): # Slice queries and centroids with cluster shortlist dim (d_shortlist) xq = queries[:d_shortlist] xc = centroids[:d_shortlist] for q in queries: # compute distance of query from each cluster centroid candidate_distances = computeDistances(q, xc) # sort cluster candidates by distance and choose small number to probe cluster_candidates = sortAscending(candidate_distances)[:num_probes] database_candidates = getClusterMembers(cluster_candidates) # Linear Scan all shortlisted clusters with search dim (d_search) k_nearest_neighbors[q] = linearScan(q, database_candidates, d_search, return k_nearest_neighbors DatabaseConstruct Clusters with  dc Select Closest Cluster :   ! k  Ck ANNS ConstructionANNS Inference Linear Scan with  ds 1  C1 Query Top k Relevant Data Points i  Ci Figure 5: The schematic of inverted file index (IVF) outlaying the construction and inference phases.', chunk_index=49, num_tokens=291, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_e17275c3-5647-4d32-96b7-909088cbcc69', content='Adaptive representations can be utilized effectively in the decoupled components of clustering and searching for a better accuracy-compute trade-off (AdANNS-IVF). Table 2: Mathematical formulae of the retrieval phase across various methods built on IVF. See Section 3 for notations.', chunk_index=50, num_tokens=60, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_34d2f024-a0ad-423e-bbea-5806eff44d81', content='Method Retrieval Formula during Inference IVF-RR IVF-MR AdANNS-IVF MG-IVF-RR RR(d)(q)    RR(d)(xj) , s.t. h(q) = arg minh   RR(d)(q)    RR(d)     MR(d)(q)    MR(d)(xj) , s.t. h(q) = arg minh   MR(d)(q)    MR(d)   MR(ds)(q)    MR(ds)(xj) , s.t. h(q) = arg minh   MR(dc)(q)    MR(dc)   RR(ds)(q)    RR(ds)(xj) , s.t. h(q) = arg minh   RR(dc)(q)    RR(dc) arg minj Ch(q) arg minj Ch(q) arg minj Ch(q) arg minj Ch(q) AdANNS-IVF-D arg minj Ch(q) IVFOPQ MR(d)(q)[1 :  d]    MR(d)(xj)[1 :  d] , s.t. h(q) = arg minh   MR(d)(q)[1 :  d]    MR(d) arg minj Ch(q) PQ(m,b)(q)    PQ(m,b)(xj) , s.t. h(q) = arg minh   (q)    h B Training and Compute Costs A bulk of our ANNS experimentation was written with Faiss [24], a library for efficient similarity search and clustering.', chunk_index=51, num_tokens=317, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_3c38731c-d686-466b-885c-e4867795d16f', content='AdANNS was implemented from scratch (Algorithm 1) due to difficulty in decoupling clustering and linear scan with Faiss, with code available at https://github.com/ RAIVNLab/AdANNS. We also provide a version of AdANNS with Faiss optimizations with the restriction that Dc   Ds as a limitation of the current implementation, which can be further optimized. All ANNS experiments (AdANNS-IVF, MG-IVF-RR, IVF-MR, IVF-RR, HNSW, HNSWOPQ, IVFOPQ) were run on an Intel Xeon 2.20GHz CPU with 12 cores. Exact Search (Flat L2, PQ, OPQ) and DiskANN experiments were run with CUDA 11.0 on a A100-SXM4 NVIDIA GPU with 40G RAM. The wall-clock inference times quoted in Figure 1a and Table 3 are reported on CPU with Faiss optimizations, and are averaged over three inference runs for ImageNet-1K retrieval. Table 3: Comparison of AdANNS-IVF and Rigid-IVF wall-clock inference times for ImageNet-1K retrieval. AdANNS-IVF has up to   1.5% gain over Rigid-IVF for a fixed search latency per query.', chunk_index=52, num_tokens=281, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_af5e68ae-f4c1-4af9-b726-07247f78b231', content='AdANNS-IVF Rigid-IVF Top-1 Search Latency/Query (ms) Top-1 Search Latency/Query (ms) 70.02 70.08 70.19 70.36 70.60 0.03 0.06 0.06 0.88 5.57 68.51 68.54 68.74 69.20 70.13 0.02 0.05 0.08 0.86 5.67 DPR [27] on NQ [32]. We follow the setup on the DPR repo4: the Wikipedia corpus has 21 million passages and Natural Questions dataset for open-domain QA settings. The training set contains 79,168 question and answer pairs, the dev set has 8,757 pairs and the test set has 3,610 pairs. B.1 Inference Compute Cost We evaluate inference compute costs for IVF in MegaFLOPS per query (MFLOPS/query) as shown in Figures 2, 10a, and 8 as follows: C = dsk + npdsND k where dc is the cluster construction embedding dimensionality, ds is the embedding dim used for linear scan within each probed cluster, which is controlled by # of search probes np. Finally, k is the number of clusters |Ci| indexed over database of size ND.', chunk_index=53, num_tokens=291, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_5c66273d-68fd-4e5b-832b-aacd34eedad6', content='The default setting in this work, unless otherwise stated, is np = 1, k = 1024, ND = 1281167 (ImageNet-1K trainset). Vanilla IVF supports only dc = ds, while AdANNS-IVF provides flexibility via decoupling clustering and search (Section 4). AdANNS-IVF-D is a special case of AdANNS-IVF with the flexibility restricted to inference, i.e., dc is a fixed high-dimensional MR. 4https://github.com/facebookresearch/DPR [1 :  d] C Evaluation Metrics In this work, we primarily use top-1 accuracy (i.e. 1-Nearest Neighbor), recall@k, corrected mean average precision (mAP@k) [30] and k-Recall@N (recall score), which are defined over all queries Q over indexed database of size ND as: top-1 = (cid:80) Q correct_pred@1 |Q| Recall@k = (cid:80) Q correct_pred@k |Q| num_classes |ND| where correct_pred@k is the number of k-NN with correctly predicted labels for a given query. As noted in Section 3, k-Recall@N is the overlap between k exact search nearest neighbors (considered as ground truth) and the top N retrieved documents.', chunk_index=54, num_tokens=284, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_8f04fadc-e810-4cd5-9580-acf7ee47ec06', content='As Faiss [24] supports a maximum of 2048- NN while searching the indexed database, we report 40-Recall@2048 in Figure 13. Also note that for ImageNet-1K, which constitutes a bulk of the experimentation in this work, |Q| = 50000, |ND| = 1281167 and num_classes = 1000. For ImageNetv2 [44], |Q| = 10000 and num_classes = 1000, and for ImageNet-4K [31], |Q| = 210100, |ND| = 4202000 and num_classes = 4202. For NQ [32], |Q| = 3610 and |ND| = 21015324.', chunk_index=55, num_tokens=161, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_22a5664a-89c0-4d13-afe9-e2f3b159abb3', content=\"As NQ consists of question-answer pairs (instance- level), num_classes = 3610 for the test set. 5 L J L G   2 3 4  2 3 4   0 5 7 R S       $ F F X U D F \\\\ $ G $ 1 1 6   2 3 4   & R P S X W H   % X G J H W     % \\\\ W H V   $ G $ 1 1 6   , 9 )   2 3 4  , 9 )   2 3 4   0 5 & R P S X W H   % X G J H W     % \\\\ W H V  7 R S       $ F F X U D F \\\\  5 L J L G   , 9 )   2 3 4   (a) Exact Search + OPQ on ImageNet-1K (b) IVF + OPQ on ImageNet-1K 7 R S       $ F F X U D F \\\\ 5 L J L G   ' L V N $ 1 1   2 3 4    ' L V N $ 1 1   2 3 4   0 5  $ G $ 1 1 6   ' L V N $ 1 1   2 3 4 & R P S X W H   % X G J H W     % \\\\ W H V  7 R S       $ F F X U D F \\\\     & R P S X W H   % X G J H W     % \\\\ W H V  $ G $ 1 1 6   + 1 6 :   2 3 4 + 1 6 :   2 3 4   0 5 5 L J L G   + 1 6 :   2 3 4 (c) DiskANN + OPQ on ImageNet-1K (d) HNSW + OPQ on ImageNet-1K Figure 6:\", chunk_index=56, num_tokens=455, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_bce5929a-2f76-4413-a317-f8b7e11e51fe', content='Top-1 Accuracy of AdANNS composite indices with OPQ distance computation compared to MR and Rigid baselines models on ImageNet-1K and Natural Questions. D AdANNS-OPQ In this section, we take a deeper dive into the quantization characteristics of MR. In this work, we restrict our focus to optimized product quantization (OPQ) [13], which adds a learned space rotation and dimensionality permutation to PQ s sub-vector quantization to learn more optimal PQ codes. We compare OPQ to vanilla PQ on ImageNet in Table 4, and observe large gains at larger embedding dimensionalities, which agrees with the findings of Jayaram Subramanya et al. [22]. We perform a study of composite OPQ m   b indices on ImageNet-1K across compression compute budgets m (where b = 8, i.e. 1 Byte), i.e. Exact Search with OPQ, IVF+OPQ, HNSW+OPQ, and DiskANN+OPQ, as seen in Figure 6. It is evident from these results: 1. Learning OPQ codebooks with AdANNS (Figure 6a) provides a 1-5% gain in top-1 accuracy over rigid representations at low compute budgets (  32 Bytes). AdANNS-OPQ saturates to Rigid-OPQ performance at low compression (  64 Bytes). 2.', chunk_index=57, num_tokens=295, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_0ecfdbdf-8cd9-4444-bde3-5c42174dfef3', content='For IVF, learning clusters with MRs instead of RRs (Figure 6b) provides substantial gains (1- 4%). In contrast to Exact-OPQ, using AdANNS for learning OPQ codebooks does not provide substantial top-1 accuracy gains over MR with d = 2048 (highest), though it is still slightly better or equal to MR-2048 at all compute budgets. This further supports that IVF performance generally scales with embedding dimensionality, which is consistent with our findings on ImageNet across robustness variants and encoders (See Figures 9 and 15 respectively). 3. Note that in contrast to Exact, IVF, and HNSW coarse quantizers, DiskANN inherently re-ranks the retrieved shortlist with high-precision embeddings (d = 2048), which is reflected in its high top-1 accuracy. We find that AdANNS with 8-byte OPQ (Figure 6c) matches the top-1 accuracy of rigid representations using 32-byte OPQ, for a 4  cost reduction for the same accuracy. Also note that using AdANNS provides large gains over using MR-2048 at high compression (1.5%), highlighting the necessity of AdANNS s flexibility for high-precision retrieval at low compute budgets. 4. Our findings on the HNSW-OPQ composite index (Figure 6d) are consistent with all other indices, i.e.', chunk_index=58, num_tokens=298, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_cccff32a-5977-40cc-a846-bd96d1d1ffc2', content='HNSW graphs constructed with AdANNS OPQ codebooks provide significant gains over RR and MR, especially at high compression (  32 Bytes). OPQ on NQ dataset       2 3 4   0 5  7 R S       $ F F X U D F \\\\   $ G $ 1 1 6   2 3 4 & R P S X W H   % X G J H W     % \\\\ W H V 5 L J L G   2 3 4 5 L J L G   , 9 )   2 3 4 & R P S X W H   % X G J H W     % \\\\ W H V   , 9 )   2 3 4   0 5   $ G $ 1 1 6   , 9 )   2 3 4    7 R S       $ F F X U D F \\\\   (a) Exact Search + OPQ on Natural Questions (b) IVF + OPQ on Natural Questions Figure 7: Top-1 Accuracy of AdANNS composite indices with OPQ distance computation compared to MR and Rigid baselines models on Natural Questions. Our observations on ImageNet with ResNet-50 MR across search structures also extend to the Natural Questions dataset with Dense Passage Retriever (DPR with BERT-Base MR embeddings).', chunk_index=59, num_tokens=297, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_ec3ffb49-0c22-4bc6-8c52-a68bf744cb6b', content='We note that AdANNS provides gains over RR-768 embeddings for both Exact Search and IVF with OPQ (Figure 7a and 7b). We find that similar to ImageNet (Figure 15) IVF performance on Natural Questions generally scales with dimensionality. AdANNS thus reduces to MR-768 performance for M   16. See Appendix G for a more in-depth discussion of AdANNS with DPR on Natural Questions. Table 4: Comparison of PQ-MR with OPQ-MR for exact search on ImageNet-1K across embedding dimensionality d   {8, 16, . . ., 2048} quantized to m   {8, 16, 32, 64} bytes. OPQ shows large gains over vanilla PQ at larger embedding dimensionalities d   128. Entries with the highest top-1 accuracy for a given (d, m) tuple are bolded.', chunk_index=60, num_tokens=193, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_d6f51b6f-13e8-4990-a338-aa2344706ec2', content='Config OPQ m Top-1 mAP@10 P@100 Top-1 mAP@10 P@100 62.18 56.71 61.23 62.22 56.70 61.23 8 16 67.91 67.85 62.85 62.95 67.21 67.21 67.88 67.96 62.96 62.94 67.21 67.21 8 16 32 68.80 69.57 69.44 63.62 64.22 64.20 67.86 68.12 68.12 68.91 69.47 69.47 63.63 64.20 64.23 67.86 68.12 68.12 8 16 32 64 68.39 69.77 70.13 70.12 63.40 64.43 64.67 64.69 67.47 68.25 68.38 68.42 68.38 69.95 70.05 70.18 63.42 64.55 64.65 64.70 67.60 68.38 68.38 68.38 128 8 16 32 64 67.27 69.51 70.27 70.61 61.99 64.32 64.72 64.93 65.78 68.12 68.51 68.49 68.40 69.78 70.60 70.65 63.11 64.56 64.97 64.98 67.34 68.38 68.51 68.51 256 8 16 32 64 66.06 68.56 70.08 70.48 60.44 63.33 64.83 64.98 64.09 66.95 68.38 68.55 67.90 69.92 70.59 70.69 62.69 64.71 65.15 65.09 66.95 68.51 68.64 68.64 512 8 16 32 64 65.09 67.68 69.51 70.53 59.03 62.11 64.01 65.02 62.53 65.39 67.34 68.52 67.51 69.67 70.44 70.72 62.12 64.53 65.11 65.17 66.56 68.38 68.64 68.64 1024 8 16 32 64 64.58 66.84 68.71 69.88 58.26 61.07 62.92 64.35 61.75 64.09 66.04 67.68 67.26 69.34 70.43 70.81 62.07 64.23 65.03 65.19 66.56 68.12 68.64 68.64 2048 8 16 32 64 62.19 65.99 67.99 69.20 56.11 60.27 62.04 63.46 59.80 63.18 64.74 66.40 66.89 69.25 70.39 70.57 61.69 64.09 64.97 65.15 66.30 67.99 68.51 68.51 E AdANNS-IVF Inverted file index (IVF) [48] is a simple yet powerful ANNS data structure used in web-scale search systems [16].', chunk_index=61, num_tokens=850, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_55ba2306-405e-4812-8a5d-55ebfe345be5', content='IVF construction involves clustering (coarse quantization often through k-means) [36] on d-dimensional representation that results in an inverted file list [53] of all the data points in each cluster. During search, the d-dimensional query representation is first assigned to the closest clusters (# probes, typically set to 1) and then an exhaustive linear scan happens within each cluster to obtain the nearest neighbors. As seen in Figure 9, IVF top-1 accuracy scales logarithmically with increasing representation dimensionality d on ImageNet-1K/V2/4K. The learned low-d representations thus provide better accuracy-compute trade-offs compared to high-d representations, thus furthering the case for usage of AdANNS with IVF. Our proposed adaptive variant of IVF, AdANNS-IVF, decouples the clustering, with dc dimensions, and the linear scan within each cluster, with ds dimensions   setting dc = ds results in non- adaptive vanilla IVF. This helps in the smooth search of design space for the optimal accuracy- compute trade-off.', chunk_index=62, num_tokens=218, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_68a872d0-8a85-4d83-8256-09dff78998e4', content=\"A naive instantiation yet strong baseline would be to use explicitly trained  $ G $ 1 1 6   , 9 )   '  0 *   , 9 )   5 5 0 *   , 9 )   6 9 '    7 R S       $ F F X U D F \\\\  , 9 )   0 5 $ G $ 1 1 6   , 9 )  , 9 )   5 5   0 ) / 2 3 6   4 X H U \\\\ Figure 8: Top-1 accuracy vs. compute cost per query of AdANNS-IVF compared to IVF-MR, IVF-RR and MG-IVF-RR baselines on ImageNet-1K. dc and ds dimensional rigid representations (called MG-IVF-RR, for multi-granular IVF with rigid representations). We also examine the setting of adaptively choosing low-dimensional MR to linear scan the shortlisted clusters built with high-dimensional MR, i.e. AdANNS-IVF-D, as seen in Table 5. As seen in Figure 8, AdANNS-IVF provides pareto-optimal accuracy-compute tradeoff across inference compute. This figure is a more exhaustive indication of AdANNS-IVF behavior compared to baselines than Figures 1a and 2.\", chunk_index=63, num_tokens=295, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_f3896617-eed2-449f-b062-f68c897d830b', content='AdANNS-IVF is evaluated for all possible tuples of dc, ds, k = |C|   {8, 16, . . . , 2048}. AdANNS-IVF-D is evaluated for a pre-built IVF index with dc = 2048 and ds   {8, . . . , 2048}. MG-IVF-RR configurations are evaluated for dc   {8, . . . , ds}, ds   {32, . . . , 2048} and k = 1024 clusters. A study over additional k values is omitted due to high compute cost. Finally, IVF-MR and IVF-RR configurations are evaluated for dc = ds   {8, 16, . . . , 2048} and k   {256, . . . , 8192}. Note that for a fair comparison, we use np = 1 across all configurations. We discuss the inference compute for these settings in Appendix B.1.', chunk_index=64, num_tokens=207, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_51853e3b-5824-4bc8-9935-e20ae9b11a0f', content='E.1 Robustness      , 9 )   0 5      , 9 )   5 5  5 H S U H V H Q W D W L R Q   6 L ] H ( [ D F W   5 5  7 R S       $ F F X U D F \\\\ ( [ D F W   0 5  ( [ D F W   5 5    ( [ D F W   0 5 5 H S U H V H Q W D W L R Q   6 L ] H     , 9 )   0 5 7 R S       $ F F X U D F \\\\     , 9 )   5 5  5 H S U H V H Q W D W L R Q   6 L ] H     ( [ D F W   0 5   , 9 )   0 5  7 R S       $ F F X U D F \\\\    (a) ImageNet-1K (b) ImageNetV2 (c) ImageNet-4K Figure 9: Top-1 Accuracy variation of IVF-MR of ImageNet 1K, ImageNetV2 and ImageNet-4K. RR baselines are omitted on ImageNet-4K due to high compute cost.', chunk_index=65, num_tokens=288, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_fab964bb-f972-44b9-9bbe-8e841a83a397', content='As shown in Figure 9, we examined the clustering capabilities of MRs on both in-distribution (ID) queries via ImageNet-1K and out-of-distribution (OOD) queries via ImageNetV2 [44], as well as on larger-scale ImageNet-4K [31]. For ID queries on ImageNet-1K (Figure 9a), IVF-MR is at least as accurate as Exact-RR for d   256 with a single search probe, demonstrating the quality of in- distribution low-d clustering with MR. On OOD queries (Figure 9b), we observe that IVF-MR is on average 2% more robust than IVF-RR across all cluster construction and linear scan dimensionalities d. It is also notable that clustering with MRs followed by linear scan with # probes = 1 is more robust than exact search with RR embeddings across all d   2048, indicating the adaptability of MRs to distribution shifts during inference. As seen in Table 5, on ImageNetV2 AdANNS-IVF-D is the best Table 5: Top-1 Accuracy of AdANNS-IVF-D on out-of-distribution queries from ImageNetV2 compared to both IVF and Exact Search with MR and RR embeddings. Note that for AdANNS- IVF-D, the dimensionality used to build clusters dc = 2048.', chunk_index=66, num_tokens=290, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_3e6bfd43-ef36-4170-adbc-b4882c2f11be', content='AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR 8 16 32 64 128 256 512 1024 2048 53.51 57.32 57.32 57.85 58.02 58.01 58.03 57.66 58.04 50.44 56.35 57.64 58.01 58.09 58.33 57.84 57.58 58.04 50.41 56.64 57.96 58.94 59.13 59.18 59.40 59.11 59.63 49.03 55.04 56.06 56.84 56.14 55.60 55.46 54.80 56.17 48.79 55.08 56.69 57.37 57.17 57.09 57.12 57.53 57.84 configuration for d   16, and is similarly accurate to IVF-MR at all other d. AdANNS-IVF-D with d = 128 is able to match its own accuracy with d = 2048, a 16  compute gain during inference. This demonstrates the potential of AdANNS to adaptively search pre-indexed clustering structures.', chunk_index=67, num_tokens=295, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_d59e464a-98d7-4b73-a8ef-92c8c2678eae', content='On 4-million scale ImageNet-4K (Figure 9c), we observe similar accuracy trends of IVF-MR compared to Exact-MR as in ImageNet-1K (Figure 9a) and ImageNetV2 (Figure 9b). We omit baseline IVF-RR and Exact-RR experiments due to high compute cost at larger scale. E.2 IVF-MR Ablations a          & R P S X W H 7 R S 0 ) / 2 3 6   4 X H U \\\\   , 9 )       3 U R E H , 9 )       3 U R E H    ( [ D F W   , 9 )       3 U R E H , 9 )       3 U R E H 5 H F D O O # 5 H S U H V H Q W D W L R Q   6 L ] H  5 H F D O O # 1   5 H F D O O #  5 H F D O O #   5 H F D O O #    5 H F D O O #       (a) 4K Search Probes (np) (b) Centroid Recall Figure 10:', chunk_index=68, num_tokens=267, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_0a3fd449-12bb-481f-be9d-7398c3af0e28', content='Ablations on IVF-MR Clustering: a) Analysis of accuracy-compute tradeoff with in- creasing IVF-MR search probes np on ImageNet-4K compared to Exact-MR and b) k-Recall@N on ImageNet-1K cluster centroids across representation sizes d. Cluster centroids retrieved with highest embedding dim d = 2048 were considered ground-truth centroids. As seen in Figure 10a, IVF-MR can match the accuracy of Exact Search on ImageNet-4K with   100  less compute. We also explored the capability of MRs at retrieving cluster centroids with low-d compared to a ground truth of 2048-d with k-Recall@N, as seen in Figure 10b. MRs were able to saturate to near-perfect 1-Recall@N for d   32 and N   4, indicating the potential of AdANNS at matching exact search performance with less than 10 search probes np. E.3 Clustering Distribution We examined the distribution of learnt clusters across embedding dimensionalities d for both MR and RR models, as seen in Figure 11. We observe IVF-MR to have less variance than IVF-RR at d   {8, 16}, and slightly higher variance for d   32, while IVF-MR outperforms IVF-RR in top-1 across all d (Figure 9a).', chunk_index=69, num_tokens=299, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_224422db-457a-4bba-a734-2e301d122550', content='This indicates that although MR learns clusters that are less uniformly distributed than RR at high d, the quality of learnt clustering is superior to RR across all d. Note that a uniform distribution is N/k data points per cluster, i.e.   1250 for ImageNet-1K with k = 1024.', chunk_index=70, num_tokens=64, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_e3bde862-a67e-44ef-8a2f-2fc17744b963', content='We quantitatively evaluate the proximity of the MR and RR clustering distributions with Total Variation 150 300 300 2500Number of data points per cell RR-128 300 350Number of cells 1000 1000 dTV=0.00076dTV,2048=0.00065 MR-16 1000 1000 150 RR-256 200 MR-1024 150 500 MR-256 2500Number of data points per cell 1500 100 350Number of cells 150 RR-512 2000 dTV=0.00068dTV,2048=0.00064 dTV=0.00054dTV,2048=0.00054 250 200 dTV=0.00016dTV,2048=0.00095 1500 MR-512 300 200 1000 MR-32 200 RR-8 2500Number of data points per cell MR-8 400Number of cells 150 200 300 500 300Number of cells 500 RR-16 100 2000 200 MR-64 2000 1000 250 250 250 1000 1500 2000 500 300Number of cells 250 dTV=0.00075dTV,2048=0.00059 1000 1500 2000 dTV=0.00061dTV,2048=0.00069 RR-1024 2500Number of data points per cell 1500 100 2500Number of data points per cell 100 2000 2500Number of data points per cell RR-64 200 2000 2000 RR-32 100 300 2500Number of data points per cell 1000 350Number of cells 200 500 500 1500 2500Number of data points per cell RR-2048 100 2000 300 350Number of cells 400Number of cells 100 500 500 200 dTV=0.00027dTV,2048=0.00121 500 1500 dTV=0.00037dTV,2048=0.00078 250 MR-128 150 400Number of cells 1500 100 100 1500 MR-2048 dTV=0.00078dTV,2048=0.00128 2500Number of data points per cell Figure 11:', chunk_index=71, num_tokens=531, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_759deb2a-eeb7-495e-a180-891f1ce16404', content='Clustering distributions for IVF-MR and IVF-RR across embedding dimensionality d on ImageNet-1K. An IVF-MR and IVF-RR clustered with d = 16 embeddings is denoted by MR-16 and RR-16 respectively. Distance [33], which is defined over two discrete probability distributions p, q over [n] as follows: dT V (p, q) = 1 2 (cid:88) |pi   qi| i [n] We also compute dT V,2048(MR-d) = dT V (MR-d, RR-2048), which evaluates the total variation dis- tance of a given low-d MR from high-d RR-2048. We observe a monotonically decreasing dT V,2048 with increasing d, which demonstrates that MR clustering distributions get closer to RR-2048 as we increase the embedding dimensionality d. We observe in Figure 11 that dT V (MR-d, RR-d)   7e   4 for d   {8, 256, . . . , 2048} and   3e   4 for d   {16, 32, 64}.', chunk_index=72, num_tokens=253, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_dee241e1-6862-46fc-a456-e980d3d76b7b', content='These findings agree with the top-1 improvement of MR over RR as shown in Figure 9a, where there are smaller improvements for d   {16, 32, 64} (smaller dT V ) and larger improvements for d   {8, 256, . . . , 2048} (larger dT V ). These results demonstrate a correlation between top-1 performance of IVF-MR and the quality of clusters learnt with MR. F AdANNS-DiskANN DiskANN is a state-of-the-art graph-based ANNS index capable of serving queries from both RAM and SSD. DiskANN builds a greedy best-first graph with OPQ distance computation, with compressed vectors stored in memory. The index and full-precision vectors are stored on the SSD. During search, Table 6: Wall clock search latency ( s) of AdANNS-DiskANN across graph construction dimension- ality d   {8, 16, . . . , 2048} and compute budget in terms of OPQ budget M   {8, 16, 32, 48, 64}. Search latency is fairly consistent across fixed embedding dimensionality D.', chunk_index=73, num_tokens=242, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_e398841d-9bae-4937-926f-ebe0b50d0178', content='M =8 M =16 M =32 M =48 M =64 8 16 32 64 128 256 512 1024 2048 495 555 669 864 1182 1923 2802 5127 9907 571 655 855 1311 1779 3272 5456 9833 - 653 843 1156 1744 3423 5724 10205 - - 844 1161 2849 2780 4683 10183 - - 848 2011 1818 3171 5087 9329 when a query s neighbor shortlist is fetched from the SSD, its full-precision vector is also fetched in a single disk read. This enables efficient and fast distance computation with PQ on a large initial shortlist of candidate nearest neighbors in RAM followed by a high-precision re-ranking with full- precision vectors fetched from the SSD on a much smaller shortlist. The experiments carried out in this work primarily utilize a DiskANN graph index built in-memory5 with OPQ distance computation. As with IVF, DiskANN is also well suited to the flexibility provided by AdANNS as we demonstrate on both ImageNet and NQ that the optimal PQ codebook for a given compute budget is learnt with a smaller em- bedding dimensionality d (see Fig- ures 6c and 7a).', chunk_index=74, num_tokens=298, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_5621a699-8b5c-4968-85e8-a9548655f839', content='We demonstrate the capability of AdANNS-DiskANN with a compute budget of m   {32, 64} in Table 1. We tabulate the search time latency of AdANNS- DiskANN in microseconds ( s) in Table 6, which grows linearly with graph construction dimensionality d. We also examine DiskANN-MR with SSD graph indices on ImageNet-1K across OPQ budgets for distance com- putation mdc   {32, 48, 64}, as seen in Figure 12. With SSD indices, we store PQ-compressed vectors on disk with mdisk = mdc, which es- sentially disables DiskANN s implicit high-precision re-ranking. We ob- serve similar trends to other composite ANNS indices on ImageNet, where the optimal dim for fixed OPQ budget is not the highest dim (d = 1024 with fp32 embeddings is current highest dim supported by DiskANN which stores vectors in 4KB sectors on disk).', chunk_index=75, num_tokens=204, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_fcf6fcae-40b4-483e-afa0-450a60239be0', content='This provides further motiva- tion for AdANNS-DiskANN, which leverages MRs to provide flexible access to the optimal dim for quantization and thus enables similar Top-1 accuracy to Rigid DiskANN for up to 1/4 the cost (Figure 6c).   5 H S U H V H Q W D W L R Q   6 L ] H 7 R S       $ F F X U D F \\\\    % \\\\ W H % \\\\ W H % \\\\ W H       Figure 12: DiskANN-MR with SSD indices for ImageNet- 1K retrieval, with compute budgets mdisk = mdc   {32, 48, 64} across graph and OPQ codebook construction dimensionalities d   {32, . . . , 1024}. Note that this does not use any re-ranking after obtaining OPQ based shortlist. G AdANNS on Natural Questions In addition to image retrieval on ImageNet, we also experiment with dense passage retrieval (DPR) on Natural Questions. As shown in Figure 6, MR representations are 1   10% more accurate than their 5https://github.com/microsoft/DiskANN RR counterparts across PQ compute budgets with Exact Search + OPQ on NQ.', chunk_index=76, num_tokens=264, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_e2d6224b-2970-43be-acd0-02f7ee117697', content='We also demonstrate that IVF-MR is 1   2.5% better than IVF-RR for Precision@k, k   {1, 5, 20, 100, 200}. Note that on NQ, IVF loses   10% accuracy compared to exact search, even with the RR-768 baseline. We hypothesize the weak performance of IVF owing to poor clusterability of the BERT-Base embeddings fine-tuned on the NQ dataset. A more thorough exploration of AdANNS-IVF on NQ is an immediate future work and is in progress.', chunk_index=77, num_tokens=126, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_2125b499-d8fb-4b74-8b8b-41aaab95f80a', content='H Ablations H.1 Recall Score Analysis            5 H F D O O #   , 9 )   6 H D U F K   3 U R E H V    np     , 9 )   6 H D U F K   3 U R E H V    np              5 H F D O O #       5 H F D O O # + 1 6 :   6 H D U F K   3 U R E H V     H I 6 H D U F K          + 1 6 :   6 H D U F K   3 U R E H V     H I 6 H D U F K 5 H F D O O #                  0 5   0 5   5 H F D O O # 0 5 , 9 )   6 H D U F K   3 U R E H V    np 0 5  5 H F D O O # , 9 )   6 H D U F K   3 U R E H V    np  0 5    0 5  0 5 0 5  Figure 13: k-Recall@N of d-dimensional MR for IVF and HNSW with increasing search probes np on ImageNet-1K and ImageNet-4K.', chunk_index=78, num_tokens=295, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_4f546122-a6f8-4bc4-bd6c-13806c09bd32', content='On ImageNet-4K, we restrict our study to IVF-MR with d   {8, 64, 256, 2048}. Other embedding dimensionalities, HNSW-MR and RR baselines are omitted due to high compute cost. We observe that trends from ImageNet-1K with increasing d and np extend to ImageNet-4K, which is 4  larger. 5 H S U H V H Q W D W L R Q   6 L ] H         0 5  5 H O D W L Y H   & R Q W U D V W     5 5   Figure 14: Relative contrast of varying capacity MRs and RRs on ImageNet-1K corroborating the findings of He et al. [18]. In this section we also examine the variation of k-Recall@N with by probing a larger search space with IVF and HNSW indices. For IVF, search probes represent the number of clusters shortlisted for linear scan during inference. For HNSW, search quality is controlled by the ef Search parameter [38], which represents the closest neighbors to query q at level lc of the graph and is analogous to number of search probes in IVF.', chunk_index=79, num_tokens=260, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_35e61e97-7df2-4e5a-ac6e-44703ab63947', content='As seen in Figure 13, general trends show a) an intuitive increase in recall with increasing search probes np) for fixed search probes, b) a decrease in recall with increasing search dimensionality d c) similar trends in ImageNet-1K and 4  larger ImageNet-4K. H.2 Relative Contrast We utilize Relative Contrast [18] to capture the difficulty of nearest neighbors search with IVF-MR compared to IVF-RR. For a given database X = {xi   Rd, i = 1, . . . , ND}, a query q   Rd, and a distance metric D(., .) we compute relative contrast Cr as a measure of the difficulty in finding the 1-nearest neighbor (1-NN) for a query q in database X as follows: 1. Compute Dq min = min i=1. . .n D(q, xi), i.e. the distance of query q to its nearest neighbor xq nn   X 2. Compute Dq x   X mean = Ex[D(q, x)] as the average distance of query q from all database points Dq mean Dq 3. Relative Contrast of a given query C q r = , which is a measure of how separable the min nn is from an average point in the database x query s nearest neighbor xq 4.', chunk_index=80, num_tokens=278, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_9b636b8f-fb8c-4c43-adad-60a008369fed', content='Compute an expectation over all queries for Relative Contrast over the entire database as Cr = Eq[Dq Eq[Dq mean] min] It is evident that Cr captures the difficulty of Nearest Neighbor Search in database X, as a Cr   1 indicates that for an average query, its nearest neighbor is almost equidistant from a random point in the database. As demonstrated in Figure 14, MRs have higher Rc than RR Embeddings for an Exact Search on ImageNet-1K for all d   16. This result implies that a portion of MR s improvement over RR for 1-NN retrieval across all embedding dimensionalities d [31] is due to a higher average separability of the MR 1-NN from a random database point. H.3 Generality across Encoders We perform an ablation over the representation function   : X   Rd learnt via a backbone neural network (primarily ResNet50 in this work), as detailed in Section 3. We also train MRL models [31]  M R(d) on ResNet18/34/101 [19] that are as accurate as their independently trained RR baseline models  RR(d), where d is the default max representation size of each architecture. We also train MRL with a ConvNeXt-Tiny backbone with [d] = {48, 96, 192, 384, 786}.', chunk_index=81, num_tokens=284, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]}), ResponseChunk(id='chunk_0a3f74d5-53de-4aea-b7e7-2fa8adabc1d7', content='MR-768 has a top-1 accuracy of 79.45% compared to independently trained publicly available RR-768 baseline with top-1 accuracy 82.1% (Code and RR model available on the official repo6). We note that this training had no hyperparameter tuning whatsoever, and this gap can be closed with additional model training effort. We then compare clustering the MRs via IVF-MR with k = 2048, np = 1 on ImageNet-1K to Exact-MR, which is shown in Figure 15. IVF-MR shows similar trends across backbones compared to Exact-MR, i.e. a maximum top-1 accuracy drop of   1.6% for a single search probe. This suggests the clustering capabilities of MR extend beyond an inductive bias of  M R(d)   ResNet50, though we leave a detailed exploration for future work. 5 H V 1 H W 7 R S       $ F F X U D F \\\\  5 H V 1 H W  ( [ D F W   0 5  5 H V 1 H W      5 H S U H V H Q W D W L R Q   6 L ] H 5 H V 1 H W  & R Q Y 1 H ; W   7 L Q \\\\ , 9 )   0 5    Figure 15:', chunk_index=82, num_tokens=296, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_e56411f7-d100-40fb-8c3c-3cda56c4b98f', content='Top-1 Accuracy variation of IVF-MR on ImageNet-1K with different embedding rep- resentation function  M R(d) (see Section 3), where     {ResNet18/34/101, ConvNeXt-Tiny}. We observe similar trends between IVF-MR and Exact-MR on ResNet18/34/101 when compared to ResNet50 (Figure 9a) which is the default in all experiments in this work. 6https://github.com/facebookresearch/ConvNeXt', chunk_index=83, num_tokens=110, metadata={'section_titles': ['networks and ISDN systems, 30(1-7):107 117, 1998.', 'RR-2048 AdANNS', 'OPQ on NQ dataset', 'num_classes |ND|', '4.3 AdANNS for Composite Indices', 'AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR', 'G AdANNS on Natural Questions', 'Top k Relevant Data Points', ' 1  C1', 'C Evaluation Metrics', '5.2 Why MRs over RRs?', 'IVF-RR', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', ' k  Ck', 'Conference on Machine Learning (ICML), 2012.', 'E.3 Clustering Distribution', '6https://github.com/facebookresearch/ConvNeXt', 'min = min i=1...n', 'Method', '(a) ImageNet-1K', 'G L . s c [', '(b) ImageNetV2', 'MR-8', 'https://pypi.org/project/annoy/. Python package version 1.13.0.', ' :   !', 'H Ablations', '4 AdANNS   Adaptive ANNS', ' i  Ci', ' Equal contribution.', '5https://github.com/microsoft/DiskANN', 'dT V (p, q) =', 'Dq mean Dq', 'F AdANNS-DiskANN', 'Retrieval Formula during Inference', 'Top-1', 'B Training and Compute Costs', '2 Related Work', 'E.1 Robustness', '(d) HNSW + OPQ on ImageNet-1K', 'E.2 IVF-MR Ablations', 'E AdANNS-IVF', 'Algorithm 1 AdANNS-IVF Psuedocode', 'IVFOPQ', 'Q correct_pred@1 |Q|', 'DatabaseConstruct Clusters with  dc', 'query s nearest neighbor xq', 'Rigid-IVF', '3 Problem Setup, Notation, and Preliminaries', 'H.2 Relative Contrast', 'Select Closest Cluster', 'PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%)', '(a) 4K Search Probes (np)', 'Cr =', 'RR-8', 'mean] min]', 't c O 8 1', 'Introduction', '5.3 Search for AdANNS Hyperparameters', '//blog.google/products/search/search-language-understanding-bert/.', 'def adannsConstruction(database, d_cluster, num_clusters):', '(b) Centroid Recall', 'B.1 Inference Compute Cost', '5 Further Analysis and Discussion', '2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss', '(c) DiskANN + OPQ on ImageNet-1K', 'for q in queries:', 'H.1 Recall Score Analysis', '(a) Exact Search + OPQ on ImageNet-1K', '(a) Image retrieval on ImageNet-1K.', 'arg minj Ch(q) arg minj Ch(q)', 'top-1 =', '400Number of cells', 'Query', '4.2 AdANNS-OPQ', 'H.3 Generality across Encoders', 'References', 'IVF-MR', '(b) IVF + OPQ on Natural Questions', 'OPQ', 'A AdANNS Framework', '2500Number of data points per cell', 'return cluster_centroids', '6 Conclusions', 'AdANNS-IVF', 'IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.', 'Transactions on Big Data, 7(3):535 547, 2019.', '(b) Passage retrieval on Natural Questions.', '|pi   qi|', 'IEEE', 'nn   X', '300Number of cells', 'Abstract', '4.1 AdANNS-IVF', 'transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.', 'Config', '(a) Exact Search + OPQ on Natural Questions', '(b) IVF + OPQ on ImageNet-1K', 'Acknowledgments', 'AdANNS-IVF-D arg minj Ch(q)', 'm Top-1 mAP@10', 'Eq[Dq Eq[Dq', 'probe', '4https://github.com/facebookresearch/DPR', 'AdANNS: A Framework for Adaptive Semantic Search', 'C = dsk +', 'ANNS ConstructionANNS Inference', 'D AdANNS-OPQ', 'MG-IVF-RR', 'Search Latency/Query (ms)', '350Number of cells', 'npdsND k', 'return k_nearest_neighbors', '(c) ImageNet-4K', 'Linear Scan with  ds', 'annual symposium on Computational geometry, pages 160 164, 1994.'], 'pages': [1]})]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aurelio_sdk import ExtractResponse\n",
    "\n",
    "# From a local file\n",
    "file_path = \"data/pdf/adaptive_semantic_search.pdf\"\n",
    "\n",
    "with open(file_path, \"rb\") as file:\n",
    "    # timeout -1 means no timeout, default is 30 seconds\n",
    "    response_pdf_file: ExtractResponse = await client.extract_file(\n",
    "        file=file, quality=\"low\", chunk=True, timeout=-1\n",
    "    )\n",
    "\n",
    "response_pdf_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtractResponse(status=<TaskStatus.completed: 'completed'>, usage=Usage(tokens=838, pages=None, seconds=291), message=None, processing_options=ExtractProcessingOptions(chunk=True, quality=<ProcessingQuality.low: 'low'>), document=ResponseDocument(id='doc_5f2a5e87-2382-4036-85d6-e7a3aecc3153', content=\" In a 2019 study, over 400 participants were enlisted to learn a mysterious, invented language. Individuals were asked about three pairs of runes. For example, which of these two characters represents an animal? Then, after a brief break, they were asked about the same roon pairs, with questions flipped. as in which of these two runes represents a non-living object. But this game had a secret.  The subject's answers in round one determined the rune's meanings in round two. In the first round, participants either had all their answers marked as correct, no matter what, or they were forced to fail every question. This meant that at the break, every participant had the same amount of information, and in round two, they were playing for real. But despite this even playing field, the successful participants from round one rose to the top of the ranks, while those cast as failures kept, well, failing. People  People often describe failure as a teachable moment, a necessary stumble on our way to improvement. But learning from our mistakes isn't always easy, especially when those failures are demoralizing, overwhelming, or just downright confusing. So what exactly prevents us from turning our mistakes into mastery? Perhaps the most obvious hurdle to learning from failure is how painful it can be. People generally want to think of themselves as capable and competent, and experiencing failure threatens that self-image. In a survey following a replication of...  the Roon study, participants in the failure group indicated much lower levels of self-confidence after participating. It's tempting to dismiss this pain as a temporary setback, but some studies have found that when people feel demoralized or incompetent, their brains often stop processing new information. This suggests that if a threat to your self-esteem is large enough, it can undermine your ability to learn. However, your tolerance for failure also depends on. on your relationship with the task at hand. In a study from 2011, researchers surveyed a group of American students  enrolled in introductory and advanced French courses. These students completed a questionnaire asking what kind of teacher they preferred, one who emphasized their strength and successes, or one who highlighted their mistakes and corrected their weaknesses. In general, responses showed that while beginner students sought positive reinforcement, advanced students were more eager for critical feedback. Researchers have theorized a handful of explanations for these results. Having just started out, beginners are still determining if they enjoy learning French, and if they want to continue studying.  so they might crave praise as a way to stay motivated. On the other hand, the advanced students are already invested, so they may want to improve their skills as efficiently as possible. The process of gaining expertise also comes with its fair share of failure, so the advanced students may have built a higher tolerance for making mistakes. But whether you're an expert or a novice, it's usually much more straightforward to learn from your successes than your failures. For example, imagine getting your grade back on an exam. If you aced it, you could reasonably assume you made good choices around when...  what and how much to study, and you can replicate those decisions for the next test. But if you failed, it could be for any number of reasons. Maybe you didn't study enough. Maybe you studied the wrong information. Or maybe you did everything right and the test covered things you shouldn't have been expected to know. In cases like this, it's unclear exactly what went wrong, making it difficult to learn how to improve. Wanting to learn from our failures is completely natural, and there's a lot of to gain by being resilient and cultivating a growth mindset. But fixating on your failures can make it easy to fail.  forget all your successes. And building on what you're doing right can be more effective than focusing on what you did wrong. One of the ways we can be more constructive with ourselves is by considering how we talk to ourselves. Self-talk can actually play a major role in performance. Learn the key to doing it right with this video. Or get actionable science-based advice on how to build character at ed.t.com slash build character.\", source='data%2fvideo%2fhow_to_overcome_our_mistakes.mp4', source_type=<SourceType.video_mp4: 'video/mp4'>, num_chunks=2, metadata={}, chunks=[ResponseChunk(id='chunk_dbec629c-85e4-4436-b591-84a840be3f2c', content=\"In a 2019 study, over 400 participants were enlisted to learn a mysterious, invented language. Individuals were asked about three pairs of runes. For example, which of these two characters represents an animal? Then, after a brief break, they were asked about the same roon pairs, with questions flipped. as in which of these two runes represents a non-living object. But this game had a secret. The subject's answers in round one determined the rune's meanings in round two. In the first round, participants either had all their answers marked as correct, no matter what, or they were forced to fail every question. This meant that at the break, every participant had the same amount of information, and in round two, they were playing for real. But despite this even playing field, the successful participants from round one rose to the top of the ranks, while those cast as failures kept, well, failing. People People often describe failure as a teachable moment, a necessary stumble on our way to improvement. But learning from our mistakes isn't always easy, especially when those failures are demoralizing, overwhelming, or just downright confusing. So what exactly prevents us from turning our mistakes into mastery? Perhaps the most obvious hurdle to learning from failure is how painful it can be. People generally want to think of themselves as capable and competent, and experiencing failure threatens that self-image. In a survey following a replication of. .\", chunk_index=1, num_tokens=291, metadata={'start_time': 0, 'end_time': 289}), ResponseChunk(id='chunk_1ca8a771-3dc3-4123-9587-811176fc39f9', content=\"For example, imagine getting your grade back on an exam. If you aced it, you could reasonably assume you made good choices around when. . . what and how much to study, and you can replicate those decisions for the next test. But if you failed, it could be for any number of reasons. Maybe you didn't study enough. Maybe you studied the wrong information. Or maybe you did everything right and the test covered things you shouldn't have been expected to know. In cases like this, it's unclear exactly what went wrong, making it difficult to learn how to improve. Wanting to learn from our failures is completely natural, and there's a lot of to gain by being resilient and cultivating a growth mindset. But fixating on your failures can make it easy to fail. forget all your successes. And building on what you're doing right can be more effective than focusing on what you did wrong. One of the ways we can be more constructive with ourselves is by considering how we talk to ourselves. Self-talk can actually play a major role in performance. Learn the key to doing it right with this video. Or get actionable science-based advice on how to build character at ed.t.com slash build character.\", chunk_index=2, num_tokens=246, metadata={'start_time': 211, 'end_time': 289})]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aurelio_sdk import ExtractResponse\n",
    "\n",
    "# From a local file\n",
    "file_path = \"data/video/how_to_overcome_our_mistakes.mp4\"\n",
    "\n",
    "with open(file_path, \"rb\") as file:\n",
    "    # timeout -1 means no timeout, default is 30 seconds\n",
    "    response_video_file: ExtractResponse = await client.extract_file(\n",
    "        file=file, quality=\"low\", chunk=True, timeout=-1\n",
    "    )\n",
    "\n",
    "response_video_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtractResponse(status=<TaskStatus.completed: 'completed'>, usage=Usage(tokens=11875, pages=8, seconds=None), message=None, processing_options=ExtractProcessingOptions(chunk=True, quality=<ProcessingQuality.low: 'low'>), document=ResponseDocument(id='doc_797e46e3-529f-4c5d-91d0-43a957e9975e', content='4 2 0 2\\ng u A 6 2\\nR S . h p - o r t s a [\\n1 v 1 9 2 5 1 . 8 0 4 2 : v i X r a\\nA temperature scale of 1 2 eV in the mass-radius relationship of white dwarfs of type DA\\nJin Lima, Ji-Yu Kima, Maurice H.P.M. van Puttena,1,\\naPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea\\nAbstract\\nThe mass-radius relationship of white dwarfs (WDs) is one of their defining characteristics, largely derived from electron degen- eracy pressure. We present a model-independent study of the observed mass-radius relationship in WD binaries of Parsons et al. (2017), listing data over a broad temperature range up to about 60,000 K (5 eV). The data show an appreciable temperature sen- sitivity with pronounced intrinsic scatter (beyond measurement uncertainty) for the canonical He-models with proton to neutron ratio 1:1. We characterize temperature sensitivity by a temperature scale T0 in model-agnostic power-law relations with tempera- ture normalized radius. For low-mass WDs, the results identify a remarkably modest T0 = 1   2 eV. We comment on a potential interpretation for atmospheres insulating super-Eddington temperature cores from the sub-Eddington photospheres of low-mass WDs.\\nKeywords: white dwarfs, mass-radius relation, Chandrasekhar\\n1. Introduction\\nWhite dwarfs (WDs) represent the final evolutionary stage of stars with masses in the range of 0.4M  < M < 8M . As such, they are quite numerous. The Gaia Early Data Release 3 (EDR3) contains 359,073 WD candidates (Gentile Fusillo et al., 2021). EDR3 extends the survey of 29,294 WDs of the Sloan Digital Sky Survey (SDSS) Data Release 16 (DR 16) by over an order of magnitude with 25,176 WDs in both surveys. WDs are variously classified by their atmospheric composition (Gen- tile Fusillo et al. 2021). Most are of type DA characterized by a dominance of hydrogen lines in their spectra. It reveals an opaque hydrogen atmosphere, whose observed temperatures are well below the Eddington limit (Fig. 1).\\nWDs have a distinct mass-radius relationship with a lower bound defined by electron degeneracy pressure (Str omgren, 1939). It is described by the equation of state (EoS) of de- generacy pressure of a Fermi gas, the relativistic limit of which defines the Chandrasekhar mass-limit. To leading order, WDs are modeled by the ideal Fermi gas at zero temperature sur- rounded by a non-degenerate atmosphere (Koester and Chan- mugam, 1990). At zero temperature, the degenerate WD core is effectively parameterized only by a proton-to-neutron ratio. In this limit, the mass-radius relation satisfies (Str omgren, 1939; Hamada and Salpeter, 1961; Koester et al., 1979)\\nR = C0\\n(cid:32)\\nM M \\n(cid:33) 1/3\\nwhere\\nC0 =\\n(cid:32) 16G3m3 e 81 2 6\\n(cid:33) 1/3 (cid:32) Amp Z\\n(cid:33) 5/3\\nM 1/3  \\n  0.01 R ,\\nEmail address: mvp@sejong.ac.kr (Maurice H.P.M. van Putten) 1INAF-OAS Bologna via P. Gobetti 101 I-40129 Bologna Italy, Italy\\n(1)\\n(2)\\nwhere G is Newton s constant,   is Planck s constant, me and mp are the masses of the electron and proton, A is the atomic number and Z denotes the number of protons in a nucleus - here, mostly alpha-nuclei. We have set the A/Z ratio to 2, since many WDs consist of a C/O or He core, the latter in particular expected for low-mass WDs (Iben and Tutukov, 1985; Marsh et al., 1995; Nelemans et al., 2001; Han et al., 2002; Istrate et al., 2016; Ren et al., 2018; Zenati et al., 2019). According to this calculation, R = C0M 1/3 holds.\\nFig. 2 shows the observed mass-radius relation for 26 WDs of type DA, all in eclipsing binaries, sampled by Parsons et al. (2017) by photometric and spectroscopic observations. Photo- metric observations use, for example, the Ultrafast Triple-beam CCD Camera (ULTRACAM, Dhillon et al. 2007) and its spec- troscopic version ULTRASPEC (Dhillon et al., 2014), currently in use as the high-speed imaging photometer on the Thai Na- tional Telescope (TNT). Spectroscopic observations have been performed by X-shooter (Vernet et al., 2011) in ESO Very Large Telescope (VLT).\\nFig. 2 shows the theoretical mass-radius relation of the de- generate core (1) to provide a lower bound. It effectively pro- vides a greatest lower bound only for M/M    0.5, even though it represents an rather elementary model of degeneracy pres- sure. The data clearly show a temperature sensitivity at rel- atively low mass M/M    0.5, see also Fig. 9 in Parsons et al. (2017). Crucially, the temperatures involved (Fig. 1) are far below the characteristic energy 0.1   1 Me V of the Fermi energies of the electrons in the degenerate core. For the present temperature range and low-mass WDs, any finite temperature corrections to the EoS (de Carvalho et al., 2014; Boshkayev, 2018; Boshkayev et al., 2021), including relativis- tic corrections, will be accordingly small for any generaliza- tions beyond (1), notably in Hamada and Salpeter (1961), Ro- tondo et al. (2011), de Carvalho et al. (2014), Boshkayev (2018)\\nPreprint submitted to New Astronomy\\nAugust 29, 2024\\nFigure 1: (Top panel.) Observed, Eddington and core temperatures of the 26 WDs of type DA in the sample of Parsons et al. (2017). Core temperatures are inferred from the Koester (1976) correlation for an optically thick atmosphere, insulating a C/O or He core at super-Eddington temperatures. (Lower panel.) Same data plotted as a function of mass with trends at slope 1.10 (Observed, blue), 1.18 (Eddington, red) and -0.11 (Core, brown). Eddington temperatures are roughly consistent with the geometric mean of observed and core tempera- tures.\\nand Baiko and Yakovlev (2019); see further Koester and Chan- mugam (1990); Koester (2002). Moreover, at sufficiently high density, the mass-radius relationship of the core becomes uni- versal, independent of the details of the EoS.\\nInstead, the origin of temperature sensitivity in the mass- radius relationship may be found in a non-degenerate atmo- sphere, if present. In particular, a finite temperature sensitivity is expected from an atmosphere about a core at super-Eddington temperatures - allowed for a sufficiently massive and optically thick atmospheres (Fontaine et al., 2001).\\nA variety of studies have been conducted to find solutions thereto and gain a deeper understanding of, e.g., an H enve- lope and/or evolution models depending on core composition (Hamada and Salpeter, 1961; Hearn and Mewe, 1976; Verbunt and Rappaport, 1988; Benvenuto and Althaus, 1999; Fontaine\\net al., 2001; Panei et al., 2007; Boshkayev et al., 2015; Par- sons et al., 2017; Kepler et al., 2019; Pei, 2022). Among these studies, various convection theories have been advanced, also to explain cooling times and evolutionary processes of WDs in- cluding the thermal insulation provided by their non-degenerate atmospheres.\\nFor instance, hot WDs in SDSS DR12 have been analyzed with models of cooling and atmospheres (B edard et al., 2020). Non-Local Thermal Equilibrium (non-LTE) atmospheres and synthetic WD spectra reveal a correlation between surface grav- ity and effective temperature.\\nThese modeled approaches, however, are intricate with po- tentially systematic uncertainties in the detailed structure of non-degenerate atmospheres, mediating heat transport by radi- ation and convection (B edard, 2024). For this reason, we set out the present model-agnostic study based on spectroscopic and photometric data, to further our understanding of the mass- radius relationship (Tremblay et al., 2017; Boshkayev et al., 2016).\\nFor the recent sample of 26 WDs of Parsons et al. (2017) (Fig. 2), we set out to derive a temperature scale T0, charac- terizing temperature sensitivity by exploring various power law relations for the mass-radius relations. The resulting T0 may serve as a novel observational constraint in future studies.\\nIn  2, we recall some preliminaries of the Eddington temper- ature and the Koester (1976) correlation of core and observed temperature. In  3, we introduce a temperature normalized ra- dius, to be used in power-law fits to the data based on two cost functions:  2 and residual Standard deviation (STD) defined by minimal least square errors. In  4, introduce three temperature- normalized power-laws and consider their fits to data in the log- log plane to effectively describe the expansion of apparent ra- dius with normalized temperature. In  5, two of the three re- lationships are ranked by probability of significance by Monte Carlo analysis. In  6, we interpret the results and summarize our findings with an outlook for future studies in  7.\\n2. The intermediate Eddington temperature\\nFig. 1 shows, as expected, the observed temperatures T to be strictly below the Eddington temperature, TEdd. After all, the modest observed temperatures on the order of a few eV im- ply the existence of an atmosphere. A reverse inequality would imply rapid evaporation by radiation pressure acting on the op- tically thin outer-most layers of any atmosphere.\\nThe Eddington temperature TEdd is defined by equating the Eddington luminosity LEdd = 3.2   104 (M/M ) L  to the lumi- nosity from a sphere of radius R. That is, LEdd = 4 R2 T 4 Edd, where   = 5.67   10 5g s 3K 4 is the Stefan-Boltzmann con- stant. This defines\\nTEdd = 39.5\\n(cid:32)\\ng 5000 g \\n(cid:33) 1 4\\nby surface gravity g = GM/R2 with a fiducial value for M = 0.5M , R = 2%R , scaled to the solar value g  = GM /R2  .\\n(3)\\nFigure 2: Mass-radius plot with temperature (color) in the sample of 26 WDs of type DA (Parsons et al., 2017). NN Ser is the hottest and SDSS J0138- 0016 is the coldest at 63,000K and, respectively, 3,570 K. For reference, it includes the theoretical zero-temperature limit (1) (dashed black line). The vertical grey region highlights three WDs of essentially the same mass with pronounced expansion in apparent radius with temperature. A significant de- parture is seen between observed and the expected radius (1), especially at low mass M/M    0.5. Included are fits to the data by a model-agnostic power- law Relation-2 (dotted colored curves,  3) studied in the present work with isothermals covering 5,000-65,000 K (bottom to top in steps of 10,000 K) in observed temperature. Relation-2 identifies a characteristic temperature scale T0 = 1   2 eV in the observed mass-radius relationship.\\nFor the sample of Parsons et al. (2017), TEdd in (3) is on-average about 40 times the observed surface temperature T (Fig. 1).\\nThe Eddington temperatures shown are roughly consistent TcT of observed and core tempera- with the geometric mean tures. As such, TEdd provides a natural reference to their corre- lations.\\nFollowing a detailed revisit of WD envelopes for the ob- served temperature T at the surface and the central temperature Tc of the core of the WD, Koester (1976) derives a correlation T 4 = 2.05   10 10 (cid:16) c with index   = 2.56, where T T   is in K. Scaled to TEdd, it takes the form\\ng/cm s 2(cid:17)\\nTc    TEdd\\nwith\\n0.02 M 1/4\\n  = 35.7 R1/2\\n0.5\\n(cid:32)\\n40 T TEdd\\n(cid:33) \\n  = 4/    1.56. Here, we use the notation R = 2% R0.02 R  and M = 0.5 M0.5 M  for a fiducial value and taking into account aforementioned mean ratio of TEdd to T .\\nFig. 1 summarizes the distributions of Tc, TEdd and T for the sample of Parsons et al. (2017). These are well below the Fermi level EF of the degenerate electrons supporting the core with characteristic temperature kBTc = z mpc2   0.1   1 MeV, where z = Rg/R is the gravitational redshift of the WD surface according to its gravitational radius Rg = GM/c2, where mp is the proton mass, kB is the Boltzmann constant, and c is the velocity of light.\\n(4)\\n(5)\\nTable 1: Three WDs of essential the same mass showing a clear increase of observed radius with temperature in the sample of Parsons et al. (2017).\\nObject\\nM [M ] R [R ]\\nTeff [K]\\nkBTeff [eV]\\nCSS 0970\\n0.4146\\n0.025\\n30000\\n2.9\\nSDSS J1028+0931\\n0.4146\\n0.018\\n12000\\n1.1\\nSDSS J1210+3347\\n0.4150\\n0.016\\n6000\\n0.52\\n3. Temperature normalized radius\\nFig. 2 shows the data in a mass-radius plot alongside the theoretical zero-temperature limit (1). Highlighted by color is a general trend of increasing radius with temperature. This trend is particularly striking upon considering similar masses. Table 1 lists three WDs of mass M   0.415M  clearly showing a pronounced correlation of apparent radius expanding by   50% with temperature increasing to   5 eV.\\nIn absolute terms, relative to the Fermi level of the electrons (Fig. 1), these temperatures are extremely modest leaving the star essentially unperturbed (Hamada and Salpeter, 1961). For the present sample of Parsons et al. (2017), this suggests, in- stead, a temperature sensitivity in the H atmosphere of WDs considered previously by modeled approaches in Parsons et al. (2017) more likely so than in the degenerate core. We return to this in  7.\\nHere, we circumvent model assumptions by using generic and model-agnostic power-laws for an effective description by a temperature-normalized radius. In doing so, we derive a char- acteristic temperature scale characterizing temperature sensitiv- ity, blind to the underlying physical origin. To be specific, based on Fig. 1 and Table 1, we consider (cid:33) \\n(cid:32)\\nM M \\nR = R0\\nf (T/T0)\\n(6)\\nwith free parameters ( , T0). Here, f (T/T0) is dimensionless and R0 is a constant fixing the dimension of length.\\nStarting point of our approach are effective mass-radius re- lationships of the form R   M  f (T/T0) for some power-law index   and temperature scale T0. Equivalently, this considers a correlation of mass to the scaled radius\\nR  =\\nR f (T/T0)\\n(7)\\nWe apply the temperature scaled radius (7) to fit the data in the form\\nR  = R0 (M/M )  .\\n(8)\\nFor the sample of Parsons et al. (2017), we determine best-fit parameters ( , T0) to the mass-radius data of 26 WDs of type DA, all in eclipsing binaries.\\nThe function f (T/T0) in (6), to be discussed further below, will be a power-law comprising the free parameters ( , T0). In a fit to the mass-radius data, these parameters will be considered over a broad range of values\\n0 < T0 < 9 eV, 0 <   < 1.\\n(9)\\nThe characteristic temperature scale T0 is limited to T0 < 9 eV. In this energy range, radii obtain accurately, while beyond, accuracy diminishes. We keep   < 1, reflecting the assumption that temperature has a secondary impact on the radius. Our best-fit is defined by   estimated using ODR (Orthogonal Dis- tance Regression) and, subsequently, the optimal value of   and T0 at the minimum STD and  2 of residuals.\\nFollowing standard practice, our power-laws (6-8) are ana-\\nlyzed by fits to linear trends in the log-log plane to\\nlog R  =   log M + C.\\nIn fits to data by (10),  2 optimizes both the index   in scaling by a power-law in mass and the constant C, while minimizing STD optimizes only  . In the present analysis quantifying the goodness-of-fit according to residual scatter about a trend line (10), the level shift C along the ordinate - absorbing R0 in (6-8) - is safely ignored and it suffices to determine   by minimization of residuals.\\n4. Temperature-normalized mass-radius relations\\nIn the present model-agnostic approach, we explore fits to the data using three temperature-normalized power-laws. To this end, optimize by  2 and STD in our parameter estimation from fits to the sample of Parsons et al. (2017).\\nThe first power-law Relation-1 is\\nI. f (x) = x ,\\nwhere x = T/T0. Here, T0 acts as a constant because of (9). For this reason,   is not affected by a choice of T0. We find\\n  =  0.954,   = 0.195  2 : STD :   =  0.951,   = 0.190\\nwith the minimum  2 = 0.0183 and, respectively, with residual   = 0.03664. The second power-law Relation-2 is\\nII. f (x) = (1 + x)  .\\nIn contrast to Relation-1, Relation-2 includes the zero temper- ature limit of the Chandrasekhar mass-radius relation (1). Ac- cordingly, T0 is no longer ignorable and is determined in the optimization process in fitting (13) to the data. We find\\n  =  0.969,   = 0.389, T0 = 16226  2 : STD :   =  0.965,   = 0.356, T0 = 13896\\nwith the minimum  2 = 0.0159 and, respectively, with residual   = 0.0344. The third power-law Relation-3 is\\nIII. f (x) = 1 + x ,\\nsimilar but not identical to Relation-2. Relation-3 also includes the Chandrasekhar limit of zero temperature and T0 is not ig- norable. We find\\n  =  0.971,   = 0.690, T0 = 66500,  2 : STD :   =  0.965,   = 0.647, T0 = 65914\\n(10)\\n(11)\\n(12)\\n(13)\\n(14)\\n(15)\\n(16)\\nwith the minimum  2 = 0.0161 and, respectively, a residual   = 0.0346.\\nFig. 3 summarizes our three results, each indicated by color: blue, green and red for Relation-1, Relation-2 and Relation- 3, respectively. The curve represents the optimal   with each T0 at the minimum STD ( 2). The junction point shows the common minimum STD ( 2). Both Relation-2 and Relation-3 leave rather similar residuals (in   and  2) for each of Relation- 2 and Relation-3. To rank these two relations, we proceed as follows.\\n5. Ranking relations by Monte Carlo Analysis\\nIn this section, Relation-2 and Relation-3 are ranked for sig-\\nnificance by Monte Carlo (MC) analysis.\\nMC analysis is a useful method for robust parameter estima- tion and ranking relations in the face of measurement uncertain- ties. Though ODR methods can be used also to estimate param- eters, the results do not necessarily agree, making it difficult to rank Relation-2 and Relation-3 for their relative significance.\\nIn this light, we pursue MC analysis by creating synthetic data by randomly selecting samples of varying radius, mass, and temperature within the measurement confidence intervals,\\nXnew = X + N(0,  ).\\nFollowing this procedure, we estimate  ,  , and T0 without con- sidering errors in ODR. Results extended over 5M calculations are used to determine a ranking of Relation-2 and Relation-3 according to STD or  2) (Fig. 4) and infer a probability P of relative significance by counting the total number of times ei- ther one has preferred rank (Table 2). The MC simulation also provides accurate values in the mean of  ,  , and T0 over the total number of iterations.\\nIn our MC analysis, we consider realizations of arrays of 26   3 = 78 entries, comprising mass, radius and tempera- ture of the 26 WDs. As observed quantities, mass, radius and temperature data are independent. An accordingly fair (unbi- ased) draw of realizations extends over random draws from 98 confidence intervals, unconstrained and independently, blind to physical meaning and pre-conceived notions of correlations. In our analysis, the range of allowed values is densely covering by using a very large number of 5M iterations.\\nTable 2 shows the output of our MC analysis. The results indicate that the WD radius is firstly determined by mass more so than temperature. Relation-2 and Relation-3 have similar   values, but   and T0 are notably distinct with otherwise similar residuals in  2 and STD (Figs. 2-5).Table 2 includes Relation-1, results for which are consistent with the above discussion.\\n6. Interpretation of Results\\nEclipsing binaries allow precise measurements of WD mass and radius (Bours et al., 2016; Parsons et al., 2017). How- ever, as in Fig. 2, the theoretical mass-radius relation (1) pro- vides a lower bound. It is generally not the greatest lower bound by significant departures for hot, low-mass white dwarfs\\n(17)\\nFigure 3: Tracks of STD residuals (left panel) and  2 (right panel) for the Relation-1 (blue), Relation-2 (green) and Relation-3 (red) as a function of the normalized temperature power-law index  , following minimization over all T0 in (9). Note that Relation-1 has no explicit dependence on T0. Both cost functions produce very similar results for   and  .\\nTable 2: Monte Carlo analysis on Relations 1-3 with ranking by probability P of having the lowest residual in  2 (left) or STD (right) for the same data (Fig. 4, synthesized over 5M representations).\\nSTD\\nRelation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3\\nR0/R \\n7.6   10 3\\n6.2   10 3\\n5.8   10 3\\n8.4   10 4\\n6.2   10 3\\n5.8   10 3\\n0.951\\n 0.966\\n 0.967\\n0.951\\n 0.965\\n 0.966\\n0.194\\n0.409\\n0.690\\n0.193\\n0.375\\n0.685\\n T0\\n25000\\n19000\\n65000\\nconst\\n15000\\n64000\\n  1%\\n98.7%\\n  1%\\n  1%\\n98.6%\\n  1%\\nFigure 4: Probability of Relation-2 to have a smaller residual in  2 (continuous curve) and STD (dashed curve) than Relation-3 in a MC analysis extending over a large number of synthetic data sets. Both cost functions support the conclusion that Relation-2 is preferred over Relation-3 in providing a model- agnostic fit to the mass-radius data of WDs.\\n(M/M    0.5). The gray region in Fig. 2 (Table 1) is il- lustrative, highlighting a pronounced trend in observed WD radius with temperatures at otherwise very similar masses in M/M  < 0.5. Evidently, this trend cannot be explained by the zero-temperature mass-radius relationship.\\nSeveral theoretical calculations (de Carvalho et al., 2014; Boshkayev, 2018, 2019) have been advanced to explain this de- parture. While rotation and density affects the radius, the ra- dius of relatively dense WDs is not significantly influenced by temperature even though such is more so for low-density or ro- tating WDs compared to their high-density counter parts. Our a model-agnostic study of T0 identify a characteristic temper- ature in the expansion of the radius of the photosphere. It re- veals a consistent trend wherein WDs with higher temperatures exhibit relatively larger radius, clearly apparent in the overall trend (Benvenuto and Althaus, 1999; Panei et al., 2007; Par- sons et al., 2017; Joyce et al., 2018; Zenati et al., 2019; Romero et al., 2019).\\nIn a novel model-agnostic revisit of the mass-radius relation- ship, we quantify this temperature dependence by a temperature scale T0 = (1   2) eV in Relation-2. Relation-2 is found to be statistically more significant than Relation-3 based on our MC analysis ( 4). Over 5M iterations, Fig.4 shows Relation-2 to have a lower  2 and lower   than Relation-3 at a probability of 98.7%, respectively, 98.6% . In the present approach, we circumvent potential systematic uncertainties otherwise present in model-dependent approaches (Parsons et al. 2017 and refer- ences therein).\\nWe summarize our approach in Fig. 5 (A-C). Panel (A) shows the discrepancy between the theoretical mass-radius re- lation at zero temperature (1) and the observed radius. Panel (B) shows a fit to the mass to the un-normalized radius, re- vealing scatter than clearly exceeds that of measurement uncer- tainty. Panel (C) shows the result of a linear relation between the temperature-normalized radius R  and (M/M ) . At rela- tively small residual scatter, this result identifies a temperature scale T0 and a radius primarily determined by mass. Accord- ing to Table 2 and in the notation of (1), we infer a mass-radius relation\\nR = C\\n(cid:32)\\nM M \\n(cid:33) 1/3\\n(18)\\nFigure 5: Mass-radius plots of the data (black dots). Panel (A) highlights the deviation from the theoretical zero-temperature limit (1). Panel (B) highlights the excess scatter in the data following a fit (red line) to the un-normalized radius R   M . Panel (C) shows a fit (green line) to our temperature-normalized radius R    M  in Relation-2 (13). A small residual scatter (cf. Fig. 3) evidences the effectiveness of our normalization in Relation-2, except for three outliers with relatively large observational uncertainties. All three panels A-C include the theoretical relation (blue dotted line). Bottom panels show the adjusted radius in our normalization produced by minimization of  2 (left) and STD (right). The adjustment by our temperature normalization in R  = R/ f (x), x = T/T0, is about 23% (  = 1.76) and 28% (  = 2.05) in Relation-2 and, respectively, Relation-3 in  2 optimization. The same is 24% (  = 1.82) and 29% (  = 2.07) in Relation-2, respectively, Relation-3 in STD optimization. This adjustment is most relevant at high temperatures.\\nwith the temperature-dependent expansion of an atmosphere in- cluded in\\nC   C0\\n(cid:32)\\n1 + T T0\\n(cid:33)1/3 (cid:32)\\nM M \\n(cid:33) 2/3\\naccording to Relation-2, parameterized by STD in the approxi- mations   =  0.965    1 and   = 0.375   1/3.\\n(19)\\nOur model-agnostic analysis hereby effectively reveals the presence of non-degenerate atmosphere, sufficiently massive and opaque, to account for the observed temperature sensitivity in Fig. 2 parameterized by above-mentioned T0. Indeed, further confirmation can be found in consistency of the present Parsons et al. (2017) data with the detailed model for H atmospheres of B edard et al. (2020).\\n7. Conclusions and Outlook\\nWe summarize the apparent mass-radius relation (18) with temperature dependent coefficient (19) due to this atmosphere by including Relation-2 as a factor modifying C0 in (19).\\nA principal outcome of our model-independent study is a temperature scale T0 = 1   2 eV in temperature sensitivity of the photospheric radius of the WDs of type DA in Parsons et al. (2017), shown in Fig. 5 and summarized in (18-19).\\nT0 derives from 1.6 eV and 1.3 eV according to  2 and, re- spectively, STD in fits of Relation-2 to the data. T0 appears to be particularly relevant to low mass M/M    0.5 in the present sample.\\nWhile the Parsons et al. (2017) sample of WDs covers a size- able range in masses with distinct temperature sensitivity be- low M/M    0.5 and above M/M    0.5, they nevertheless are of relatively high mass-density      0 (Fig. 6). Cru- cially, densities for the entire at hand are above the threshold  0 = 105 g cm 3 above which the mass-radius relation of the degenerate core is expected to be insensitive to temperature (Boshkayev, 2018). In this sense, the present sample is rela- tively homogeneous and does not test temperature sensitivity of the core beyond what is expected from (1).\\nFor the above-mentioned low-mass WDs, a non-degenerate atmosphere effectively insulates a core at super-Eddington tem- peratures (4) from the observed sub-Eddington temperatures T by virtue of an atmosphere that is sufficiently massive and opaque. In the following scaling, we shall ignore a potential role of a corona due to magnetic fields (Aznar Cuadrado et al., 2004; Jordan et al., 2007; Ferrario et al., 2020).\\nFollowing  2, the Eddington temperature introduces a scale height hE by virtue of thermal kinetic energy of the ions, distinct from escape by radiation pressure. For a hydrogen atmosphere,\\nFigure 6: (Top panel.) For the WD sample of Parsons et al. (2017), shown are the expected relative expansion of a non-degenerate atmosphere, sufficiently massive and opaque, surrounding a degenerate core based on super-Eddington temperature Tc inferred from the Koester (1976) correlation to the observed temperature T (Fig. 1). The Parsons et al. (2017) sample satisfies TEdd     TcT , reconciling Tc   TEdd with T   TEdd, leaving T on the order of a few eV. (Lower panel.) For the entire Parsons et al. (2017) sample, the mass- densities of the core according to the theoretical zero-temperature relationship (1) are above the threshold 105 g cm 3 of Boshkayev (2018), where temperature sensitivity is negligible.\\nwe have\\nhE  \\nkBTEdd mpg\\n= 13 km R2\\n0.02 M 1\\n0.5   0.3%RWD.\\nBy high thermal conductivity, the core is believed to be at an essentially uniform temperature. As a result, the atmosphere assumes a mean temperature  TEdd with      . By (20), this implies a height\\nH =   hE   10% RWD.\\nThis expected height (Fig. 6) introduces a radial expansion qualitatively consistent with the data (Fig. 5).\\nIn this light, the relatively modest characteristic temperature T0 = 1   2 eV in (18-19) can be identified with the core temper- ature higher by a factor of Tc/T   (TEdd/T )2 = O based on Figs. 1-2 and  2.\\n(cid:16)\\n103(cid:17)\\nThe heat transport from core to surface in such atmospheres gives rise to a complex mass-radius relationship, here described by (18-19). This appears to be particular relevant to low-mass WDs, essentially below the mean of the WD mass distribution. Above, the relatively high-mass WDs appear to follow the the- oretical mass-radius relation (1), evidencing the absence of an atmosphere and/or a relatively low temperature core. The origin of this discrepancy appears to be beyond the present considera- tions, that might involve a distinct composition and associated formation history. Derived for WDs of type DA, our results may serve as a reference for similar model-independent analysis of WDs of different type.\\n(20)\\n(21)\\nAcknowledgements\\nThe authors thank the anonymous reviewer for constructive comments which greatly contributed to clarity of presentation, and thank M.A. Abchouyeh for stimulating discussions. This work is supported, in part, by NRF No. RS-2024-00334550.\\nReferences\\nAznar Cuadrado, R., Jordan, S., Napiwotzki, R., Schmid, H.M., Solanki, S.K., Mathys, G., 2004. Discovery of kilogauss magnetic fields in three DA white dwarfs. A&A 423, 1081 1094. doi:10.1051/0004-6361:20040355, arXiv:astro-ph/0405308.\\nBaiko, D.A., Yakovlev, D.G., 2019. Quantum ion thermodynamics in liquid interiors of white dwarfs. MNRAS 490, 5839 5847. doi:10.1093/mnras/ stz3029, arXiv:1910.06771.\\nB edard, A., 2024.\\nThe spectral evolution of white dwarfs: where do doi:10.1007/s10509-024-04307-5,\\nwe stand? arXiv:2405.01268.\\nAp&SS 369, 43.\\nB edard, A., Bergeron, P., Brassard, P., Fontaine, G., 2020. On the Spec- tral Evolution of Hot White Dwarf Stars. I. A Detailed Model Atmo- sphere Analysis of Hot White Dwarfs from SDSS DR12. ApJ 901, 93. doi:10.3847/1538-4357/abafbe, arXiv:2008.07469.\\nBenvenuto, O.G., Althaus, L.G., 1999. Grids of white dwarf evolutionary mod- els with masses from M=0.1 to 1.2 m solar. MNRAS 303, 30 38. doi:10. 1046/j.1365-8711.1999.02215.x, arXiv:astro-ph/9811414.\\nBoshkayev, K., 2018. Equilibrium Configurations of Rotating White Dwarfs at Finite Temperatures. Astronomy Reports 62, 847 852. doi:10.1134/ S106377291812017X, arXiv:1807.00332.\\nBoshkayev, K., 2019.\\nStatic and rotating white dwarfs at finite temper- arXiv e-prints , arXiv:1909.10899doi:10.48550/arXiv.1909.\\natures. 10899, arXiv:1909.10899.\\nBoshkayev, K., O., L., M., M., H., Q., 2021. Static and rotating white dwarfs at finite temperatures. IJMPh , 61doi:10.26577/ijmph.2021.v12.i2.07, arXiv:1909.10899.\\nBoshkayev, K., Rueda, J.A., Ruffini, R., Siutsou, I., 2015. General Rela- tivistic and Newtonian White Dwarfs, in: Thirteenth Marcel Grossmann Meeting: On Recent Developments in Theoretical and Experimental Gen- eral Relativity, Astrophysics and Relativistic Field Theories, pp. 2468 2474. doi:10.1142/9789814623995$_-$0472, arXiv:1503.04171.\\nBoshkayev, K.A., Rueda, J.A., Zhami, B.A., Kalymova, Z.A., Balgymbekov, G.S., 2016. Equilibrium structure of white dwarfs at finite temperatures, in: International Journal of Modern Physics Conference Series, p. 1660129. doi:10.1142/S2010194516601290, arXiv:1510.02024.\\nBours, M.C.P., Marsh, T.R., Parsons, S.G., Dhillon, V.S., Ashley, R.P., Bento, J.P., Breedt, E., Butterley, T., Caceres, C., Chote, P., Copperwheat, C.M., Hardy, L.K., Hermes, J.J., Irawati, P., Kerry, P., Kilkenny, D., Littlefair, S.P., McAllister, M.J., Rattanasoon, S., Sahman, D.I., Vu ckovi c, M., Wilson, R.W., 2016. Long-term eclipse timing of white dwarf binaries: an observa- tional hint of a magnetic mechanism at work. MNRAS 460, 3873 3887. doi:10.1093/mnras/stw1203, arXiv:1606.00780.\\nde Carvalho, S.M., Rotondo, M., Rueda, J.A., Ruffini, R., 2014. Relativistic feynman-metropolis-teller treatment at finite temperatures. Phys. Rev. C 89, 015801. URL: https://link.aps.org/doi/10.1103/PhysRevC.89. 015801, doi:10.1103/PhysRevC.89.015801.\\nde Carvalho, S.M., Rotondo, M., Rueda, J.A., Ruffini, R., 2014. Relativistic Feynman-Metropolis-Teller treatment at finite temperatures. Phys. Rev. C 89, 015801. doi:10.1103/PhysRevC.89.015801.\\nDhillon, V.S., Marsh, T.R., Atkinson, D.C., Bezawada, N., Bours, M.C.P., Copperwheat, C.M., Gamble, T., Hardy, L.K., Hickman, R.D.H., Irawati, P., Ives, D.J., Kerry, P., Leckngam, A., Littlefair, S.P., McLay, S.A., O Brien, K., Peacocke, P.T., Poshyachinda, S., Richichi, A., Soonthorn- thum, B., Vick, A., 2014. ULTRASPEC: a high-speed imaging photome- ter on the 2.4-m Thai National Telescope. MNRAS 444, 4009 4021. doi:10.1093/mnras/stu1660, arXiv:1408.2733.\\nDhillon, V.S., Marsh, T.R., Stevenson, M.J., Atkinson, D.C., Kerry, P., Pea- cocke, P.T., Vick, A.J.A., Beard, S.M., Ives, D.J., Lunney, D.W., McLay, S.A., Tierney, C.J., Kelly, J., Littlefair, S.P., Nicholson, R., Pashley, R.,\\nHarlaftis, E.T., O Brien, K., 2007. ULTRACAM: an ultrafast, triple- beam CCD camera for high-speed astrophysics. MNRAS 378, 825 840. doi:10.1111/j.1365-2966.2007.11881.x, arXiv:0704.2557.\\nFerrario, L., Wickramasinghe, D., Kawka, A., 2020. Magnetic fields in isolated and interacting white dwarfs. Advances in Space Research 66, 1025 1056. doi:10.1016/j.asr.2019.11.012, arXiv:2001.10147.\\nFontaine, G., Brassard, P., Bergeron, P., 2001. The Potential of White Dwarf\\nCosmochronology. PASP 113, 409 435. doi:10.1086/319535.\\nGentile Fusillo, N.P., Tremblay, P.E., Cukanovaite, E., Vorontseva, A., Lalle- ment, R., Hollands, M., G ansicke, B.T., Burdge, K.B., McCleery, J., Jordan, S., 2021. A catalogue of white dwarfs in Gaia EDR3. MNRAS 508, 3877  3896. doi:10.1093/mnras/stab2672, arXiv:2106.07669.\\nHamada, T., Salpeter, E.E., 1961. Models for Zero-Temperature Stars. ApJ\\n134, 683. doi:10.1086/147195.\\nHan, Z., Podsiadlowski, P., Maxted, P.F.L., Marsh, T.R.,\\nIvanova, N., The origin of subdwarf B stars - I. The formation channels. doi:10.1046/j.1365-8711.2002.05752.x,\\n2002. MNRAS 336, 449 466. arXiv:astro-ph/0206130.\\nHearn, A.G., Mewe, R., 1976. The corona around the white dwarf Sirius B\\ndetermined from X-ray measurements. A&A 50, 319 321.\\nIben, I., J., Tutukov, A.V., 1985. On the evolution of close binaries with components of initial mass between 3 M and 12 M. ApJS 58, 661 710. doi:10.1086/191054.\\nIstrate, A.G., Marchant, P., Tauris, T.M., Langer, N., Stancliffe, R.J., Grassitelli, L., 2016. Models of low-mass helium white dwarfs including gravitational settling, thermal and chemical diffusion, and rotational mixing. A&A 595, A35. doi:10.1051/0004-6361/201628874, arXiv:1606.04947.\\nJordan, S., Aznar Cuadrado, R., Napiwotzki, R., Schmid, H.M., Solanki, S.K., 2007. The fraction of DA white dwarfs with kilo-Gauss magnetic fields. A&A 462, 1097 1101. doi:10.1051/0004-6361:20066163, arXiv:astro-ph/0610875.\\nJoyce, S.R.G., Barstow, M.A., Casewell, S.L., Burleigh, M.R., Holberg, J.B., Bond, H.E., 2018. Testing the white dwarf mass-radius relation and com- paring optical and far-UV spectroscopic results with Gaia DR2, HST, and FUSE. MNRAS 479, 1612 1626. doi:10.1093/mnras/sty1425, arXiv:1806.00061.\\nKepler, S.O., Pelisoli, I., Koester, D., Reindl, N., Geier, S., Romero, A.D., Ourique, G., Oliveira, C.d.P., Amaral, L.A., 2019. White dwarf and subd- warf stars in the Sloan Digital Sky Survey Data Release 14. MNRAS 486, 2169 2183. doi:10.1093/mnras/stz960, arXiv:1904.01626.\\nKoester, D., 1976. Convective Mixing and Accretion in White Dwarfs. A&A\\n52, 415.\\nKoester, D., 2002. White dwarfs: Recent developments. A&A Rev. 11, 33 66.\\ndoi:10.1007/s001590100015.\\nKoester, D., Chanmugam, G., 1990. Physics of white dwarf stars. Reports on\\nProgress in Physics 53, 837.\\nKoester, D., Chanmugam, G., 1990. REVIEW: Physics of white dwarf stars. Reports on Progress in Physics 53, 837 915. doi:10.1088/0034-4885/ 53/7/001.\\nKoester, D., Schulz, H., Weidemann, V., 1979. Atmospheric parameters and\\nmass distribution of DA white dwarfs. A&A 76, 262 275.\\nMarsh, T.R., Dhillon, V.S., Duck, S.R., 1995. Low-Mass White Dwarfs Need Friends - Five New Double-Degenerate Close Binary Stars. MNRAS 275, 828. doi:10.1093/mnras/275.3.828.\\nNelemans, G., Zwart, S., Verbunt, F., Yungelson, L., 2001. Population synthesis for double white dwarfs. ii. semi-detached systems: Am cvn stars. arXiv preprint astro-ph/0101123 .\\nPanei, J.A., Althaus, L.G., Chen, X., Han, Z., 2007. Full evolution of low- mass white dwarfs with helium and oxygen cores. MNRAS 382, 779 792. doi:10.1111/j.1365-2966.2007.12400.x.\\nParsons, S.G., G ansicke, B.T., Marsh, T.R., Ashley, R.P., Bours, M.C.P., Breedt, E., Burleigh, M.R., Copperwheat, C.M., Dhillon, V.S., Green, M., Hardy, L.K., Hermes, J.J., Irawati, P., Kerry, P., Littlefair, S.P., McAllis- ter, M.J., Rattanasoon, S., Rebassa-Mansergas, A., Sahman, D.I., Schreiber, M.R., 2017. Testing the white dwarf mass-radius relationship with eclips- ing binaries. MNRAS 470, 4473 4492. doi:10.1093/mnras/stx1522, arXiv:1706.05016.\\nPei, T.H., 2022. The Highly Accurate Relation between the Radius and Mass of the White Dwarf Star from Zero to Finite Temperature. Frontiers in As- tronomy and Space Sciences 8, 243. doi:10.3389/fspas.2021.799210. Ren, J.J., Rebassa-Mansergas, A., Parsons, S.G., Liu, X.W., Luo, A.L., Kong,\\nX., Zhang, H.T., 2018. White dwarf-main sequence binaries from LAM- OST: the DR5 catalogue. MNRAS 477, 4641 4654. doi:10.1093/mnras/ sty805, arXiv:1803.09523.\\nRomero, A.D., Kepler, S.O., Joyce, S.R.G., Lauffer, G.R., C orsico, A.H., 2019. The white dwarf mass-radius relation and its dependence on the hydro- gen envelope. MNRAS 484, 2711 2724. doi:10.1093/mnras/stz160, arXiv:1901.04644.\\nRotondo, M., Rueda, J.A., Ruffini, R., Xue, S.S., 2011. Relativistic Feynman- Metropolis-Teller theory for white dwarfs in general relativity. Phys. Rev. D 84, 084007. doi:10.1103/PhysRevD.84.084007, arXiv:1012.0154. Str omgren, B., 1939. Book Review: An Introduction to the Study of Stellar\\nStructure, by S. Chandrasekhar. Popular Astronomy 47, 287.\\nTremblay, P.E., Gentile-Fusillo, N., Raddi, R., Jordan, S., Besson, C., G ansicke, B.T., Parsons, S.G., Koester, D., Marsh, T., Bohlin, R., Kali- rai, J., Deustua, S., 2017. The Gaia DR1 mass-radius relation for white dwarfs. MNRAS 465, 2849 2861. doi:10.1093/mnras/stw2854, arXiv:1611.00629.\\nVerbunt, F., Rappaport, S., 1988. Mass Transfer Instabilities Due to Angular Momentum Flows in Close Binaries. ApJ 332, 193. doi:10.1086/166645. Vernet, J., Dekker, H., D Odorico, S., Kaper, L., Kjaergaard, P., Hammer, F., Randich, S., Zerbi, F., Groot, P.J., Hjorth, J., Guinouard, I., Navarro, R., Adolfse, T., Albers, P.W., Amans, J.P., Andersen, J.J., Andersen, M.I., Bi- netruy, P., Bristow, P., Castillo, R., Chemla, F., Christensen, L., Conconi, P., Conzelmann, R., Dam, J., de Caprio, V., de Ugarte Postigo, A., Delabre, B., di Marcantonio, P., Downing, M., Elswijk, E., Finger, G., Fischer, G., Flores, H., Franc ois, P., Goldoni, P., Guglielmi, L., Haigron, R., Hanen- burg, H., Hendriks, I., Horrobin, M., Horville, D., Jessen, N.C., Kerber, F., Kern, L., Kiekebusch, M., Kleszcz, P., Klougart, J., Kragt, J., Larsen, H.H., Lizon, J.L., Lucuix, C., Mainieri, V., Manuputy, R., Martayan, C., Mason, E., Mazzoleni, R., Michaelsen, N., Modigliani, A., Moehler, S., M ller, P., Norup S rensen, A., N rregaard, P., P eroux, C., Patat, F., Pena, E., Pragt, J., Reinero, C., Rigal, F., Riva, M., Roelfsema, R., Royer, F., Sacco, G., Santin, P., Schoenmaker, T., Spano, P., Sweers, E., Ter Horst, R., Tintori, M., Tromp, N., van Dael, P., van der Vliet, H., Venema, L., Vidali, M., Vinther, J., Vola, P., Winters, R., Wistisen, D., Wulterkens, G., Zacchei, A., 2011. X-shooter, the new wide band intermediate reso- lution spectrograph at the ESO Very Large Telescope. A&A 536, A105. doi:10.1051/0004-6361/201117752, arXiv:1110.1944.\\nZenati, Y., Toonen, S., Perets, H.B., 2019. Formation and evolution of hy- brid He-CO white dwarfs and their properties. MNRAS 482, 1135 1142. doi:10.1093/mnras/sty2723, arXiv:1803.04444.\\n', source='https://arxiv.org/pdf/2408.15291', source_type=<SourceType.application_pdf: 'application/pdf'>, num_chunks=43, metadata={}, chunks=[ResponseChunk(id='chunk_58505e03-ba07-44cf-ba7d-891e4d71a3d6', content='4 2 0 2 g u A 6 2 R S . h p - o r t s a [ 1 v 1 9 2 5 1 . 8 0 4 2 : v i X r a A temperature scale of 1 2 eV in the mass-radius relationship of white dwarfs of type DA Jin Lima, Ji-Yu Kima, Maurice H.P.M. van Puttena,1, aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea Abstract The mass-radius relationship of white dwarfs (WDs) is one of their defining characteristics, largely derived from electron degen- eracy pressure. We present a model-independent study of the observed mass-radius relationship in WD binaries of Parsons et al. (2017), listing data over a broad temperature range up to about 60,000 K (5 eV). The data show an appreciable temperature sen- sitivity with pronounced intrinsic scatter (beyond measurement uncertainty) for the canonical He-models with proton to neutron ratio 1:1. We characterize temperature sensitivity by a temperature scale T0 in model-agnostic power-law relations with tempera- ture normalized radius. For low-mass WDs, the results identify a remarkably modest T0 = 1   2 eV.', chunk_index=1, num_tokens=283, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_768b620d-721c-4080-97e1-f5c610355889', content='We comment on a potential interpretation for atmospheres insulating super-Eddington temperature cores from the sub-Eddington photospheres of low-mass WDs. Keywords: white dwarfs, mass-radius relation, Chandrasekhar 1. Introduction White dwarfs (WDs) represent the final evolutionary stage of stars with masses in the range of 0.4M  < M < 8M . As such, they are quite numerous. The Gaia Early Data Release 3 (EDR3) contains 359,073 WD candidates (Gentile Fusillo et al., 2021). EDR3 extends the survey of 29,294 WDs of the Sloan Digital Sky Survey (SDSS) Data Release 16 (DR 16) by over an order of magnitude with 25,176 WDs in both surveys. WDs are variously classified by their atmospheric composition (Gen- tile Fusillo et al. 2021). Most are of type DA characterized by a dominance of hydrogen lines in their spectra. It reveals an opaque hydrogen atmosphere, whose observed temperatures are well below the Eddington limit (Fig. 1). WDs have a distinct mass-radius relationship with a lower bound defined by electron degeneracy pressure (Str omgren, 1939).', chunk_index=2, num_tokens=264, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_d5aab498-02ef-4111-8267-a1121742edc0', content='It is described by the equation of state (EoS) of de- generacy pressure of a Fermi gas, the relativistic limit of which defines the Chandrasekhar mass-limit. To leading order, WDs are modeled by the ideal Fermi gas at zero temperature sur- rounded by a non-degenerate atmosphere (Koester and Chan- mugam, 1990). At zero temperature, the degenerate WD core is effectively parameterized only by a proton-to-neutron ratio. In this limit, the mass-radius relation satisfies (Str omgren, 1939; Hamada and Salpeter, 1961; Koester et al., 1979) R = C0 (cid:32) M M (cid:33) 1/3 where C0 = (cid:32) 16G3m3 e 81 2 6 (cid:33) 1/3 (cid:32) Amp Z (cid:33) 5/3 M 1/3 0.01 R , Email address: mvp@sejong.ac.kr (Maurice H.P.M. van Putten) 1INAF-OAS Bologna via P.', chunk_index=3, num_tokens=250, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_2253dadd-9431-4dd8-b23c-1889730ec4d6', content='Gobetti 101 I-40129 Bologna Italy, Italy (1) (2) where G is Newton s constant,   is Planck s constant, me and mp are the masses of the electron and proton, A is the atomic number and Z denotes the number of protons in a nucleus - here, mostly alpha-nuclei. We have set the A/Z ratio to 2, since many WDs consist of a C/O or He core, the latter in particular expected for low-mass WDs (Iben and Tutukov, 1985; Marsh et al., 1995; Nelemans et al., 2001; Han et al., 2002; Istrate et al., 2016; Ren et al., 2018; Zenati et al., 2019). According to this calculation, R = C0M 1/3 holds. Fig. 2 shows the observed mass-radius relation for 26 WDs of type DA, all in eclipsing binaries, sampled by Parsons et al. (2017) by photometric and spectroscopic observations.', chunk_index=4, num_tokens=229, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_5b3e8cb8-5037-41c1-b6b1-6f17783971ff', content='Photo- metric observations use, for example, the Ultrafast Triple-beam CCD Camera (ULTRACAM, Dhillon et al. 2007) and its spec- troscopic version ULTRASPEC (Dhillon et al., 2014), currently in use as the high-speed imaging photometer on the Thai Na- tional Telescope (TNT). Spectroscopic observations have been performed by X-shooter (Vernet et al., 2011) in ESO Very Large Telescope (VLT). Fig. 2 shows the theoretical mass-radius relation of the de- generate core (1) to provide a lower bound. It effectively pro- vides a greatest lower bound only for M/M    0.5, even though it represents an rather elementary model of degeneracy pres- sure. The data clearly show a temperature sensitivity at rel- atively low mass M/M    0.5, see also Fig. 9 in Parsons et al. (2017). Crucially, the temperatures involved (Fig. 1) are far below the characteristic energy 0.1   1 Me V of the Fermi energies of the electrons in the degenerate core.', chunk_index=5, num_tokens=247, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_8ab7b24d-2c1d-43af-bb01-58ae0a1c00d0', content='For the present temperature range and low-mass WDs, any finite temperature corrections to the EoS (de Carvalho et al., 2014; Boshkayev, 2018; Boshkayev et al., 2021), including relativis- tic corrections, will be accordingly small for any generaliza- tions beyond (1), notably in Hamada and Salpeter (1961), Ro- tondo et al. (2011), de Carvalho et al. (2014), Boshkayev (2018) Preprint submitted to New Astronomy August 29, 2024 Figure 1: (Top panel.) Observed, Eddington and core temperatures of the 26 WDs of type DA in the sample of Parsons et al. (2017). Core temperatures are inferred from the Koester (1976) correlation for an optically thick atmosphere, insulating a C/O or He core at super-Eddington temperatures. (Lower panel.) Same data plotted as a function of mass with trends at slope 1.10 (Observed, blue), 1.18 (Eddington, red) and -0.11 (Core, brown).', chunk_index=6, num_tokens=252, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_45ec6ba0-516a-4ccd-81ef-90324e508a30', content='Eddington temperatures are roughly consistent with the geometric mean of observed and core tempera- tures. and Baiko and Yakovlev (2019); see further Koester and Chan- mugam (1990); Koester (2002). Moreover, at sufficiently high density, the mass-radius relationship of the core becomes uni- versal, independent of the details of the EoS. Instead, the origin of temperature sensitivity in the mass- radius relationship may be found in a non-degenerate atmo- sphere, if present. In particular, a finite temperature sensitivity is expected from an atmosphere about a core at super-Eddington temperatures - allowed for a sufficiently massive and optically thick atmospheres (Fontaine et al., 2001). A variety of studies have been conducted to find solutions thereto and gain a deeper understanding of, e.g., an H enve- lope and/or evolution models depending on core composition (Hamada and Salpeter, 1961; Hearn and Mewe, 1976; Verbunt and Rappaport, 1988; Benvenuto and Althaus, 1999; Fontaine et al., 2001; Panei et al., 2007; Boshkayev et al., 2015; Par- sons et al., 2017; Kepler et al., 2019; Pei, 2022).', chunk_index=7, num_tokens=290, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_33872860-7624-41a0-aa74-41472b09e0ee', content='Among these studies, various convection theories have been advanced, also to explain cooling times and evolutionary processes of WDs in- cluding the thermal insulation provided by their non-degenerate atmospheres. For instance, hot WDs in SDSS DR12 have been analyzed with models of cooling and atmospheres (B edard et al., 2020). Non-Local Thermal Equilibrium (non-LTE) atmospheres and synthetic WD spectra reveal a correlation between surface grav- ity and effective temperature. These modeled approaches, however, are intricate with po- tentially systematic uncertainties in the detailed structure of non-degenerate atmospheres, mediating heat transport by radi- ation and convection (B edard, 2024). For this reason, we set out the present model-agnostic study based on spectroscopic and photometric data, to further our understanding of the mass- radius relationship (Tremblay et al., 2017; Boshkayev et al., 2016). For the recent sample of 26 WDs of Parsons et al. (2017) (Fig. 2), we set out to derive a temperature scale T0, charac- terizing temperature sensitivity by exploring various power law relations for the mass-radius relations. The resulting T0 may serve as a novel observational constraint in future studies.', chunk_index=8, num_tokens=273, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_1d04c8a4-ebe7-4f09-a98d-f9b146ef3efe', content='In  2, we recall some preliminaries of the Eddington temper- ature and the Koester (1976) correlation of core and observed temperature. In  3, we introduce a temperature normalized ra- dius, to be used in power-law fits to the data based on two cost functions:  2 and residual Standard deviation (STD) defined by minimal least square errors. In  4, introduce three temperature- normalized power-laws and consider their fits to data in the log- log plane to effectively describe the expansion of apparent ra- dius with normalized temperature. In  5, two of the three re- lationships are ranked by probability of significance by Monte Carlo analysis. In  6, we interpret the results and summarize our findings with an outlook for future studies in  7. 2. The intermediate Eddington temperature Fig. 1 shows, as expected, the observed temperatures T to be strictly below the Eddington temperature, TEdd. After all, the modest observed temperatures on the order of a few eV im- ply the existence of an atmosphere. A reverse inequality would imply rapid evaporation by radiation pressure acting on the op- tically thin outer-most layers of any atmosphere.', chunk_index=9, num_tokens=257, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_113b456c-4c52-497a-a3a6-67cfe48d2da3', content='The Eddington temperature TEdd is defined by equating the Eddington luminosity LEdd = 3.2   104 (M/M ) L  to the lumi- nosity from a sphere of radius R. That is, LEdd = 4 R2 T 4 Edd, where   = 5.67   10 5g s 3K 4 is the Stefan-Boltzmann con- stant. This defines TEdd = 39.5 (cid:32) g 5000 g (cid:33) 1 4 by surface gravity g = GM/R2 with a fiducial value for M = 0.5M , R = 2%R , scaled to the solar value g  = GM /R2  . (3) Figure 2: Mass-radius plot with temperature (color) in the sample of 26 WDs of type DA (Parsons et al., 2017). NN Ser is the hottest and SDSS J0138- 0016 is the coldest at 63,000K and, respectively, 3,570 K. For reference, it includes the theoretical zero-temperature limit (1) (dashed black line). The vertical grey region highlights three WDs of essentially the same mass with pronounced expansion in apparent radius with temperature.', chunk_index=10, num_tokens=280, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_2b773722-3d8c-43bc-9102-14a068282b17', content='A significant de- parture is seen between observed and the expected radius (1), especially at low mass M/M    0.5. Included are fits to the data by a model-agnostic power- law Relation-2 (dotted colored curves,  3) studied in the present work with isothermals covering 5,000-65,000 K (bottom to top in steps of 10,000 K) in observed temperature. Relation-2 identifies a characteristic temperature scale T0 = 1   2 eV in the observed mass-radius relationship. For the sample of Parsons et al. (2017), TEdd in (3) is on-average about 40 times the observed surface temperature T (Fig. 1). The Eddington temperatures shown are roughly consistent TcT of observed and core tempera- with the geometric mean tures. As such, TEdd provides a natural reference to their corre- lations. Following a detailed revisit of WD envelopes for the ob- served temperature T at the surface and the central temperature Tc of the core of the WD, Koester (1976) derives a correlation T 4 = 2.05   10 10 (cid:16) c with index   = 2.56, where T T   is in K.', chunk_index=11, num_tokens=271, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_6433b8e1-4054-487c-bb60-5f1c0df14e5d', content='Scaled to TEdd, it takes the form g/cm s 2(cid:17) Tc    TEdd with 0.02 M 1/4 = 35.7 R1/2 0.5 (cid:32) 40 T TEdd (cid:33) = 4/    1.56. Here, we use the notation R = 2% R0.02 R  and M = 0.5 M0.5 M  for a fiducial value and taking into account aforementioned mean ratio of TEdd to T . Fig. 1 summarizes the distributions of Tc, TEdd and T for the sample of Parsons et al. (2017). These are well below the Fermi level EF of the degenerate electrons supporting the core with characteristic temperature kBTc = z mpc2   0.1   1 MeV, where z = Rg/R is the gravitational redshift of the WD surface according to its gravitational radius Rg = GM/c2, where mp is the proton mass, kB is the Boltzmann constant, and c is the velocity of light. (4) (5) Table 1: Three WDs of essential the same mass showing a clear increase of observed radius with temperature in the sample of Parsons et al. (2017).', chunk_index=12, num_tokens=275, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_5b9df0c1-e6c0-4f99-9737-eff797fdccd5', content='Object M [M ] R [R ] Teff [K] kBTeff [eV] CSS 0970 0.4146 0.025 30000 2.9 SDSS J1028+0931 0.4146 0.018 12000 1.1 SDSS J1210+3347 0.4150 0.016 6000 0.52 3. Temperature normalized radius Fig. 2 shows the data in a mass-radius plot alongside the theoretical zero-temperature limit (1). Highlighted by color is a general trend of increasing radius with temperature. This trend is particularly striking upon considering similar masses. Table 1 lists three WDs of mass M   0.415M  clearly showing a pronounced correlation of apparent radius expanding by   50% with temperature increasing to   5 eV. In absolute terms, relative to the Fermi level of the electrons (Fig. 1), these temperatures are extremely modest leaving the star essentially unperturbed (Hamada and Salpeter, 1961). For the present sample of Parsons et al. (2017), this suggests, in- stead, a temperature sensitivity in the H atmosphere of WDs considered previously by modeled approaches in Parsons et al. (2017) more likely so than in the degenerate core. We return to this in  7.', chunk_index=13, num_tokens=288, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_7142814c-6ce6-4b8e-a0c9-400697bc0634', content='Here, we circumvent model assumptions by using generic and model-agnostic power-laws for an effective description by a temperature-normalized radius. In doing so, we derive a char- acteristic temperature scale characterizing temperature sensitiv- ity, blind to the underlying physical origin. To be specific, based on Fig. 1 and Table 1, we consider (cid:33) (cid:32) M M R = R0 f (T/T0) (6) with free parameters ( , T0). Here, f (T/T0) is dimensionless and R0 is a constant fixing the dimension of length. Starting point of our approach are effective mass-radius re- lationships of the form R   M  f (T/T0) for some power-law index   and temperature scale T0. Equivalently, this considers a correlation of mass to the scaled radius R  = R f (T/T0) (7) We apply the temperature scaled radius (7) to fit the data in the form R  = R0 (M/M )  . (8) For the sample of Parsons et al. (2017), we determine best-fit parameters ( , T0) to the mass-radius data of 26 WDs of type DA, all in eclipsing binaries.', chunk_index=14, num_tokens=269, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_c115da2f-0e75-4822-bd69-99d1045c5d7a', content='The function f (T/T0) in (6), to be discussed further below, will be a power-law comprising the free parameters ( , T0). In a fit to the mass-radius data, these parameters will be considered over a broad range of values 0 < T0 < 9 eV, 0 <   < 1. (9) The characteristic temperature scale T0 is limited to T0 < 9 eV. In this energy range, radii obtain accurately, while beyond, accuracy diminishes. We keep   < 1, reflecting the assumption that temperature has a secondary impact on the radius. Our best-fit is defined by   estimated using ODR (Orthogonal Dis- tance Regression) and, subsequently, the optimal value of   and T0 at the minimum STD and  2 of residuals. Following standard practice, our power-laws (6-8) are ana- lyzed by fits to linear trends in the log-log plane to log R  =   log M + C. In fits to data by (10),  2 optimizes both the index   in scaling by a power-law in mass and the constant C, while minimizing STD optimizes only  .', chunk_index=15, num_tokens=248, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_bf08779f-ca79-40e9-ac2f-62782c94cd6c', content='In the present analysis quantifying the goodness-of-fit according to residual scatter about a trend line (10), the level shift C along the ordinate - absorbing R0 in (6-8) - is safely ignored and it suffices to determine   by minimization of residuals. 4. Temperature-normalized mass-radius relations In the present model-agnostic approach, we explore fits to the data using three temperature-normalized power-laws. To this end, optimize by  2 and STD in our parameter estimation from fits to the sample of Parsons et al. (2017). The first power-law Relation-1 is I. f (x) = x , where x = T/T0. Here, T0 acts as a constant because of (9). For this reason,   is not affected by a choice of T0. We find =  0.954,   = 0.195  2 : STD :   =  0.951,   = 0.190 with the minimum  2 = 0.0183 and, respectively, with residual   = 0.03664. The second power-law Relation-2 is II. f (x) = (1 + x)  . In contrast to Relation-1, Relation-2 includes the zero temper- ature limit of the Chandrasekhar mass-radius relation (1).', chunk_index=16, num_tokens=282, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_cf0e5064-24c3-4b4f-a151-27dd223efc1f', content='Ac- cordingly, T0 is no longer ignorable and is determined in the optimization process in fitting (13) to the data. We find =  0.969,   = 0.389, T0 = 16226  2 : STD :   =  0.965,   = 0.356, T0 = 13896 with the minimum  2 = 0.0159 and, respectively, with residual   = 0.0344. The third power-law Relation-3 is III. f (x) = 1 + x , similar but not identical to Relation-2. Relation-3 also includes the Chandrasekhar limit of zero temperature and T0 is not ig- norable. We find =  0.971,   = 0.690, T0 = 66500,  2 : STD :   =  0.965,   = 0.647, T0 = 65914 (10) (11) (12) (13) (14) (15) (16) with the minimum  2 = 0.0161 and, respectively, a residual   = 0.0346. Fig. 3 summarizes our three results, each indicated by color: blue, green and red for Relation-1, Relation-2 and Relation- 3, respectively.', chunk_index=17, num_tokens=289, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_4c215949-f476-4634-9711-6f9c8162378d', content='The curve represents the optimal   with each T0 at the minimum STD ( 2). The junction point shows the common minimum STD ( 2). Both Relation-2 and Relation-3 leave rather similar residuals (in   and  2) for each of Relation- 2 and Relation-3. To rank these two relations, we proceed as follows. 5. Ranking relations by Monte Carlo Analysis In this section, Relation-2 and Relation-3 are ranked for sig- nificance by Monte Carlo (MC) analysis. MC analysis is a useful method for robust parameter estima- tion and ranking relations in the face of measurement uncertain- ties. Though ODR methods can be used also to estimate param- eters, the results do not necessarily agree, making it difficult to rank Relation-2 and Relation-3 for their relative significance. In this light, we pursue MC analysis by creating synthetic data by randomly selecting samples of varying radius, mass, and temperature within the measurement confidence intervals, Xnew = X + N(0,  ). Following this procedure, we estimate  ,  , and T0 without con- sidering errors in ODR. Results extended over 5M calculations are used to determine a ranking of Relation-2 and Relation-3 according to STD or  2) (Fig. 4) and infer a probability P of relative significance by counting the total number of times ei- ther one has preferred rank (Table 2).', chunk_index=18, num_tokens=298, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_ccb96689-63de-4291-a683-d34b42d85cbe', content='The MC simulation also provides accurate values in the mean of  ,  , and T0 over the total number of iterations. In our MC analysis, we consider realizations of arrays of 26   3 = 78 entries, comprising mass, radius and tempera- ture of the 26 WDs. As observed quantities, mass, radius and temperature data are independent. An accordingly fair (unbi- ased) draw of realizations extends over random draws from 98 confidence intervals, unconstrained and independently, blind to physical meaning and pre-conceived notions of correlations. In our analysis, the range of allowed values is densely covering by using a very large number of 5M iterations. Table 2 shows the output of our MC analysis. The results indicate that the WD radius is firstly determined by mass more so than temperature. Relation-2 and Relation-3 have similar   values, but   and T0 are notably distinct with otherwise similar residuals in  2 and STD (Figs. 2-5).Table 2 includes Relation-1, results for which are consistent with the above discussion. 6. Interpretation of Results Eclipsing binaries allow precise measurements of WD mass and radius (Bours et al., 2016; Parsons et al., 2017). How- ever, as in Fig. 2, the theoretical mass-radius relation (1) pro- vides a lower bound.', chunk_index=19, num_tokens=292, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_999e5b9f-4e17-428f-8bf6-0e4d12fce7b5', content='It is generally not the greatest lower bound by significant departures for hot, low-mass white dwarfs (17) Figure 3: Tracks of STD residuals (left panel) and  2 (right panel) for the Relation-1 (blue), Relation-2 (green) and Relation-3 (red) as a function of the normalized temperature power-law index  , following minimization over all T0 in (9). Note that Relation-1 has no explicit dependence on T0. Both cost functions produce very similar results for   and  . Table 2: Monte Carlo analysis on Relations 1-3 with ranking by probability P of having the lowest residual in  2 (left) or STD (right) for the same data (Fig. 4, synthesized over 5M representations).', chunk_index=20, num_tokens=165, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_1684c9d2-915d-4f52-ad49-dcecde488837', content='STD Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3 R0/R 7.6   10 3 6.2   10 3 5.8   10 3 8.4   10 4 6.2   10 3 5.8   10 3 0.951 0.966 0.967 0.951 0.965 0.966 0.194 0.409 0.690 0.193 0.375 0.685 T0 25000 19000 65000 const 15000 64000 1% 98.7% 1% 1% 98.6% 1% Figure 4: Probability of Relation-2 to have a smaller residual in  2 (continuous curve) and STD (dashed curve) than Relation-3 in a MC analysis extending over a large number of synthetic data sets. Both cost functions support the conclusion that Relation-2 is preferred over Relation-3 in providing a model- agnostic fit to the mass-radius data of WDs. (M/M    0.5). The gray region in Fig. 2 (Table 1) is il- lustrative, highlighting a pronounced trend in observed WD radius with temperatures at otherwise very similar masses in M/M  < 0.5.', chunk_index=21, num_tokens=298, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_da0ddc04-b62f-4e42-aa30-e2dab6250d6f', content='Evidently, this trend cannot be explained by the zero-temperature mass-radius relationship. Several theoretical calculations (de Carvalho et al., 2014; Boshkayev, 2018, 2019) have been advanced to explain this de- parture. While rotation and density affects the radius, the ra- dius of relatively dense WDs is not significantly influenced by temperature even though such is more so for low-density or ro- tating WDs compared to their high-density counter parts. Our a model-agnostic study of T0 identify a characteristic temper- ature in the expansion of the radius of the photosphere. It re- veals a consistent trend wherein WDs with higher temperatures exhibit relatively larger radius, clearly apparent in the overall trend (Benvenuto and Althaus, 1999; Panei et al., 2007; Par- sons et al., 2017; Joyce et al., 2018; Zenati et al., 2019; Romero et al., 2019). In a novel model-agnostic revisit of the mass-radius relation- ship, we quantify this temperature dependence by a temperature scale T0 = (1   2) eV in Relation-2. Relation-2 is found to be statistically more significant than Relation-3 based on our MC analysis ( 4).', chunk_index=22, num_tokens=278, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_4fa0745a-6c87-4e7f-bcd4-34c6c9ac5b47', content='Over 5M iterations, Fig.4 shows Relation-2 to have a lower  2 and lower   than Relation-3 at a probability of 98.7%, respectively, 98.6% . In the present approach, we circumvent potential systematic uncertainties otherwise present in model-dependent approaches (Parsons et al. 2017 and refer- ences therein). We summarize our approach in Fig. 5 (A-C). Panel (A) shows the discrepancy between the theoretical mass-radius re- lation at zero temperature (1) and the observed radius. Panel (B) shows a fit to the mass to the un-normalized radius, re- vealing scatter than clearly exceeds that of measurement uncer- tainty. Panel (C) shows the result of a linear relation between the temperature-normalized radius R  and (M/M ) . At rela- tively small residual scatter, this result identifies a temperature scale T0 and a radius primarily determined by mass. Accord- ing to Table 2 and in the notation of (1), we infer a mass-radius relation R = C (cid:32) M M (cid:33) 1/3 (18) Figure 5: Mass-radius plots of the data (black dots). Panel (A) highlights the deviation from the theoretical zero-temperature limit (1).', chunk_index=23, num_tokens=276, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_2f84c95c-631a-4198-9fc1-99fcd7b745fc', content='Panel (B) highlights the excess scatter in the data following a fit (red line) to the un-normalized radius R   M . Panel (C) shows a fit (green line) to our temperature-normalized radius R    M  in Relation-2 (13). A small residual scatter (cf. Fig. 3) evidences the effectiveness of our normalization in Relation-2, except for three outliers with relatively large observational uncertainties. All three panels A-C include the theoretical relation (blue dotted line). Bottom panels show the adjusted radius in our normalization produced by minimization of  2 (left) and STD (right). The adjustment by our temperature normalization in R  = R/ f (x), x = T/T0, is about 23% (  = 1.76) and 28% (  = 2.05) in Relation-2 and, respectively, Relation-3 in  2 optimization. The same is 24% (  = 1.82) and 29% (  = 2.07) in Relation-2, respectively, Relation-3 in STD optimization.', chunk_index=24, num_tokens=233, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_262eddcb-d740-4afc-91ae-1f23277328d6', content='This adjustment is most relevant at high temperatures. with the temperature-dependent expansion of an atmosphere in- cluded in C   C0 (cid:32) 1 + T T0 (cid:33)1/3 (cid:32) M M (cid:33) 2/3 according to Relation-2, parameterized by STD in the approxi- mations   =  0.965    1 and   = 0.375   1/3. (19) Our model-agnostic analysis hereby effectively reveals the presence of non-degenerate atmosphere, sufficiently massive and opaque, to account for the observed temperature sensitivity in Fig. 2 parameterized by above-mentioned T0. Indeed, further confirmation can be found in consistency of the present Parsons et al. (2017) data with the detailed model for H atmospheres of B edard et al. (2020). 7. Conclusions and Outlook We summarize the apparent mass-radius relation (18) with temperature dependent coefficient (19) due to this atmosphere by including Relation-2 as a factor modifying C0 in (19). A principal outcome of our model-independent study is a temperature scale T0 = 1   2 eV in temperature sensitivity of the photospheric radius of the WDs of type DA in Parsons et al. (2017), shown in Fig. 5 and summarized in (18-19).', chunk_index=25, num_tokens=289, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_a1e2f270-9178-4cd6-8429-8540c477c5fa', content='T0 derives from 1.6 eV and 1.3 eV according to  2 and, re- spectively, STD in fits of Relation-2 to the data. T0 appears to be particularly relevant to low mass M/M    0.5 in the present sample. While the Parsons et al. (2017) sample of WDs covers a size- able range in masses with distinct temperature sensitivity be- low M/M    0.5 and above M/M    0.5, they nevertheless are of relatively high mass-density      0 (Fig. 6). Cru- cially, densities for the entire at hand are above the threshold  0 = 105 g cm 3 above which the mass-radius relation of the degenerate core is expected to be insensitive to temperature (Boshkayev, 2018). In this sense, the present sample is rela- tively homogeneous and does not test temperature sensitivity of the core beyond what is expected from (1). For the above-mentioned low-mass WDs, a non-degenerate atmosphere effectively insulates a core at super-Eddington tem- peratures (4) from the observed sub-Eddington temperatures T by virtue of an atmosphere that is sufficiently massive and opaque.', chunk_index=26, num_tokens=264, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_017ca6e2-d508-42f0-bf32-5108bb7eedef', content='In the following scaling, we shall ignore a potential role of a corona due to magnetic fields (Aznar Cuadrado et al., 2004; Jordan et al., 2007; Ferrario et al., 2020). Following  2, the Eddington temperature introduces a scale height hE by virtue of thermal kinetic energy of the ions, distinct from escape by radiation pressure. For a hydrogen atmosphere, Figure 6: (Top panel.) For the WD sample of Parsons et al. (2017), shown are the expected relative expansion of a non-degenerate atmosphere, sufficiently massive and opaque, surrounding a degenerate core based on super-Eddington temperature Tc inferred from the Koester (1976) correlation to the observed temperature T (Fig. 1). The Parsons et al. (2017) sample satisfies TEdd     TcT , reconciling Tc   TEdd with T   TEdd, leaving T on the order of a few eV. (Lower panel.) For the entire Parsons et al. (2017) sample, the mass- densities of the core according to the theoretical zero-temperature relationship (1) are above the threshold 105 g cm 3 of Boshkayev (2018), where temperature sensitivity is negligible. we have hE kBTEdd mpg = 13 km R2 0.02 M 1 0.5   0.3%RWD.', chunk_index=27, num_tokens=299, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_d572fe01-9673-4530-ad93-fb0e415c9d4f', content='By high thermal conductivity, the core is believed to be at an essentially uniform temperature. As a result, the atmosphere assumes a mean temperature  TEdd with      . By (20), this implies a height H =   hE   10% RWD. This expected height (Fig. 6) introduces a radial expansion qualitatively consistent with the data (Fig. 5). In this light, the relatively modest characteristic temperature T0 = 1   2 eV in (18-19) can be identified with the core temper- ature higher by a factor of Tc/T   (TEdd/T )2 = O based on Figs. 1-2 and  2. (cid:16) 103(cid:17) The heat transport from core to surface in such atmospheres gives rise to a complex mass-radius relationship, here described by (18-19). This appears to be particular relevant to low-mass WDs, essentially below the mean of the WD mass distribution. Above, the relatively high-mass WDs appear to follow the the- oretical mass-radius relation (1), evidencing the absence of an atmosphere and/or a relatively low temperature core. The origin of this discrepancy appears to be beyond the present considera- tions, that might involve a distinct composition and associated formation history.', chunk_index=28, num_tokens=274, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_58f686f6-b104-48c9-a312-af6d076ea5ba', content='Derived for WDs of type DA, our results may serve as a reference for similar model-independent analysis of WDs of different type. (20) (21) Acknowledgements The authors thank the anonymous reviewer for constructive comments which greatly contributed to clarity of presentation, and thank M.A. Abchouyeh for stimulating discussions. This work is supported, in part, by NRF No. RS-2024-00334550. References Aznar Cuadrado, R., Jordan, S., Napiwotzki, R., Schmid, H.M., Solanki, S.K., Mathys, G., 2004. Discovery of kilogauss magnetic fields in three DA white dwarfs. A&A 423, 1081 1094. doi:10.1051/0004-6361:20040355, arXiv:astro-ph/0405308. Baiko, D.A., Yakovlev, D.G., 2019. Quantum ion thermodynamics in liquid interiors of white dwarfs. MNRAS 490, 5839 5847. doi:10.1093/mnras/ stz3029, arXiv:1910.06771. B edard, A., 2024. The spectral evolution of white dwarfs: where do doi:10.1007/s10509-024-04307-5, we stand?', chunk_index=29, num_tokens=293, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_c3da4773-3a38-49fb-8ec5-cb13d1ef4dce', content='arXiv:2405.01268. Ap&SS 369, 43. B edard, A., Bergeron, P., Brassard, P., Fontaine, G., 2020. On the Spec- tral Evolution of Hot White Dwarf Stars. I. A Detailed Model Atmo- sphere Analysis of Hot White Dwarfs from SDSS DR12. ApJ 901, 93. doi:10.3847/1538-4357/abafbe, arXiv:2008.07469. Benvenuto, O.G., Althaus, L.G., 1999. Grids of white dwarf evolutionary mod- els with masses from M=0.1 to 1.2 m solar. MNRAS 303, 30 38. doi:10. 1046/j.1365-8711.1999.02215.x, arXiv:astro-ph/9811414. Boshkayev, K., 2018. Equilibrium Configurations of Rotating White Dwarfs at Finite Temperatures. Astronomy Reports 62, 847 852. doi:10.1134/ S106377291812017X, arXiv:1807.00332. Boshkayev, K., 2019.', chunk_index=30, num_tokens=276, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_8c549666-06a4-4191-a161-d3a3912ba01f', content='Static and rotating white dwarfs at finite temper- arXiv e-prints , arXiv:1909.10899doi:10.48550/arXiv.1909. atures. 10899, arXiv:1909.10899. Boshkayev, K., O., L., M., M., H., Q., 2021. Static and rotating white dwarfs at finite temperatures. IJMPh , 61doi:10.26577/ijmph.2021.v12.i2.07, arXiv:1909.10899. Boshkayev, K., Rueda, J.A., Ruffini, R., Siutsou, I., 2015. General Rela- tivistic and Newtonian White Dwarfs, in: Thirteenth Marcel Grossmann Meeting: On Recent Developments in Theoretical and Experimental Gen- eral Relativity, Astrophysics and Relativistic Field Theories, pp. 2468 2474. doi:10.1142/9789814623995$_-$0472, arXiv:1503.04171. Boshkayev, K.A., Rueda, J.A., Zhami, B.A., Kalymova, Z.A., Balgymbekov, G.S., 2016. Equilibrium structure of white dwarfs at finite temperatures, in:', chunk_index=31, num_tokens=299, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_008f8ccb-bd33-485a-8d84-00799cb2444c', content='International Journal of Modern Physics Conference Series, p. 1660129. doi:10.1142/S2010194516601290, arXiv:1510.02024. Bours, M.C.P., Marsh, T.R., Parsons, S.G., Dhillon, V.S., Ashley, R.P., Bento, J.P., Breedt, E., Butterley, T., Caceres, C., Chote, P., Copperwheat, C.M., Hardy, L.K., Hermes, J.J., Irawati, P., Kerry, P., Kilkenny, D., Littlefair, S.P., McAllister, M.J., Rattanasoon, S., Sahman, D.I., Vu ckovi c, M., Wilson, R.W., 2016. Long-term eclipse timing of white dwarf binaries: an observa- tional hint of a magnetic mechanism at work. MNRAS 460, 3873 3887. doi:10.1093/mnras/stw1203, arXiv:1606.00780. de Carvalho, S.M., Rotondo, M., Rueda, J.A., Ruffini, R., 2014. Relativistic feynman-metropolis-teller treatment at finite temperatures. Phys. Rev. C 89, 015801.', chunk_index=32, num_tokens=288, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_67fcb637-fd9f-428e-bb9b-14d3ec585594', content='URL: https://link.aps.org/doi/10.1103/PhysRevC.89. 015801, doi:10.1103/PhysRevC.89.015801. de Carvalho, S.M., Rotondo, M., Rueda, J.A., Ruffini, R., 2014. Relativistic Feynman-Metropolis-Teller treatment at finite temperatures. Phys. Rev. C 89, 015801. doi:10.1103/PhysRevC.89.015801. Dhillon, V.S., Marsh, T.R., Atkinson, D.C., Bezawada, N., Bours, M.C.P., Copperwheat, C.M., Gamble, T., Hardy, L.K., Hickman, R.D.H., Irawati, P., Ives, D.J., Kerry, P., Leckngam, A., Littlefair, S.P., McLay, S.A., O Brien, K., Peacocke, P.T., Poshyachinda, S., Richichi, A., Soonthorn- thum, B., Vick, A., 2014. ULTRASPEC: a high-speed imaging photome- ter on the 2.4-m Thai National Telescope.', chunk_index=33, num_tokens=272, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_1c2df2b2-ca74-4f2d-9b30-c4f6df7278d1', content='MNRAS 444, 4009 4021. doi:10.1093/mnras/stu1660, arXiv:1408.2733. Dhillon, V.S., Marsh, T.R., Stevenson, M.J., Atkinson, D.C., Kerry, P., Pea- cocke, P.T., Vick, A.J.A., Beard, S.M., Ives, D.J., Lunney, D.W., McLay, S.A., Tierney, C.J., Kelly, J., Littlefair, S.P., Nicholson, R., Pashley, R., Harlaftis, E.T., O Brien, K., 2007. ULTRACAM: an ultrafast, triple- beam CCD camera for high-speed astrophysics. MNRAS 378, 825 840. doi:10.1111/j.1365-2966.2007.11881.x, arXiv:0704.2557. Ferrario, L., Wickramasinghe, D., Kawka, A., 2020. Magnetic fields in isolated and interacting white dwarfs. Advances in Space Research 66, 1025 1056. doi:10.1016/j.asr.2019.11.012, arXiv:2001.10147.', chunk_index=34, num_tokens=281, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_eb8b4c2a-fcd7-4ea5-b721-5dd7afd6e854', content='Fontaine, G., Brassard, P., Bergeron, P., 2001. The Potential of White Dwarf Cosmochronology. PASP 113, 409 435. doi:10.1086/319535. Gentile Fusillo, N.P., Tremblay, P.E., Cukanovaite, E., Vorontseva, A., Lalle- ment, R., Hollands, M., G ansicke, B.T., Burdge, K.B., McCleery, J., Jordan, S., 2021. A catalogue of white dwarfs in Gaia EDR3. MNRAS 508, 3877  3896. doi:10.1093/mnras/stab2672, arXiv:2106.07669. Hamada, T., Salpeter, E.E., 1961. Models for Zero-Temperature Stars. ApJ 134, 683. doi:10.1086/147195. Han, Z., Podsiadlowski, P., Maxted, P.F.L., Marsh, T.R., Ivanova, N., The origin of subdwarf B stars - I. The formation channels. doi:10.1046/j.1365-8711.2002.05752.x, 2002.', chunk_index=35, num_tokens=280, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_fca88c2c-af4b-4bab-8516-3b5aa137bc57', content='MNRAS 336, 449 466. arXiv:astro-ph/0206130. Hearn, A.G., Mewe, R., 1976. The corona around the white dwarf Sirius B determined from X-ray measurements. A&A 50, 319 321. Iben, I., J., Tutukov, A.V., 1985. On the evolution of close binaries with components of initial mass between 3 M and 12 M. ApJS 58, 661 710. doi:10.1086/191054. Istrate, A.G., Marchant, P., Tauris, T.M., Langer, N., Stancliffe, R.J., Grassitelli, L., 2016. Models of low-mass helium white dwarfs including gravitational settling, thermal and chemical diffusion, and rotational mixing. A&A 595, A35. doi:10.1051/0004-6361/201628874, arXiv:1606.04947. Jordan, S., Aznar Cuadrado, R., Napiwotzki, R., Schmid, H.M., Solanki, S.K., 2007. The fraction of DA white dwarfs with kilo-Gauss magnetic fields.', chunk_index=36, num_tokens=269, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_092cb723-4385-4068-a909-3544b2800fec', content='A&A 462, 1097 1101. doi:10.1051/0004-6361:20066163, arXiv:astro-ph/0610875. Joyce, S.R.G., Barstow, M.A., Casewell, S.L., Burleigh, M.R., Holberg, J.B., Bond, H.E., 2018. Testing the white dwarf mass-radius relation and com- paring optical and far-UV spectroscopic results with Gaia DR2, HST, and FUSE. MNRAS 479, 1612 1626. doi:10.1093/mnras/sty1425, arXiv:1806.00061. Kepler, S.O., Pelisoli, I., Koester, D., Reindl, N., Geier, S., Romero, A.D., Ourique, G., Oliveira, C.d.P., Amaral, L.A., 2019. White dwarf and subd- warf stars in the Sloan Digital Sky Survey Data Release 14. MNRAS 486, 2169 2183. doi:10.1093/mnras/stz960, arXiv:1904.01626. Koester, D., 1976. Convective Mixing and Accretion in White Dwarfs. A&A 52, 415.', chunk_index=37, num_tokens=290, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_f773288c-9fe0-45bd-8938-82a7c93d0b40', content='Koester, D., 2002. White dwarfs: Recent developments. A&A Rev. 11, 33 66. doi:10.1007/s001590100015. Koester, D., Chanmugam, G., 1990. Physics of white dwarf stars. Reports on Progress in Physics 53, 837. Koester, D., Chanmugam, G., 1990. REVIEW: Physics of white dwarf stars. Reports on Progress in Physics 53, 837 915. doi:10.1088/0034-4885/ 53/7/001. Koester, D., Schulz, H., Weidemann, V., 1979. Atmospheric parameters and mass distribution of DA white dwarfs. A&A 76, 262 275. Marsh, T.R., Dhillon, V.S., Duck, S.R., 1995. Low-Mass White Dwarfs Need Friends - Five New Double-Degenerate Close Binary Stars. MNRAS 275, 828. doi:10.1093/mnras/275.3.828. Nelemans, G., Zwart, S., Verbunt, F., Yungelson, L., 2001. Population synthesis for double white dwarfs. ii. semi-detached systems:', chunk_index=38, num_tokens=279, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_6f40a653-69ff-467b-84fa-3a7f01947a75', content='Am cvn stars. arXiv preprint astro-ph/0101123 . Panei, J.A., Althaus, L.G., Chen, X., Han, Z., 2007. Full evolution of low- mass white dwarfs with helium and oxygen cores. MNRAS 382, 779 792. doi:10.1111/j.1365-2966.2007.12400.x. Parsons, S.G., G ansicke, B.T., Marsh, T.R., Ashley, R.P., Bours, M.C.P., Breedt, E., Burleigh, M.R., Copperwheat, C.M., Dhillon, V.S., Green, M., Hardy, L.K., Hermes, J.J., Irawati, P., Kerry, P., Littlefair, S.P., McAllis- ter, M.J., Rattanasoon, S., Rebassa-Mansergas, A., Sahman, D.I., Schreiber, M.R., 2017. Testing the white dwarf mass-radius relationship with eclips- ing binaries. MNRAS 470, 4473 4492. doi:10.1093/mnras/stx1522, arXiv:1706.05016. Pei, T.H., 2022. The Highly Accurate Relation between the Radius and Mass of the White Dwarf Star from Zero to Finite Temperature.', chunk_index=39, num_tokens=297, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_dc0765c0-e939-4621-b903-d918173bd605', content='Frontiers in As- tronomy and Space Sciences 8, 243. doi:10.3389/fspas.2021.799210. Ren, J.J., Rebassa-Mansergas, A., Parsons, S.G., Liu, X.W., Luo, A.L., Kong, X., Zhang, H.T., 2018. White dwarf-main sequence binaries from LAM- OST: the DR5 catalogue. MNRAS 477, 4641 4654. doi:10.1093/mnras/ sty805, arXiv:1803.09523. Romero, A.D., Kepler, S.O., Joyce, S.R.G., Lauffer, G.R., C orsico, A.H., 2019. The white dwarf mass-radius relation and its dependence on the hydro- gen envelope. MNRAS 484, 2711 2724. doi:10.1093/mnras/stz160, arXiv:1901.04644. Rotondo, M., Rueda, J.A., Ruffini, R., Xue, S.S., 2011. Relativistic Feynman- Metropolis-Teller theory for white dwarfs in general relativity. Phys. Rev. D 84, 084007. doi:10.1103/PhysRevD.84.084007, arXiv:1012.0154.', chunk_index=40, num_tokens=299, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_53751cd2-aeff-47fd-8f18-b0d2bebe10e8', content='Str omgren, B., 1939. Book Review: An Introduction to the Study of Stellar Structure, by S. Chandrasekhar. Popular Astronomy 47, 287. Tremblay, P.E., Gentile-Fusillo, N., Raddi, R., Jordan, S., Besson, C., G ansicke, B.T., Parsons, S.G., Koester, D., Marsh, T., Bohlin, R., Kali- rai, J., Deustua, S., 2017. The Gaia DR1 mass-radius relation for white dwarfs. MNRAS 465, 2849 2861. doi:10.1093/mnras/stw2854, arXiv:1611.00629. Verbunt, F., Rappaport, S., 1988. Mass Transfer Instabilities Due to Angular Momentum Flows in Close Binaries. ApJ 332, 193. doi:10.1086/166645.', chunk_index=41, num_tokens=212, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_0ccaab5a-60c5-4821-8d22-36c054d2251b', content='Vernet, J., Dekker, H., D Odorico, S., Kaper, L., Kjaergaard, P., Hammer, F., Randich, S., Zerbi, F., Groot, P.J., Hjorth, J., Guinouard, I., Navarro, R., Adolfse, T., Albers, P.W., Amans, J.P., Andersen, J.J., Andersen, M.I., Bi- netruy, P., Bristow, P., Castillo, R., Chemla, F., Christensen, L., Conconi, P., Conzelmann, R., Dam, J., de Caprio, V., de Ugarte Postigo, A., Delabre, B., di Marcantonio, P., Downing, M., Elswijk, E., Finger, G., Fischer, G., Flores, H., Franc ois, P., Goldoni, P., Guglielmi, L., Haigron, R., Hanen- burg, H., Hendriks, I., Horrobin, M., Horville, D., Jessen, N.C., Kerber, F., Kern, L., Kiekebusch, M., Kleszcz, P., Klougart, J., Kragt, J., Larsen, H.H., Lizon, J.L., Lucuix, C., Mainieri, V., Manuputy, R., Martayan, C., Mason, E., Mazzoleni, R., Michaelsen, N., Modigliani, A., Moehler, S., M ller, P., Norup S rensen, A., N rregaard, P., P eroux, C., Patat, F., Pena, E., Pragt, J., Reinero, C., Rigal, F., Riva, M., Roelfsema, R., Royer, F., Sacco, G., Santin, P., Schoenmaker, T., Spano, P., Sweers, E., Ter Horst, R., Tintori, M., Tromp, N., van Dael, P., van der Vliet, H., Venema, L., Vidali, M., Vinther, J., Vola, P., Winters, R., Wistisen, D., Wulterkens, G., Zacchei, A., 2011.', chunk_index=42, num_tokens=516, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]}), ResponseChunk(id='chunk_e2070c9b-cbf7-4e5d-a080-3e69b46e00f6', content='X-shooter, the new wide band intermediate reso- lution spectrograph at the ESO Very Large Telescope. A&A 536, A105. doi:10.1051/0004-6361/201117752, arXiv:1110.1944. Zenati, Y., Toonen, S., Perets, H.B., 2019. Formation and evolution of hy- brid He-CO white dwarfs and their properties. MNRAS 482, 1135 1142. doi:10.1093/mnras/sty2723, arXiv:1803.04444.', chunk_index=43, num_tokens=133, metadata={'section_titles': ['Boshkayev, K., 2019.', 'Tc    TEdd', 'aPhysics and Astronomy, Sejong University, 209 Neungdong-ro, 05006, Seoul, South Korea', 'nificance by Monte Carlo (MC) analysis.', 'R = C', 'Preprint submitted to New Astronomy', 'Progress in Physics 53, 837.', 'where', 'Abstract', 'References', 'R = R0', 'STD', 'Relation-1 Relation-2 Relation-3 Relation-1 Relation-2 Relation-3', 'log R  =   log M + C.', 'C   C0', 'Object', 'Acknowledgements', 'R f (T/T0)', 'with', 'Xnew = X + N(0,  ).', 'Keywords: white dwarfs, mass-radius relation, Chandrasekhar', 'Teff [K]', '40 T TEdd', 'const', 'kBTEdd mpg', 'R = C0'], 'pages': [1]})]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aurelio_sdk import ExtractResponse\n",
    "\n",
    "# From URL\n",
    "url = \"https://arxiv.org/pdf/2408.15291\"\n",
    "response_pdf_url: ExtractResponse = await client.extract_url(\n",
    "    url=url, quality=\"low\", chunk=True, timeout=-1\n",
    ")\n",
    "\n",
    "response_pdf_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtractResponse(status=<TaskStatus.completed: 'completed'>, usage=Usage(tokens=10, pages=None, seconds=15), message=None, processing_options=ExtractProcessingOptions(chunk=True, quality=<ProcessingQuality.low: 'low'>), document=ResponseDocument(id='doc_f3fdfb3e-6e74-425f-a28c-3f7b186b6d4b', content=\" I'm a monster! I'm a monster!\", source='https://storage.googleapis.com/gtv-videos-bucket/sample/ForBiggerMeltdowns.mp4', source_type=<SourceType.video_mp4: 'video/mp4'>, num_chunks=1, metadata={}, chunks=[ResponseChunk(id='chunk_aedc3fd7-0d28-4485-8889-4fb5852de299', content=\"I'm a monster! I'm a monster!\", chunk_index=1, num_tokens=10, metadata={'start_time': 0, 'end_time': 7})]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aurelio_sdk import ExtractResponse\n",
    "\n",
    "# From URL\n",
    "url = \"https://storage.googleapis.com/gtv-videos-bucket/sample/ForBiggerMeltdowns.mp4\"\n",
    "response_video_url: ExtractResponse = await client.extract_url(\n",
    "    url=url, quality=\"low\", chunk=True, timeout=-1\n",
    ")\n",
    "\n",
    "response_video_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get document status and handle timeouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: TaskStatus.pending\n",
      "Message: Processing is taking longer than the timeout of 10s. Check the status: GET /extract/document/doc_4842138e-4018-4ffa-b83d-3a317f5838a8\n",
      "Document ID: doc_4842138e-4018-4ffa-b83d-3a317f5838a8\n"
     ]
    }
   ],
   "source": [
    "from aurelio_sdk import ExtractResponse\n",
    "\n",
    "# From a local file\n",
    "file_path = \"data/pdf/adaptive_semantic_search.pdf\"\n",
    "\n",
    "with open(file_path, \"rb\") as file:\n",
    "    # Load file high quality and set timeout 10 seconds\n",
    "    response_pdf_file: ExtractResponse = await client.extract_file(\n",
    "        file=file, quality=\"high\", chunk=True, timeout=10\n",
    "    )\n",
    "\n",
    "# Get document status, message and document id\n",
    "print(\"Status:\", response_pdf_file.status)\n",
    "print(\"Message:\", response_pdf_file.message)\n",
    "print(\"Document ID:\", response_pdf_file.document.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: TaskStatus.pending\n"
     ]
    }
   ],
   "source": [
    "# Get document status and response\n",
    "document_response: ExtractResponse = await client.get_document(\n",
    "    document_id=response_pdf_file.document.id\n",
    ")\n",
    "\n",
    "print(\"Status:\", document_response.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or use a pre-built function, which helps to avoid long hanging requests (Recommended)\n",
    "document_response = await client.wait_for_document_completion(\n",
    "    document_id=response_pdf_file.document.id, timeout=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtractResponse(status=<TaskStatus.completed: 'completed'>, usage=Usage(tokens=23737, pages=25, seconds=None), message=None, processing_options=ExtractProcessingOptions(chunk=True, quality=<ProcessingQuality.high: 'high'>), document=ResponseDocument(id='doc_4842138e-4018-4ffa-b83d-3a317f5838a8', content='3 2 0 2\\nt c O 8 1 ] G L . s c [ 2 v 5 3 4 9 1 . 5 0 3 2 : v i X r a\\nAdANNS: A Framework for Adaptive Semantic Search\\nAniket Rege    Aditya Kusupati    Sharan Ranjit S  Alan Fan  Qingqing Cao , Sham Kakade  Prateek Jain  Ali Farhadi   University of Washington,  Google Research,  Harvard University {kusupati,ali}@cs.washington.edu, prajain@google.com\\nAbstract\\nWeb-scale search systems learn an encoder to embed a given query which is then hooked into an approximate nearest neighbor search (ANNS) pipeline to retrieve similar data points. To accurately capture tail queries and data points, learned representations typically are rigid, high-dimensional vectors that are generally used as-is in the entire ANNS pipeline and can lead to computationally expensive retrieval. In this paper, we argue that instead of rigid representations, different stages of ANNS can leverage adaptive representations of varying capacities to achieve significantly better accuracy-compute trade-offs, i.e., stages of ANNS that can get away with more approximate computation should use a lower-capacity representation of the same data point. To this end, we introduce AdANNS , a novel ANNS design framework that explicitly leverages the flexibility of Ma- tryoshka Representations [31]. We demonstrate state-of-the-art accuracy-compute trade-offs using novel AdANNS-based key ANNS building blocks like search data structures (AdANNS-IVF) and quantization (AdANNS-OPQ). For exam- ple on ImageNet retrieval, AdANNS-IVF is up to 1.5% more accurate than the rigid representations-based IVF [48] at the same compute budget; and matches accuracy while being up to 90  faster in wall-clock time. For Natural Questions, 32-byte AdANNS-OPQ matches the accuracy of the 64-byte OPQ baseline [13] constructed using rigid representations   same accuracy at half the cost! We further show that the gains from AdANNS translate to modern-day composite ANNS indices that combine search structures and quantization. Finally, we demonstrate that AdANNS can enable inference-time adaptivity for compute-aware search on ANNS indices built non-adaptively on matryoshka representations. Code is open-sourced at https://github.com/RAIVNLab/AdANNS.\\nIntroduction\\nSemantic search [24] on learned representations [40, 41, 50] is a major component in retrieval pipelines [4, 9]. In its simplest form, semantic search methods learn a neural network to embed queries as well as a large number (N ) of data points in a d-dimensional vector space. For a given query, the nearest (in embedding space) point is retrieved using either an exact search or using approximate nearest neighbor search (ANNS) [21] which is now indispensable for real-time large-scale retrieval.\\nExisting semantic search methods learn fixed or rigid representations (RRs) which are used as is in all the stages of ANNS (data structures for data pruning and quantization for cheaper distance computation; see Section 2). That is, while ANNS indices allow a variety of parameters for searching the design space to optimize the accuracy-compute trade-off, the provided data dimensionality is typically assumed to be an immutable parameter. To make it concrete, let us consider inverted file index (IVF) [48], a popular web-scale ANNS technique [16]. IVF has two stages (Section 3) during inference: (a) cluster mapping: mapping the query to a cluster of data points [36], and (b) linear\\n Equal contribution.\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\\n(a) Image retrieval on ImageNet-1K.  (b) Passage retrieval on Natural Questions. \\nFigure 5: The schematic of inverted file index (IVF) outlaying the construction and inference phases. Adaptive representations can be utilized effectively in the decoupled components of clustering and searching for a better accuracy-compute trade-off (AdANNS-IVF).\\n\\nscan: distance computation w.r.t all points in the retrieved cluster to find the nearest neighbor (NN). Standard IVF utilizes the same high-dimensional RR for both phases, which can be sub-optimal.\\nWhy the sub-optimality? Imagine one needs to partition a dataset into k clusters for IVF and the dimensionality of the data is d   IVF uses full d representation to partition into k clusters. However, suppose we have an alternate approach that somehow projects the data in d/2 dimensions and learns 2k clusters. Note that the storage and computation to find the nearest cluster remains the same in both cases, i.e., when we have k clusters of d dimensions or 2k clusters of d/2 dimensions. 2k clusters can provide significantly more refined partitioning, but the distances computed between queries and clusters could be significantly more inaccurate after projection to d/2 dimensions.\\nSo, if we can find a mechanism to obtain a d/2-dimensional representation of points that can accurately approximate the topology/distances of d-dimensional representation, then we can potentially build significantly better ANNS structure that utilizes different capacity representations for the cluster mapping and linear scan phases of IVF. But how do we find such adaptive representations? These desired adaptive representations should be cheap to obtain and still ensure distance preservation across dimensionality. Post-hoc dimensionality reduction techniques like SVD [14] and random projections [25] on high-dimensional RRs are potential candidates, but our experiments indicate that in practice they are highly inaccurate and do not preserve distances well enough (Figure 2).\\nInstead, we identify that the recently proposed Matryoshka Representations (MRs) [31] satisfy the specifications for adaptive representations. Matryoshka representations pack information in a hierarchical nested manner, i.e., the first m-dimensions of the d-dimensional MR form an accurate low-dimensional representation while being aware of the information in the higher dimensions. This allows us to deploy MRs in two major and novel ways as part of ANNS: (a) low-dimensional representations for accuracy-compute optimal clustering and quantization, and (b) high-dimensional representations for precise re-ranking when feasible.\\nTo this effort, we introduce AdANNS , a novel design framework for semantic search that uses matryoshka representation-based adaptive representations across different stages of ANNS to ensure significantly better accuracy-compute trade-off than the state-of-the-art baselines.\\nTypical ANNS systems have two key components: (a) search data structure to store datapoints, (b) distance computation to map a given query to points in the data structure. Through AdANNS, we address both these components and significantly improve their performance. In particular, we first propose AdANNS-IVF (Section 4.1) which tackles the first component of ANNS systems. AdANNS- IVF uses standard full-precision computations but uses adaptive representations for different IVF stages. On ImageNet 1-NN image retrieval (Figure 1a), AdANNS-IVF is up to 1.5% more accurate for the compute budget and 90  cheaper in deployment for the same accuracy as IVF.\\nWe then propose AdANNS-OPQ (Section 4.2) which addresses the second component by using AdANNS-based quantization (OPQ [13])   here we use exhaustive search overall points. AdANNS- OPQ is as accurate as the baseline OPQ on RRs while being at least 2  faster on Natural Ques- tions [32] 1-NN passage retrieval (Figure 1b). Finally, we combine the two techniques to obtain AdANNS-IVFOPQ (Section 4.3) which is more accurate while being much cheaper   up to 8    than the traditional IVFOPQ [24] index. To demonstrate generality of our technique, we adapt AdANNS to DiskANN [22] which provides interesting accuracy-compute tradeoff; see Table 1.\\nWhile MR already has multi-granular representations, careful integration with ANNS building blocks is critical to obtain a practical method and is our main contribution. In fact, Kusupati et al. [31] proposed a simple adaptive retrieval setup that uses smaller-dimensional MR for shortlisting in re- trieval followed by precise re-ranking with a higher-dimensional MR. Such techniques, unfortunately, cannot be scaled to industrial systems as they require forming a new index for every shortlisting provided by low-dimensional MR. Ensuring that the method aligns well with the modern-day ANNS pipelines is important as they already have mechanisms to handle real-world constraints like load-balancing [16] and random access from disk [22]. So, AdANNS is a step towards making the abstraction of adaptive search and retrieval feasible at the web-scale.\\nThrough extensive experimentation, we also show that AdANNS generalizes across search data structures, distance approximations, modalities (text & image), and encoders (CNNs & Transformers) while still translating the theoretical gains to latency reductions in deployment. While we have mainly focused on IVF and OPQ-based ANNS in this work, AdANNS also blends well with other ANNS pipelines. We also show that AdANNS can enable compute-aware elastic search on prebuilt indices without making any modifications (Section 5.1); note that this is in contrast to AdANNS-IVF that builds the index explicitly utilizing  adaptivity  in representations. Finally, we provide an extensive analysis on the alignment of matryoshka representation for better semantic search (Section 5.2).\\nWe make the following key contributions:\\n  We introduce AdANNS , a novel framework for semantic search that leverages matryoshka representations for designing ANNS systems with better accuracy-compute trade-offs.\\n  AdANNS powered search data structure (AdANNS-IVF) and quantization (AdANNS-OPQ) show a significant improvement in accuracy-compute tradeoff compared to existing solutions.\\n  AdANNS generalizes to modern-day composite ANNS indices and can also enable compute-aware elastic search during inference with no modifications.\\n2 Related Work\\nApproximate nearest neighbour search (ANNS) is a paradigm to come as close as possible [7] to retrieving the  true  nearest neighbor (NN) without the exorbitant search costs associated with exhaustive search [21, 52]. The  approximate  nature comes from data pruning as well as the cheaper distance computation that enable real-time web-scale search. In its naive form, NN-search has a complexity of O(dN ); d is the data dimensionality used for distance computation and N is the size of the database. ANNS employs each of these approximations to reduce the linear dependence on the dimensionality (cheaper distance computation) and data points visited during search (data pruning).\\nCheaper distance computation. From a bird s eye view, cheaper distance computation is always obtained through dimensionality reduction (quantization included). PCA and SVD [14, 26] can reduce dimensionality and preserve distances only to a limited extent without sacrificing accuracy. On the other hand, quantization-based techniques [6, 15] like (optimized) product quantization ((O)PQ) [13, 23] have proved extremely crucial for relatively accurate yet cheap distance computation and simultaneously reduce the memory overhead significantly. Another naive solution is to indepen- dently train the representation function with varying low-dimensional information bottlenecks [31] which is rarely used due to the costs of maintaining multiple models and databases.\\nData pruning. Enabled by various data structures, data pruning reduces the number of data points visited as part of the search. This is often achieved through hashing [8, 46], trees [3, 12, 16, 48] and graphs [22, 38]. More recently there have been efforts towards end-to-end learning of the search data structures [17, 29, 30]. However, web-scale ANNS indices are often constructed on rigid d-dimensional real vectors using the aforementioned data structures that assist with the real-time search. For a more comprehensive review of ANNS structures please refer to [5, 34, 51].\\nComposite indices. ANNS pipelines often benefit from the complementary nature of various building blocks [24, 42]. In practice, often the data structures (coarse-quantizer) like IVF [48] and HNSW [37] are combined with cheaper distance alternatives like PQ [23] (fine-quantizer) for massive speed-ups in web-scale search. While the data structures are built on d-dimensional real vectors, past works consistently show that PQ can be safely used for distance computation during search time. As evident in modern web-scale ANNS systems like DiskANN [22], the data structures are built on d-dimensional real vectors but work with PQ vectors (32   64-byte) for fast distance computations.\\nANNS benchmark datasets. Despite the Herculean advances in representation learning [19, 42], ANNS progress is often only benchmarked on fixed representation vectors provided for about a dozen million to billion scale datasets [1, 47] with limited access to the raw data. This resulted in the improvement of algorithmic design for rigid representations (RRs) that are often not specifically designed for search. All the existing ANNS methods work with the assumption of using the provided d-dimensional representation which might not be Pareto-optimal for the accuracy-compute trade- off in the first place. Note that the lack of raw-image and text-based benchmarks led us to using ImageNet-1K [45] (1.3M images, 50K queries) and Natural Questions [32] (21M passages, 3.6K queries) for experimentation. While not billion-scale, the results observed on ImageNet often translate to real-world progress [28], and Natural Questions is one of the largest question answering datasets benchmarked for dense passage retrieval [27], making our results generalizable and widely applicable.\\nIn this paper, we investigate the utility of adaptive representations   embeddings of different dimen- sionalities having similar semantic information   in improving the design of ANNS algorithms. This helps in transitioning out of restricted construction and inference on rigid representations for ANNS. To this end, we extensively use Matryoshka Representations (MRs) [31] which have desired adaptive properties in-built. To the best of our knowledge, this is the first work that improves accuracy-compute trade-off in ANNS by leveraging adaptive representations on different phases of construction and inference for ANNS data structures.\\n3 Problem Setup, Notation, and Preliminaries\\nThe problem setup of approximate nearest neighbor search (ANNS) [21] consists of a database of N data points, [x1, x2, . . . , xN ], and a query, q, where the goal is to  approximately  retrieve the nearest data point to the query. Both the database and query are embedded to Rd using a representation function   : X   Rd, often a neural network that can be learned through various representation learning paradigms [2, 19, 20, 40, 42].\\nMatryoshka Representations (MRs). The d-dimensional representations from   can have a nested structure like Matryoshka Representations (MRs) [31] in-built    MR(d). Matryoshka Representation Learning (MRL) learns these nested representations with a simple strategy of optimizing the same training objective at varying dimensionalities. These granularities are ordered such that the lowest representation size forms a prefix for the higher-dimensional representations. So, high-dimensional MR inherently contains low-dimensional representations of varying granularities that can be accessed for free   first m-dimensions (m   [d]) ie.,  MR(d)[1 : m] from the d-dimensional MR form an m-dimensional representation which is as accurate as its independently trained rigid representation (RR) counterpart    RR(m). Training an encoder with MRL does not involve any overhead or hyperparameter tuning and works seamlessly across modalities, training objectives, and architectures.\\nInverted File Index (IVF). IVF [48] is an ANNS data structure used in web-scale search sys- tems [16] owing to its simplicity, minimal compute overhead, and high accuracy. IVF construction involves clustering (coarse quantization through k-means) [36] on d-dimensional representation that results in an inverted file list [53] of all the data points in each cluster. During search, d-dimensional query representation is assigned to the most relevant cluster (Ci; i   [k]) by finding the closest cen- troid ( i) using an appropriate distance metric (L2 or cosine). This is followed by an exhaustive linear search across all data points in the cluster which gives the closest NN (see Figure 5 in Appendix A for IVF overview). Lastly, IVF can scale to web-scale by utilizing a hierarchical IVF structure within each cluster [16]. Table 2 in Appendix A describes the retrieval formula for multiple variants of IVF.\\nOptimized Product Quantization (OPQ). Product Quantization (PQ) [23] works by splitting a d-dimensional real vector into m sub-vectors and quantizing each sub-vector with an independent 2b\\nlength codebook across the database. After PQ, each d-dimensional vector can be represented by a compact m   b bit vector; we make each vector m bytes long by fixing b = 8. During search time, distance computation between the query vector and PQ database is extremely efficient with only m codebook lookups. The generality of PQ encompasses scalar/vector quantization [15, 36] as special cases. However, PQ can be further improved by rotating the d-dimensional space appropriately to maximize distance preservation after PQ. Optimized Product Quantization (OPQ) [13] achieves this by learning an orthonormal projection matrix R that rotates the d-dimensional space to be more amenable to PQ. OPQ shows consistent gains over PQ across a variety of ANNS tasks and has become the default choice in standard composite indices [22, 24].\\nDatasets. We evaluate the ANNS algorithms while changing the representations used for the search thus making it impossible to evaluate on the usual benchmarks [1]. Hence we experiment with two public datasets: (a) ImageNet-1K [45] dataset on the task of image retrieval   where the goal is to retrieve images from a database (1.3M image train set) belonging to the same class as the query image (50K image validation set) and (b) Natural Questions (NQ) [32] dataset on the task of question answering through dense passage retrieval   where the goal is to retrieve the relevant passage from a database (21M Wikipedia passages) for a query (3.6K questions).\\nMetrics Performance of ANNS is often measured using recall score [22], k-recall@N   recall of the exact NN across search complexities which denotes the recall of k  true  NN when N data points are retrieved. However, the presence of labels allows us to compute 1-NN (top-1) accuracy. Top-1 accuracy is a harder and more fine-grained metric that correlates well with typical retrieval metrics like recall and mean average precision (mAP@k). Even though we report top-1 accuracy by default during experimentation, we discuss other metrics in Appendix C. Finally, we measure the compute overhead of ANNS using MFLOPS/query and also provide wall-clock times (see Appendix B.1).\\nEncoders. For ImageNet, we encode both the database and query set using a ResNet50 ( I ) [19] trained on ImageNet-1K. For NQ, we encode both the passages in the database and the questions in the query set using a BERT-Base ( N ) [10] model fine-tuned on NQ for dense passage retrieval [27].\\nWe use the trained ResNet50 models with varying representation sizes (d = [8, 16, . . . , 2048]; default being 2048) as suggested by Kusupati et al. [31] alongside the MRL-ResNet50 models trained with MRL for the same dimensionalities. The RR and MR models are trained to ensure the supervised one-vs-all classification accuracy across all data dimensionalities is nearly the same   1-NN accuracy of 2048-d RR and MR models are 71.19% and 70.97% respectively on ImageNet-1K. Independently trained models,  RR(d) , output d = [8, 16 . . . , 2048] dimensional RRs while a single MRL-ResNet50 model,  MR(d) We also train BERT-Base models in a similar vein as the aforementioned ResNet50 models. The key difference is that we take a pre-trained BERT-Base model and fine-tune on NQ as suggested by Karpukhin et al. [27] with varying (5) representation sizes (bottlenecks) (d = [48, 96, . . . , 768]; default being 768) to obtain  RR(d) that creates RRs for the NQ dataset. To get the MRL-BERT- Base model, we fine-tune a pre-trained BERT-Base encoder on the NQ train dataset using the MRL objective with the same granularities as RRs to obtain  MR(d) which contains all five granularities. Akin to ResNet50 models, the RR and MR BERT-Base models on NQ are built to have similar 1-NN accuracy for 768-d of 52.2% and 51.5% respectively. More implementation details can be found in Appendix B and additional experiment-specific information is provided at the appropriate places.\\n4 AdANNS   Adaptive ANNS\\nIn this section, we present our proposed AdANNS framework that exploits the inherent flexibility of matryoshka representations to improve the accuracy-compute trade-off for semantic search com- ponents. Standard ANNS pipeline can be split into two key components: (a) search data structure that indexes and stores data points, (b) query-point computation method that outputs (approximate) distance between a given query and data point. For example, standard IVFOPQ [24] method uses an IVF structure to index points on full-precision vectors and then relies on OPQ for more efficient distance computation between the query and the data points during the linear scan.\\nBelow, we show that AdANNS can be applied to both the above-mentioned ANNS components and provides significant gains on the computation-accuracy tradeoff curve. In particular, we present AdANNS-IVF which is AdANNS version of the standard IVF index structure [48], and the closely related ScaNN structure [16]. We also present AdANNS-OPQ which introduces representation adap- tivity in the OPQ, an industry-default quantization. Then, in Section 4.3 we further demonstrate the combination of the two techniques to get AdANNS-IVFOPQ   an AdANNS version of IVFOPQ [24]   and AdANNS-DiskANN, a similar variant of DiskANN [22]. Overall, our experiments show that AdANNS-IVF is significantly more accuracy-compute optimal compared to the IVF indices built on RRs and AdANNS-OPQ is as accurate as the OPQ on RRs while being significantly cheaper.\\n4.1 AdANNS-IVF\\nRecall from Section 1 that IVF has a clustering and a linear scan phase, where both phase use same dimen- sional rigid representation. Now, AdANNS-IVF allows the clustering phase to use the first dc dimensions of the given matryoshka represen- tation (MR). Similarly, the linear scan within each cluster uses ds di- mensions, where again ds represents top ds coordinates from MR. Note that setting dc = ds results in non- adaptive regular IVF. Intuitively, we would set dc   ds, so that instead of clustering with a high-dimensional representation, we can approximate it accurately with a low-dimensional em- bedding of size dc followed by a lin- ear scan with a higher ds-dimensional representation. Intuitively, this helps in the smooth search of design space for state-of-the-art accuracy-compute trade-off. Furthermore, this can provide a precise operating point on accuracy-compute tradeoff curve which is critical in several practical settings.\\nOur experiments on regular IVF with MRs and RRs (IVF-MR & IVF-RR) of varying dimensionali- ties and IVF configurations (# clusters, # probes) show that (Figure 2) matryoshka representations result in a significantly better accuracy-compute trade-off. We further studied and found that learned lower-dimensional representations offer better accuracy-compute trade-offs for IVF than higher- dimensional embeddings (see Appendix E for more results).\\nAdANNS utilizes d-dimensional matryoshka representation to get accurate dc and ds dimensional vectors at no extra compute cost. The resulting AdANNS-IVF provides a much better accuracy- compute trade-off (Figure 2) on ImageNet-1K retrieval compared to IVF-MR, IVF-RR, and MG- IVF-RR   multi-granular IVF with rigid representations (akin to AdANNS without MR)   a strong baseline that uses dc and ds dimensional RRs. Finally, we exhaustively search the design space of IVF by varying dc, ds   [8, 16, . . . , 2048] and the number of clusters k   [8, 16, . . . , 2048]. Please see Appendix E for more details. For IVF experiments on the NQ dataset, please refer to Appendix G.\\nEmpirical results. Figure 2 shows that AdANNS-IVF outperforms the baselines across all accuracy-compute settings for ImageNet-1K retrieval. AdANNS-IVF results in 10  lower compute for the best accuracy of the extremely expensive MG-IVF-RR and non-adaptive IVF-MR. Specifi- cally, as shown in Figure 1a, AdANNS-IVF is up to 1.5% more accurate for the same compute and has up to 100  lesser FLOPS/query (90  real-world speed-up!) than the status quo ANNS on rigid representations (IVF-RR). We filter out points for the sake of presentation and encourage the reader to check out Figure 8 in Appendix E for an expansive plot of all the configurations searched.\\nThe advantage of AdANNS for construction of search structures is evident from the improvements in IVF (AdANNS-IVF) and can be easily extended to other ANNS structures like ScaNN [16] and\\nHNSW [38]. For example, HNSW consists of multiple layers with graphs of NSW graphs [37] of increasing complexity. AdANNS can be adopted to HNSW, where the construction of each level can be powered by appropriate dimensionalities for an optimal accuracy-compute trade-off. In general, AdANNS provides fine-grained control over compute overhead (storage, working memory, inference, and construction cost) during construction and inference while providing the best possible accuracy.\\n4.2 AdANNS-OPQ\\nStandard Product Quantization (PQ) essentially performs block-wise vector quantization via cluster- ing. For example, suppose we need 32-byte PQ compressed vectors from the given 2048 dimensional representations. Then, we can chunk the representations in m = 32 equal blocks/sub-vectors of 64-d each, and each sub-vector space is clustered into 28 = 256 partitions. That is, the representation of each point is essentially cluster-id for each block. Optimized PQ (OPQ) [13] further refines this idea, by first rotating the representations using a learned orthogonal matrix, and then applying PQ on top of the rotated representations. In ANNS, OPQ is used extensively to compress vectors and improves approximate distance computation primarily due to significantly lower memory overhead than storing full-precision data points IVF.\\nAdANNS-OPQ utilizes MR representations to apply OPQ on lower-dimensional representations. That is, for a given quantization budget, AdANNS allows using top ds   d dimensions from MR and then computing clusters with ds/m-dimensional blocks where m is the number of blocks. Depending on ds and m, we have further flexibility of trading-off dimensionality/capacity for increasing the number of clusters to meet the given quantization budget. AdANNS-OPQ tries multiple ds, m, and number of clusters for a fixed quantization budget to obtain the best performing configuration.\\nWe experimented with 8   128 byte OPQ budgets for both ImageNet and Natural Questions retrieval with an exhaustive search on the quantized vectors. We compare AdANNS-OPQ which uses MRs of varying granularities to the baseline OPQ built on the highest dimensional RRs. We also evaluate OPQ vectors obtained projection using SVD [14] on top of the highest-dimensional RRs.\\nEmpirical results. Figures 3 and 1b show that AdANNS-OPQ significantly outperforms   up to 4% accuracy gain   the baselines (OPQ on RRs) across compute budgets on both ImageNet and NQ. In particular, AdANNS-OPQ tends to match the accuracy of a 64-byte (a typical choice in ANNS) OPQ baseline with only a 32-byte budget. This results in a 2  reduction in both storage and compute FLOPS which translates to significant gains in real-world web-scale deployment (see Appendix D).\\nWe only report the best AdANNS-OPQ for each budget typically obtained through a much lower- dimensional MR (128 & 192; much faster to build as well) than the highest-dimensional MR (2048 & 768) for ImageNet and NQ respectively (see Appendix G for more details). At the same time, we\\n71 70 e *0 69 oO icy 3 68     5 69 3 8 8 67 < < 66 4 68 a e  h- OPQ-RR   65  @ AdANNS-IVFOPQ 67 veal > + OPQ-RR-SVD Ae Rigid-IVFOPQ 16 32 64 8 16 32 64 Compute Budget (Bytes) Compute Budget (Bytes)\\n\\n\\nnote that building compressed OPQ vectors on projected RRs using SVD to the smaller dimensions (or using low-dimensional RRs, see Appendix D) as the optimal AdANNS-OPQ does not help in improving the accuracy. The significant gains we observe in AdANNS-OPQ are purely due to better information packing in MRs   we hypothesize that packing the most important information in the initial coordinates results in a better PQ quantization than RRs where the information is uniformly distributed across all the dimensions [31, 49]. See Appendix D for more details and experiments.\\n4.3 AdANNS for Composite Indices\\nWe now extend AdANNS to composite indices [24] which put together two main ANNS building blocks   search structures and quantization   together to obtain efficient web-scale ANNS indices used in practice. A simple instantiation of a composite index would be the combination of IVF and OPQ   IVFOPQ   where the clustering in IVF happens with full-precision real vectors but the linear scan within each cluster is approximated using OPQ-compressed variants of the representation   since often the full-precision vectors of the database cannot fit in RAM. Contemporary ANNS indices like DiskANN [22] make this a default choice where they build the search graph with a full-precision vector and approximate the distance computations during search with an OPQ-compressed vector to obtain a very small shortlist of retrieved datapoints. In DiskANN, the shortlist of data points is then re-ranked to form the final list using their full-precision vectors fetched from the disk. AdANNS is naturally suited to this shortlist-rerank framework: we use a low-d MR for forming index, where we could tune AdANNS parameters according to the accuracy-compute trade-off of the graph and OPQ vectors. We then use a high-d MR for re-ranking.\\nEmpirical results. Figure 4 shows that AdANNS-IVFOPQ is 1   4% better than the baseline at all the PQ compute budgets. Furthermore, AdANNS-IVFOPQ has the same ac- curacy as the baselines at 8  lower overhead. With DiskANN, AdANNS accelerates shortlist generation by us- ing low-dimensional representations and recoups the accuracy by re- ranking with the highest-dimensional MR at negligible cost. Table 1 shows that AdANNS-DiskANN is more accurate than the baseline for both 1-NN and ranking performance at only half the cost. Using low-dimensional representations further speeds up inference in AdANNS-DiskANN (see Appendix F).\\nRR-2048 AdANNS PQ Budget (Bytes) Top-1 Accuracy (%) mAP@10 (%) Precision@40 (%) 32 70.37 62.46 65.65 16 70.56 64.70 68.25\\n\\n<table><thead><tr><th></th><th>RR-2048</th><th>AdANNS</th></tr></thead><tbody><tr><td>PQ Budget (Bytes)</td><td>32</td><td>16</td></tr><tr><td colspan=\"3\">Top-1 Accuracy (%) mAP@10 (%) 62.46 64.70</td></tr><tr><td>Precision @40 (%)</td><td>65.65</td><td>68.25</td></tr></tbody></table>\\nThese results show the generality of AdANNS and its broad applicability across a variety of ANNS indices built on top of the base building blocks. Currently, AdANNS piggybacks on typical ANNS pipelines for their inherent accounting of the real-world system constraints [16, 22, 25]. However, we believe that AdANNS s flexibility and significantly better accuracy-compute trade-off can be further informed by real-world deployment constraints. We leave this high-potential line of work that requires extensive study to future research.\\n5 Further Analysis and Discussion\\n5.1 Compute-aware Elastic Search During Inference\\nAdANNS search structures cater to many specific large-scale use scenarios that need to satisfy precise resource constraints during construction as well as inference. However, in many cases, construction and storage of the indices are not the bottlenecks or the user is unable to search the design space. In these settings, AdANNS-D enables adaptive inference through accurate yet cheaper distance computation using the low-dimensional prefix of matryoshka representation. Akin to composite indices (Section 4.3) that use PQ vectors for cheaper distance computation, we can use the low- dimensional MR for faster distance computation on ANNS structure built non-adaptively with a high-dimensional MR without any modifications to the existing index.\\nEmpirical results. Figure 2 shows that for a given compute budget using IVF on ImageNet-1K retrieval, AdANNS-IVF is better than AdANNS-IVF-D due to the explicit control during the building\\nof the ANNS structure which is expected. However, the interesting observation is that AdANNS-D matches or outperforms the IVF indices built with MRs of varying capacities for ImageNet retrieval.\\nHowever, these methods are applicable in specific scenarios of deployment. Obtaining optimal AdANNS search structure (highly accurate) or even the best IVF-MR index relies on a relatively expensive design search but delivers indices that fit the storage, memory, compute, and accuracy constraints all at once. On the other hand AdANNS-D does not require a precisely built ANNS index but can enable compute-aware search during inference. AdANNS-D is a great choice for setups that can afford only one single database/index but need to cater to varying deployment constraints, e.g., one task requires 70% accuracy while another task has a compute budget of 1 MFLOPS/query.\\n5.2 Why MRs over RRs?\\nQuite a few of the gains from AdANNS are owing to the quality and capabilities of matryoshka representations. So, we conducted extensive analysis to understand why matryoshka representations seem to be more aligned for semantic search than the status-quo rigid representations.\\nDifficulty of NN search. Relative contrast (Cr) [18] is inversely proportional to the difficulty of nearest neighbor search on a given database. On ImageNet-1K, Figure 14 shows that MRs have better Cr than RRs across dimensionalities, further supporting that matryoshka representations are more aligned (easier) for NN search than existing rigid representations for the same accuracy. More details and analysis about this experiment can be found in Appendix H.2.\\nClustering distributions. We also investigate the potential deviation in clustering distributions for MRs across dimensionalities compared to RRs. Unlike the RRs where the information is uniformly diffused across dimensions [49], MRs have hierarchical information packing. Figure 11 in Appendix E.3 shows that matryoshka representations result in clusters similar (measured by total variation distance [33]) to that of rigid representations and do not result in any unusual artifacts.\\nRobustness. Figure 9 in Appendix E shows that MRs continue to be better than RRs even for out-of- distribution (OOD) image queries (ImageNetV2 [44]) using ANNS. It also shows that the highest data dimensionality need not always be the most robust which is further supported by the higher recall using lower dimensions. Further details about this experiment can be found in Appendix E.1.\\nGenerality across encoders. IVF-MR consistently has higher accuracy than IVF-RR across dimen- sionalities despite having similar accuracies with exact NN search (for ResNet50 on ImageNet and BERT-Base on NQ). We find that our observations on better alignment of MRs for NN search hold across neural network architectures, ResNet18/34/101 [19] and ConvNeXt-Tiny [35]. Appendix H.3 delves deep into the experimentation done using various neural architectures on ImageNet-1K.\\nRecall score analysis. Analysis of recall score (see Appendix C) in Appendix H.1 shows that for a similar top-1 accuracy, lower-dimensional representations have better 1-Recall@1 across search complexities for IVF and HNSW on ImageNet-1K. Across the board, MRs have higher recall scores and top-1 accuracy pointing to easier  searchability  and thus suitability of matryoshka representations for ANNS. Larger-scale experiments and further analysis can be found in Appendix H.\\nThrough these analyses, we argue that matryoshka representations are better suited for semantic search than rigid representations, thus making them an ideal choice for AdANNS.\\n5.3 Search for AdANNS Hyperparameters\\nChoosing the optimal hyperparameters for AdANNS, such as dc, ds, m, # clusters, # probes, is an interesting and open problem that requires more rigorous examination. As the ANNS index is formed once and used for potentially billions of queries with massive implications for cost, latency and queries-per-second, a hyperparameter search for the best index is generally an acceptable industry practice [22, 38]. The Faiss library [24] provides guidelines2 to choose the appropriate index for a specific problem, including memory constraints, database size, and the need for exact results. There have been efforts at automating the search for optimal indexing parameters, such as Autofaiss3, which maximizes recall given compute constraints.\\n2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss\\nIn case of AdANNS, we suggest starting at the best configurations of MRs followed by a local design space search to lead to near-optimal AdANNS configurations (e.g. use IVF-MR to bootstrap AdANNS-IVF). We also share some observations during the course of our experiments:\\n1. AdANNS-IVF: Top-1 accuracy generally improves (with diminishing returns after a point) with increasing dimensionality of clustering (d,.) and search (d,), as we show on ImageNet variants and with multiple encoders in the Appendix (Figures 9 and 15). Clustering with low-d MRs matches the performance of high-d MRs as they likely contain similar amounts of useful information, making the increased compute cost not worth the marginal gains. Increasing # probes naturally boosts performance (Appendix, Figure 10a). Lastly, it is generally accepted that a good starting point for the # clusters k is V/Np/2, where Np is the number of indexable items [39]. k = Np is the optimal choice of k from a FLOPS computation perspective as can be seen in Appendix B.1.\\n2. AdANNS-OPQ: we observe that for a fixed compute budget in bytes (m), the top-1 accuracy reaches a peak at d < dmax (Appendix, Table 4). We hypothesize that the better performance of AdANNS-OPQ at d < dmax is due to the curse of dimensionality, i.e. it is easier to learn PQ codebooks on smaller embeddings with similar amounts of information. We find that using an MR with d = 4   m is a good starting point on ImageNet and NQ. We also suggest using an 8-bit (256-length) codebook for OPQ as the default for each of the sub-block quantizer.\\n3. AdANNS-DiskANN: Our observations with DiskANN are consistent with other indexing struc- tures, i.e. the optimal graph construction dimensionality d < dmax (Appendix, Figure 12). A careful study of DiskANN on different datasets is required for more general guidelines to choose graph construction and OPQ dimensionality d.\\n5.4 Limitations\\nAdANNS s core focus is to improve the design of the existing ANNS pipelines. To use AdANNS on a corpus, we need to back-fill [43] the MRs of the data   a significant yet a one-time overhead. We also notice that high-dimensional MRs start to degrade in performance when optimizing also for an extremely low-dimensional granularity (e.g., < 24-d for NQ)   otherwise is it quite easy to have comparable accuracies with both RRs and MRs. Lastly, the existing dense representations can only in theory be converted to MRs with an auto-encoder-style non-linear transformation. We believe most of these limitations form excellent future work to improve AdANNS further.\\n6 Conclusions\\nWe proposed a novel framework, AdANNS , that leverages adaptive representations for different phases of ANNS pipelines to improve the accuracy-compute tradeoff. AdANNS utilizes the inherent flexibility of matryoshka representations [31] to design better ANNS building blocks than the standard ones which use the rigid representation in each phase. AdANNS achieves SOTA accuracy-compute trade-off for the two main ANNS building blocks: search data structures (AdANNS-IVF) and quantization (AdANNS-OPQ). The combination of AdANNS-based building blocks leads to the construction of better real-world composite ANNS indices   with as much as 8  reduction in cost at the same accuracy as strong baselines   while also enabling compute-aware elastic search. Finally, we note that combining AdANNS with elastic encoders [11] enables truly adaptive large-scale retrieval.\\nAcknowledgments\\nWe are grateful to Kaifeng Chen, Venkata Sailesh Sanampudi, Sanjiv Kumar, Harsha Vardhan Simhadri, Gantavya Bhatt, Matthijs Douze and Matthew Wallingford for helpful discussions and feedback. Aditya Kusupati also thanks Tom Duerig and Rahul Sukthankar for their support. Part of the paper s large-scale experimentation is supported through a research GCP credit award from Google Cloud and Google Research. Sham Kakade acknowledges funding from the ONR award N00014-22-1-2377 and NSF award CCF-2212841. Ali Farhadi acknowledges funding from the NSF awards IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543, and gifts from Allen Institute for Artificial Intelligence and Google.\\nReferences\\n[1] M. Aum ller, E. Bernhardsson, and A. Faithfull. Ann-benchmarks: A benchmarking tool for approximate nearest neighbor algorithms. Information Systems, 87:101374, 2020.\\n[2] Y. Bengio. Deep learning of representations for unsupervised and transfer learning. In Proceed- ings of ICML workshop on unsupervised and transfer learning, pages 17 36. JMLR Workshop and Conference Proceedings, 2012.\\n[3] E. Bernhardsson. Annoy: Approximate Nearest Neighbors in C++/Python, 2018. URL https://pypi.org/project/annoy/. Python package version 1.13.0.\\n[4] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. Computer networks and ISDN systems, 30(1-7):107 117, 1998.\\nIEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897.\\n[6] T. Chen, L. Li, and Y. Sun. Differentiable product quantization for end-to-end embedding compression. In International Conference on Machine Learning, pages 1617 1626. PMLR, 2020.\\n[7] K. L. Clarkson. An algorithm for approximate closest-point queries. In Proceedings of the tenth annual symposium on Computational geometry, pages 160 164, 1994.\\n[8] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational geometry, pages 253 262, 2004.\\n[9] J. Dean. Challenges in building large-scale information retrieval systems. In Keynote of the 2nd ACM International Conference on Web Search and Data Mining (WSDM), volume 10, 2009.\\n[10] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n[11] Devvrit, S. Kudugunta, A. Kusupati, T. Dettmers, K. Chen, I. Dhillon, Y. Tsvetkov, H. Hannaneh, S. Kakade, A. Farhadi, and P. Jain. Matformer: Nested transformer for elastic inference. arXiv preprint arxiv:2310.07707, 2023.\\n[12] J. H. Friedman, J. L. Bentley, and R. A. Finkel. An algorithm for finding best matches in logarithmic expected time. ACM Transactions on Mathematical Software (TOMS), 3(3):209  226, 1977.\\n[13] T. Ge, K. He, Q. Ke, and J. Sun. Optimized product quantization for approximate nearest neighbor search. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2946 2953, 2013.\\n[14] G. Golub and W. Kahan. Calculating the singular values and pseudo-inverse of a matrix. Journal of the Society for Industrial and Applied Mathematics, Series B: Numerical Analysis, 2(2): 205 224, 1965.\\n[15] R. Gray. Vector quantization. IEEE Assp Magazine, 1(2):4 29, 1984.\\n[16] R. Guo, P. Sun, E. Lindgren, Q. Geng, D. Simcha, F. Chern, and S. Kumar. Accelerating large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning, pages 3887 3896. PMLR, 2020.\\n[17] N. Gupta, P. H. Chen, H.-F. Yu, C.-J. Hsieh, and I. S. Dhillon. End-to-end learning to index and search in large output spaces. arXiv preprint arXiv:2210.08410, 2022.\\n[18] J. He, S. Kumar, and S.-F. Chang. On the difficulty of nearest neighbor search. In International Conference on Machine Learning (ICML), 2012.\\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770  778, 2016.\\n[20] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729 9738, 2020.\\n[21] P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 604 613, 1998.\\n[22] S. Jayaram Subramanya, F. Devvrit, H. V. Simhadri, R. Krishnawamy, and R. Kadekodi. Diskann: Fast accurate billion-point nearest neighbor search on a single node. Advances in Neural Information Processing Systems, 32, 2019.\\n[23] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010.\\n[24] J. Johnson, M. Douze, and H. J gou. Billion-scale similarity search with GPUs. Transactions on Big Data, 7(3):535 547, 2019. IEEE\\n[25] W. B. Johnson. Extensions of lipschitz mappings into a hilbert space. Contemp. Math., 26: 189 206, 1984.\\n[26] I. T. Jolliffe and J. Cadima. Principal component analysis: a review and recent developments. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065):20150202, 2016.\\n[27] V. Karpukhin, B. O guz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020.\\n[28] S. Kornblith, J. Shlens, and Q. V. Le. Do better imagenet models transfer better? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2661 2671, 2019.\\n[29] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned index structures. In Proceedings of the 2018 international conference on management of data, pages 489 504, 2018.\\n[30] A. Kusupati, M. Wallingford, V. Ramanujan, R. Somani, J. S. Park, K. Pillutla, P. Jain, S. Kakade, and A. Farhadi. Llc: Accurate, multi-purpose learnt low-dimensional binary codes. Advances in Neural Information Processing Systems, 34:23900 23913, 2021.\\n[31] A. Kusupati, G. Bhatt, A. Rege, M. Wallingford, A. Sinha, V. Ramanujan, W. Howard-Snyder, K. Chen, S. Kakade, P. Jain, and A. Farhadi. Matryoshka representation learning. In Advances in Neural Information Processing Systems, December 2022.\\n[32] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453 466, 2019.\\n[33] D. A. Levin and Y. Peres. Markov chains and mixing times, volume 107. American Mathematical Soc., 2017.\\n[34] W. Li, Y. Zhang, Y. Sun, W. Wang, W. Zhang, and X. Lin. Approximate nearest neighbor search on high dimensional data experiments, analyses, and improvement. IEEE Transactions on Knowledge and Data Engineering, 2020.\\n[35] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11976 11986, 2022.\\n[36] S. Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2): 129 137, 1982.\\n[37] Y. Malkov, A. Ponomarenko, A. Logvinov, and V. Krylov. Approximate nearest neighbor algorithm based on navigable small world graphs. Information Systems, 45:61 68, 2014.\\n[38] Y. A. Malkov and D. Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis & Machine Intelligence, 42(04):824 836, 2020.\\n[39] K. Mardia, J. Kent, and J. Bibby. Multivariate analysis. Probability and Mathematical Statistics, 1979.\\n[40] P. Nayak. Understanding searches better than ever before. Google AI Blog, 2019. URL https: //blog.google/products/search/search-language-understanding-bert/.\\n[41] A. Neelakantan, T. Xu, R. Puri, A. Radford, J. M. Han, J. Tworek, Q. Yuan, N. Tezak, J. W. Kim, C. Hallacy, et al. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005, 2022.\\n[42] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748 8763. PMLR, 2021.\\n[43] V. Ramanujan, P. K. A. Vasu, A. Farhadi, O. Tuzel, and H. Pouransari. Forward compatible training for representation learning. arXiv preprint arXiv:2112.02805, 2021.\\n[44] B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning, pages 5389 5400. PMLR, 2019.\\n[45] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211 252, 2015.\\n[46] R. Salakhutdinov and G. Hinton. Semantic hashing. International Journal of Approximate Reasoning, 50(7):969 978, 2009.\\n[47] H. V. Simhadri, G. Williams, M. Aum ller, M. Douze, A. Babenko, D. Baranchuk, Q. Chen, L. Hosseini, R. Krishnaswamy, G. Srinivasa, et al. Results of the neurips 21 challenge on billion-scale approximate nearest neighbor search. arXiv preprint arXiv:2205.03763, 2022.\\n[48] J. Sivic and A. Zisserman. Video google: A text retrieval approach to object matching in videos. In Computer Vision, IEEE International Conference on, volume 3, pages 1470 1470. IEEE Computer Society, 2003.\\n[49] D. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N. Srebro. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):2822 2878, 2018.\\n[50] C. Waldburger. As search needs evolve, microsoft makes ai tools for better search available to researchers and developers. Microsoft AI Blog, 2019. URL https://blogs.microsoft. com/ai/bing-vector-search/.\\n[51] M. Wang, X. Xu, Q. Yue, and Y. Wang. A comprehensive survey and experimental comparison of graph-based approximate nearest neighbor search. Proceedings of the VLDB Endowment, 14 (11):1964 1978, 2021.\\n[52] R. Weber, H.-J. Schek, and S. Blott. A quantitative analysis and performance study for similarity- search methods in high-dimensional spaces. In VLDB, volume 98, pages 194 205, 1998.\\n[53] I. H. Witten, I. H. Witten, A. Moffat, T. C. Bell, T. C. Bell, E. Fox, and T. C. Bell. Managing gigabytes: compressing and indexing documents and images. Morgan Kaufmann, 1999.\\nA AdANNS Framework\\nAlgorithm 1 AdANNS-IVF Psuedocode\\n# Index database to construct clusters and build inverted file system\\ndef adannsConstruction(database, d_cluster, num_clusters):\\n# Slice database with cluster construction dim (d_cluster) xb = database[:d_cluster] cluster_centroids = constructClusters(xb, num_clusters)\\nreturn cluster_centroids def adannsInference(queries, centroids, d_shortlist, d_search, num_probes, k): # Slice queries and centroids with cluster shortlist dim (d_shortlist) xq = queries[:d_shortlist] xc = centroids[:d_shortlist] for q in queries: # compute distance of query from each cluster centroid candidate_distances = computeDistances(q, xc) # sort cluster candidates by distance and choose small number to probe cluster_candidates = sortAscending(candidate_distances)[:num_probes] database_candidates = getClusterMembers(cluster_candidates) # Linear Scan all shortlisted clusters with search dim (d_search) k_nearest_neighbors[q] = linearScan(q, database_candidates, d_search,\\nreturn k_nearest_neighbors\\nANNS Inference ANNS Construction Database C _ construct Clusters With Ra ~<--.2 -=- . 7 eS TTT esse, Ha eco Hi eco Bi Xe, XeC, XeC, Linear Top k Scan with ->   | _ Relevant Rs Data Points\\n\\n\\nFigure 5: The schematic of inverted file index (IVF) outlaying the construction and inference phases. Adaptive representations can be utilized effectively in the decoupled components of clustering and searching for a better accuracy-compute trade-off (AdANNS-IVF).\\nMethod Retrieval Formula during Inference IVF-RR IVF-MR AdANNS-IVF MG-IVF-RR   RR(d)(q)    RR(d)(xj) , s.t. h(q) = arg minh   RR(d)(q)    RR(d)     MR(d)(q)    MR(d)(xj) , s.t. h(q) = arg minh   MR(d)(q)    MR(d)   MR(ds)(q)    MR(ds)(xj) , s.t. h(q) = arg minh   MR(dc)(q)    MR(dc)   RR(ds)(q)    RR(ds)(xj) , s.t. h(q) = arg minh   RR(dc)(q)    RR(dc)   arg minj Ch(q) arg minj Ch(q) h h arg minj Ch(q) arg minj Ch(q) h h     AdANNS-IVF-D arg minj Ch(q) IVFOPQ   MR(d)(q)[1 :  d]    MR(d)(xj)[1 :  d] , s.t. h(q) = arg minh   MR(d)(q)[1 :  d]    MR(d) arg minj Ch(q)   PQ(m,b)(q)    PQ(m,b)(xj) , s.t. h(q) = arg minh   (q)    h  h [1 :  d] \\n\\n<table><thead><tr><th>Method |</th><th>Retrieval Formula during Inference</th></tr></thead><tbody><tr><td>IVF-RR</td><td>. d d . d pk arg minjecy,,, | (g)  oR (w,)||, st. h(q) = arg min, ||o** (q)  \\\\ mn</td></tr><tr><td>AdANNS-IVF | MG-IVF-RR</td><td>arg minjecy,, |OMR (q)  OR) (x5)|], st. h(q) = arg min), |]oM) (q) ~ Hi Ride) | Ride) arg minjec,,,, |O&amp;)(q)  dR) (a)||, st. h(g) = arg miny, ||OR) (q) </td></tr><tr><td>AdANNS-IVF-D IVFOPQ</td><td>i d jl d jl i MR (d d) jl arg minjec, ., [OM (@)[L = d]  OM (#;)[1 : d]ll, st. h(q) = argmin,, ||\" (@)[I : 3  wR: dll arg minjecy,,) []G2 (g)  GPR (a), 8. h(q) = arg min), ||9(q) ~ pall</td></tr></tbody></table>\\nB Training and Compute Costs\\nA bulk of our ANNS experimentation was written with Faiss [24], a library for efficient similarity search and clustering. AdANNS was implemented from scratch (Algorithm 1) due to difficulty in decoupling clustering and linear scan with Faiss, with code available at https://github.com/ RAIVNLab/AdANNS. We also provide a version of AdANNS with Faiss optimizations with the restriction that Dc   Ds as a limitation of the current implementation, which can be further optimized. All ANNS experiments (AdANNS-IVF, MG-IVF-RR, IVF-MR, IVF-RR, HNSW, HNSWOPQ, IVFOPQ) were run on an Intel Xeon 2.20GHz CPU with 12 cores. Exact Search (Flat L2, PQ, OPQ) and DiskANN experiments were run with CUDA 11.0 on a A100-SXM4 NVIDIA GPU with 40G RAM. The wall-clock inference times quoted in Figure 1a and Table 3 are reported on CPU with Faiss optimizations, and are averaged over three inference runs for ImageNet-1K retrieval.\\nTable 3: Comparison of AdANNS-IVF and Rigid-IVF wall-clock inference times for ImageNet-1K retrieval. AdANNS-IVF has up to   1.5% gain over Rigid-IVF for a fixed search latency per query.\\nAdANNS-IVF Rigid-IVF Top-1 Search Latency/Query (ms) Top-1 Search Latency/Query (ms) 70.02 70.08 70.19 70.36 70.60 0.03 0.06 0.06 0.88 5.57 68.51 68.54 68.74 69.20 70.13 0.02 0.05 0.08 0.86 5.67\\nTable 3: Comparison of AdANNS-IVF and Rigid-IVF wall-clock inference times for ImageNet-1K retrieval. AdANNS-IVF has up to   1.5% gain over Rigid-IVF for a fixed search latency per query.\\n<table><thead><tr><th></th><th>AdANNS-IVF</th><th></th><th>Rigid-IVF</th></tr><tr><th>Top-1</th><th>Search Latency/Query (ms)</th><th>| Top-1</th><th>Search Latency/Query (ms)</th></tr></thead><tbody><tr><td>70.02</td><td>0.03</td><td>68.51</td><td>0.02</td></tr><tr><td>70.08</td><td>0.06</td><td>68.54</td><td>0.05</td></tr><tr><td>70.19</td><td>0.06</td><td>68.74</td><td>0.08</td></tr><tr><td>70.36</td><td>0.88</td><td>69.20</td><td>0.86</td></tr><tr><td>70.60</td><td>5.57</td><td>70.13</td><td>5.67</td></tr></tbody></table>\\nDPR [27] on NQ [32]. We follow the setup on the DPR repo4: the Wikipedia corpus has 21 million passages and Natural Questions dataset for open-domain QA settings. The training set contains 79,168 question and answer pairs, the dev set has 8,757 pairs and the test set has 3,610 pairs.\\nB.1 Inference Compute Cost\\nWe evaluate inference compute costs for IVF in MegaFLOPS per query (MFLOPS/query) as shown in Figures 2, 10a, and 8 as follows:\\nC = dsk + npdsND k\\nwhere dc is the cluster construction embedding dimensionality, ds is the embedding dim used for linear scan within each probed cluster, which is controlled by # of search probes np. Finally, k is the number of clusters |Ci| indexed over database of size ND. The default setting in this work, unless otherwise stated, is np = 1, k = 1024, ND = 1281167 (ImageNet-1K trainset). Vanilla IVF supports only dc = ds, while AdANNS-IVF provides flexibility via decoupling clustering and search (Section 4). AdANNS-IVF-D is a special case of AdANNS-IVF with the flexibility restricted to inference, i.e., dc is a fixed high-dimensional MR.\\n4https://github.com/facebookresearch/DPR\\nC Evaluation Metrics\\nIn this work, we primarily use top-1 accuracy (i.e. 1-Nearest Neighbor), recall@k, corrected mean average precision (mAP@k) [30] and k-Recall@N (recall score), which are defined over all queries Q over indexed database of size ND as:\\ntop-1 = Q correct_pred@1 |Q|\\nRecall@k = Q correct_pred@k |Q|   num_classes |ND|\\nwhere correct_pred@k is the number of k-NN with correctly predicted labels for a given query. As noted in Section 3, k-Recall@N is the overlap between k exact search nearest neighbors (considered as ground truth) and the top N retrieved documents. As Faiss [24] supports a maximum of 2048- NN while searching the indexed database, we report 40-Recall@2048 in Figure 13. Also note that for ImageNet-1K, which constitutes a bulk of the experimentation in this work, |Q| = 50000, |ND| = 1281167 and num_classes = 1000. For ImageNetv2 [44], |Q| = 10000 and num_classes = 1000, and for ImageNet-4K [31], |Q| = 210100, |ND| = 4202000 and num_classes = 4202. For NQ [32], |Q| = 3610 and |ND| = 21015324. As NQ consists of question-answer pairs (instance- level), num_classes = 3610 for the test set.\\n(a) Exact Search + OPQ on ImageNet-1K  (b) IVF + OPQ on ImageNet-1K \\n\\n\\n(c) DiskANN + OPQ on ImageNet-1K\\n(d) HNSW + OPQ on ImageNet-1K\\nD AdANNS-OPQ\\nIn this section, we take a deeper dive into the quantization characteristics of MR. In this work, we restrict our focus to optimized product quantization (OPQ) [13], which adds a learned space rotation and dimensionality permutation to PQ s sub-vector quantization to learn more optimal PQ codes. We compare OPQ to vanilla PQ on ImageNet in Table 4, and observe large gains at larger embedding dimensionalities, which agrees with the findings of Jayaram Subramanya et al. [22].\\nWe perform a study of composite OPQ m   b indices on ImageNet-1K across compression compute budgets m (where b = 8, i.e. 1 Byte), i.e. Exact Search with OPQ, IVF+OPQ, HNSW+OPQ, and DiskANN+OPQ, as seen in Figure 6. It is evident from these results:\\n1. Learning OPQ codebooks with AdANNS (Figure 6a) provides a 1-5% gain in top-1 accuracy over rigid representations at low compute budgets (  32 Bytes). AdANNS-OPQ saturates to Rigid-OPQ performance at low compression (  64 Bytes).\\n2. For IVF, learning clusters with MRs instead of RRs (Figure 6b) provides substantial gains (1- 4%). In contrast to Exact-OPQ, using AdANNS for learning OPQ codebooks does not provide substantial top-1 accuracy gains over MR with d = 2048 (highest), though it is still slightly better or equal to MR-2048 at all compute budgets. This further supports that IVF performance generally scales with embedding dimensionality, which is consistent with our findings on ImageNet across robustness variants and encoders (See Figures 9 and 15 respectively).\\n3. Note that in contrast to Exact, IVF, and HNSW coarse quantizers, DiskANN inherently re-ranks the retrieved shortlist with high-precision embeddings (d = 2048), which is reflected in its high top-1 accuracy. We find that AdANNS with 8-byte OPQ (Figure 6c) matches the top-1 accuracy of rigid representations using 32-byte OPQ, for a 4  cost reduction for the same accuracy. Also note that using AdANNS provides large gains over using MR-2048 at high compression (1.5%), highlighting the necessity of AdANNS s flexibility for high-precision retrieval at low compute budgets.\\n4. Our findings on the HNSW-OPQ composite index (Figure 6d) are consistent with all other indices, i.e. HNSW graphs constructed with AdANNS OPQ codebooks provide significant gains over RR and MR, especially at high compression (  32 Bytes).\\nOPQ on NQ dataset\\n40 45 & 40 Ss 35 > > 3 35 3  30 3 5 30 8 < 2  35  @ AdANNS-OPQ  & 20 ~@ AdANNS-IvF+OPQ 2 90  @ + OPQ-MR-768 Fc} ~@ IVF+OPQ-MR-768 Ae Rigid-oPQ 15 Ae Rigid-IVF+OPQ 8 16 32 48 64 96 8 16 32 448  464   (96 Compute Budget (Bytes) Compute Budget (Bytes)\\n\\n\\n(b) IVF + OPQ on Natural Questions\\nOur observations on ImageNet with ResNet-50 MR across search structures also extend to the Natural Questions dataset with Dense Passage Retriever (DPR with BERT-Base MR embeddings). We note that AdANNS provides gains over RR-768 embeddings for both Exact Search and IVF with OPQ (Figure 7a and 7b). We find that similar to ImageNet (Figure 15) IVF performance on Natural Questions generally scales with dimensionality. AdANNS thus reduces to MR-768 performance for M   16. See Appendix G for a more in-depth discussion of AdANNS with DPR on Natural Questions.\\nConfig PQ OPQ d m Top-1 mAP@10 P@100 Top-1 mAP@10 P@100 8 8 62.18 56.71 61.23 62.22 56.70 61.23 16 8 16 67.91 67.85 62.85 62.95 67.21 67.21 67.88 67.96 62.96 62.94 67.21 67.21 32 8 16 32 68.80 69.57 69.44 63.62 64.22 64.20 67.86 68.12 68.12 68.91 69.47 69.47 63.63 64.20 64.23 67.86 68.12 68.12 64 8 16 32 64 68.39 69.77 70.13 70.12 63.40 64.43 64.67 64.69 67.47 68.25 68.38 68.42 68.38 69.95 70.05 70.18 63.42 64.55 64.65 64.70 67.60 68.38 68.38 68.38 128 8 16 32 64 67.27 69.51 70.27 70.61 61.99 64.32 64.72 64.93 65.78 68.12 68.51 68.49 68.40 69.78 70.60 70.65 63.11 64.56 64.97 64.98 67.34 68.38 68.51 68.51 256 8 16 32 64 66.06 68.56 70.08 70.48 60.44 63.33 64.83 64.98 64.09 66.95 68.38 68.55 67.90 69.92 70.59 70.69 62.69 64.71 65.15 65.09 66.95 68.51 68.64 68.64 512 8 16 32 64 65.09 67.68 69.51 70.53 59.03 62.11 64.01 65.02 62.53 65.39 67.34 68.52 67.51 69.67 70.44 70.72 62.12 64.53 65.11 65.17 66.56 68.38 68.64 68.64 1024 8 16 32 64 64.58 66.84 68.71 69.88 58.26 61.07 62.92 64.35 61.75 64.09 66.04 67.68 67.26 69.34 70.43 70.81 62.07 64.23 65.03 65.19 66.56 68.12 68.64 68.64 2048\\n\\n<table><thead><tr><th colspan=\"2\">Config</th><th colspan=\"3\">PQ</th><th colspan=\"3\">OPQ</th></tr><tr><th>d</th><th>m |</th><th>Top-l1</th><th>mAP@10</th><th>P@100</th><th>| Top-l1</th><th>mAP@10</th><th>P@100</th></tr></thead><tbody><tr><td></td><td>8 |</td><td>62.18</td><td>56.71</td><td>61.23</td><td>| 62.22</td><td>56.70</td><td>61.23</td></tr><tr><td>16</td><td>8 | 6 |</td><td>67.91 67.85</td><td>62.85 62.95</td><td>67.21 67.21</td><td>| 67.88 | 67.96</td><td>62.96 62.94</td><td>67.21 67.21</td></tr><tr><td rowspan=\"2\">32</td><td>8 |</td><td>68.80</td><td>63.62</td><td>67.86</td><td>| 68.91</td><td>63.63</td><td>67.86</td></tr><tr><td>6 | 32 |</td><td>69.57 69.44</td><td>64.22 64.20</td><td>68.12 68.12</td><td>| 69.47 | 69.47</td><td>64.20 64.23</td><td>68.12 68.12</td></tr><tr><td>64</td><td>8 | 6 | 32 | 64 |</td><td>68.39 69.77 70.13 70.12</td><td>63.40 64.43 64.67 64.69</td><td>67.47 68.25 68.38 68.42</td><td>| 68.38 | 69.95 | 70.05 | 70.18</td><td>63.42 64.55 64.65 64.70</td><td>67.60 68.38 68.38 68.38</td></tr><tr><td rowspan=\"2\">128</td><td>8 | 6 | 32 | 64 |</td><td>67.27 69.51 70.27 70.61</td><td>61.99 64.32 64.72 64.93</td><td>65.78 68.12 68.51 68.49</td><td>| 68.40 | 69.78 | 70.60 | 70.65</td><td>63.11 64.56 64.97 64.98</td><td>67.34 68.38 68.51 68.51</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"4\">256</td><td>8 |</td><td>66.06</td><td>60.44</td><td>64.09</td><td>| 67.90</td><td>62.69</td><td>66.95</td></tr><tr><td>6 |</td><td>68.56</td><td>63.33</td><td>66.95</td><td>| 69.92</td><td>64.71</td><td>68.51</td></tr><tr><td>32 |</td><td>70.08</td><td>64.83</td><td>68.38</td><td>| 70.59</td><td>65.15</td><td>68.64</td></tr><tr><td>64 |</td><td>70.48</td><td>64.98</td><td>68.55</td><td>| 70.69</td><td>65.09</td><td>68.64</td></tr><tr><td rowspan=\"4\">512</td><td>8 |</td><td>65.09</td><td>59.03</td><td>62.53</td><td>| 67.51</td><td>62.12</td><td>66.56</td></tr><tr><td>6 |</td><td>67.68</td><td>62.11</td><td>65.39</td><td>| 69.67</td><td>64.53</td><td>68.38</td></tr><tr><td>32 |</td><td>69.51</td><td>64.01</td><td>67.34</td><td>| 70.44</td><td>65.11</td><td>68.64</td></tr><tr><td>64 |</td><td>70.53</td><td>65.02</td><td>68.52</td><td>| 70.72</td><td>65.17</td><td>68.64</td></tr><tr><td rowspan=\"4\">1024</td><td>8 |</td><td>64.58</td><td>58.26</td><td>61.75</td><td>| 67.26</td><td>62.07</td><td>66.56</td></tr><tr><td>6 |</td><td>66.84</td><td>61.07</td><td>64.09</td><td>| 69.34</td><td>64.23</td><td>68.12</td></tr><tr><td>32 |</td><td>68.71</td><td>62.92</td><td>66.04</td><td>| 70.43</td><td>65.03</td><td>68.64</td></tr><tr><td>64 |</td><td>69.88</td><td>64.35</td><td>67.68</td><td>| 70.81</td><td>65.19</td><td>68.64</td></tr><tr><td rowspan=\"3\">2048</td><td>8 |</td><td>62.19</td><td>56.11</td><td>59.80</td><td>| 66.89</td><td>61.69</td><td>66.30</td></tr><tr><td>6 |</td><td>65.99</td><td>60.27</td><td>63.18</td><td>| 69.25</td><td>64.09</td><td>67.99</td></tr><tr><td>32 |</td><td>67.99</td><td>62.04</td><td>64.74</td><td>| 70.39</td><td>64.97</td><td>68.51</td></tr></tbody></table>\\n8 16 32 64\\n62.19 65.99 67.99 69.20\\n56.11 60.27 62.04 63.46\\n59.80 63.18 64.74 66.40\\n66.89 69.25 70.39 70.57\\n61.69 64.09 64.97 65.15\\n66.30 67.99 68.51 68.51\\nE AdANNS-IVF\\nInverted file index (IVF) [48] is a simple yet powerful ANNS data structure used in web-scale search systems [16]. IVF construction involves clustering (coarse quantization often through k-means) [36] on d-dimensional representation that results in an inverted file list [53] of all the data points in each cluster. During search, the d-dimensional query representation is first assigned to the closest clusters (# probes, typically set to 1) and then an exhaustive linear scan happens within each cluster to obtain the nearest neighbors. As seen in Figure 9, IVF top-1 accuracy scales logarithmically with increasing representation dimensionality d on ImageNet-1K/V2/4K. The learned low-d representations thus provide better accuracy-compute trade-offs compared to high-d representations, thus furthering the case for usage of AdANNS with IVF.\\nOur proposed adaptive variant of IVF, AdANNS-IVF, decouples the clustering, with dc dimensions, and the linear scan within each cluster, with ds dimensions   setting dc = ds results in non- adaptive vanilla IVF. This helps in the smooth search of design space for the optimal accuracy- compute trade-off. A naive instantiation yet strong baseline would be to use explicitly trained\\nN = 70 oo @ a } q 4 : , . E 69 Sete octet   68 $ *   b x 3 BT ate abo Bot   AdANNS-IVF &67| 2   AdANNS-IVE-D = Ame ee 6 8 * MG-IVF-RR B66 8 ex x MG-IVF-SVD F os e IVE-MR al! wide eel, a IVERR 0.1 1 10 100 MFLOPS/Query\\n\\n\\ndc and ds dimensional rigid representations (called MG-IVF-RR, for multi-granular IVF with rigid representations). We also examine the setting of adaptively choosing low-dimensional MR to linear scan the shortlisted clusters built with high-dimensional MR, i.e. AdANNS-IVF-D, as seen in Table 5. As seen in Figure 8, AdANNS-IVF provides pareto-optimal accuracy-compute tradeoff across inference compute. This figure is a more exhaustive indication of AdANNS-IVF behavior compared to baselines than Figures 1a and 2. AdANNS-IVF is evaluated for all possible tuples of dc, ds, k = |C|   {8, 16, . . . , 2048}. AdANNS-IVF-D is evaluated for a pre-built IVF index with dc = 2048 and ds   {8, . . . , 2048}. MG-IVF-RR configurations are evaluated for dc   {8, . . . , ds}, ds   {32, . . . , 2048} and k = 1024 clusters. A study over additional k values is omitted due to high compute cost. Finally, IVF-MR and IVF-RR configurations are evaluated for dc = ds   {8, 16, . . . , 2048} and k   {256, . . . , 8192}. Note that for a fair comparison, we use np = 1 across all configurations. We discuss the inference compute for these settings in Appendix B.1.\\nE.1 Robustness\\n(a) ImageNet-1K  (b) ImageNetV2  (c) ImageNet-4K \\n\\n\\nAs shown in Figure 9, we examined the clustering capabilities of MRs on both in-distribution (ID) queries via ImageNet-1K and out-of-distribution (OOD) queries via ImageNetV2 [44], as well as on larger-scale ImageNet-4K [31]. For ID queries on ImageNet-1K (Figure 9a), IVF-MR is at least as accurate as Exact-RR for d   256 with a single search probe, demonstrating the quality of in- distribution low-d clustering with MR. On OOD queries (Figure 9b), we observe that IVF-MR is on average 2% more robust than IVF-RR across all cluster construction and linear scan dimensionalities d. It is also notable that clustering with MRs followed by linear scan with # probes = 1 is more robust than exact search with RR embeddings across all d   2048, indicating the adaptability of MRs to distribution shifts during inference. As seen in Table 5, on ImageNetV2 AdANNS-IVF-D is the best\\nd AdANNS-IVF-D IVF-MR Exact-MR IVF-RR Exact-RR 8 16 32 64 128 256 512 1024 2048 53.51 57.32 57.32 57.85 58.02 58.01 58.03 57.66 58.04 50.44 56.35 57.64 58.01 58.09 58.33 57.84 57.58 58.04 50.41 56.64 57.96 58.94 59.13 59.18 59.40 59.11 59.63 49.03 55.04 56.06 56.84 56.14 55.60 55.46 54.80 56.17 48.79 55.08 56.69 57.37 57.17 57.09 57.12 57.53 57.84\\n\\n<table><thead><tr><th>d</th><th>AdANNS-IVF-D</th><th>| IVF-MR_</th><th>Exact-MR</th><th>| IVF-RR_</th><th>Exact-RR</th></tr></thead><tbody><tr><td>8 16</td><td>53.51 57,32</td><td>50.44 56.35</td><td>50.41 56.64</td><td>49.03 55.04</td><td>48.79 55.08</td></tr><tr><td>32</td><td>57.32</td><td>57.64</td><td>57.96</td><td>56.06</td><td>56.69</td></tr><tr><td>64</td><td>57.85</td><td>58.01</td><td>58.94</td><td>56.84</td><td>57.37</td></tr><tr><td>128</td><td>58.02</td><td>58.09</td><td>59.13</td><td>56.14</td><td>57.17</td></tr><tr><td>256</td><td>58.01</td><td>58.33</td><td>59.18</td><td>55.60</td><td>57.09</td></tr><tr><td>512</td><td>58.03</td><td>57.84</td><td>59.40</td><td>55.46</td><td>57.12</td></tr><tr><td>1024</td><td>57.66</td><td>57.58</td><td>59.11</td><td>54.80</td><td>57.53</td></tr><tr><td>2048</td><td>58.04</td><td>58.04</td><td>59.63</td><td>56.17</td><td>57.84</td></tr></tbody></table>\\nOn 4-million scale ImageNet-4K (Figure 9c), we observe similar accuracy trends of IVF-MR compared to Exact-MR as in ImageNet-1K (Figure 9a) and ImageNetV2 (Figure 9b). We omit baseline IVF-RR and Exact-RR experiments due to high compute cost at larger scale.\\nE.2 IVF-MR Ablations\\n(a) 4K Search Probes (np)  (b) Centroid Recall \\n\\n\\ng  \\nT & i=\\nAs seen in Figure 10a, IVF-MR can match the accuracy of Exact Search on ImageNet-4K with   100  less compute. We also explored the capability of MRs at retrieving cluster centroids with low-d compared to a ground truth of 2048-d with k-Recall@N, as seen in Figure 10b. MRs were able to saturate to near-perfect 1-Recall@N for d   32 and N   4, indicating the potential of AdANNS at matching exact search performance with less than 10 search probes np.\\nE.3 Clustering Distribution\\nWe examined the distribution of learnt clusters across embedding dimensionalities d for both MR and RR models, as seen in Figure 11. We observe IVF-MR to have less variance than IVF-RR at d   {8, 16}, and slightly higher variance for d   32, while IVF-MR outperforms IVF-RR in top-1 across all d (Figure 9a). This indicates that although MR learns clusters that are less uniformly distributed than RR at high d, the quality of learnt clustering is superior to RR across all d. Note that a uniform distribution is N/k data points per cluster, i.e.   1250 for ImageNet-1K with k = 1024. We quantitatively evaluate the proximity of the MR and RR clustering distributions with Total Variation\\n300 Gyy=0.00078 350 Gry=0.00016 2504 Gry, 2040 0.00128 3004 300 | dry, 2048= 0.00095 4 2 2504 4 - @ 2004 3 750) : aea2 4 m ORR  So 150  5 200  6 2004 8 81504 21504 5 1004 E E 2 2 100 2 1004 504 504 504      _ 0   500 1000 1500 2000 2500   500 1000 1500 2000 2500   500 1000 1500 2000 2500 Number of data points per cell Number of data points per cell Number of data points per cell 3504 dry=0.00037 400 4 . dry= 0.00068 dry, 2048= 0.00078 \\\\ rv, 202e=0.00064 3004 NN 2 MR-64 2 \\\\ MR-256 % 2504 = e % 300 = ; & Bm RR64 2 m RR256 5 200-4 6 5 5 2004 2150+ a 2 2 100 100 4 504 o4 0   500 1000 1500 2000 2500   500 1000 1500 2000 2500   500 1000 1500 2000 2500 Number of data points per cell Number of data points per cell Number of data points per cell dry=0,00076 4005 \\\\ 400 > dry= 0.00054 3507 ry, 2008=0.00065   dry, 2040= 0.00054 Mt w 3004   in 2 = MR512 | 23007   = MR1024 | = 300- * m= MR-2048   2505 mw RR512 v \\\\ m RR-1024 v m RR-2048 3) | 3 \\\\   200 5 2004   5 2004 2 8   2 = 150-4       2 2 \" 2 100 4 1004 1 hy 100 4 504 ey o4 0 0   500 1000 1500 2000 2500   500 1000 1500 2000 2500   500 1000 1500 2000 2500 Number of data points per cell Number of data points per cell Number of data points per cell\\n\\n\\nDistance [33], which is defined over two discrete probability distributions p, q over [n] as follows:\\n1 drv (p,q) = 3 > \\\\pi   ai i [n]\\nWe also compute dT V,2048(MR-d) = dT V (MR-d, RR-2048), which evaluates the total variation dis- tance of a given low-d MR from high-d RR-2048. We observe a monotonically decreasing dT V,2048 with increasing d, which demonstrates that MR clustering distributions get closer to RR-2048 as we increase the embedding dimensionality d. We observe in Figure 11 that dT V (MR-d, RR-d)   7e   4 for d   {8, 256, . . . , 2048} and   3e   4 for d   {16, 32, 64}. These findings agree with the top-1 improvement of MR over RR as shown in Figure 9a, where there are smaller improvements for d   {16, 32, 64} (smaller dT V ) and larger improvements for d   {8, 256, . . . , 2048} (larger dT V ). These results demonstrate a correlation between top-1 performance of IVF-MR and the quality of clusters learnt with MR.\\nF AdANNS-DiskANN\\nDiskANN is a state-of-the-art graph-based ANNS index capable of serving queries from both RAM and SSD. DiskANN builds a greedy best-first graph with OPQ distance computation, with compressed vectors stored in memory. The index and full-precision vectors are stored on the SSD. During search,\\nd M =8 M =16 M =32 M =48 M =64 8 16 32 64 128 256 512 1024 2048 495 555 669 864 1182 1923 2802 5127 9907 - 571 655 855 1311 1779 3272 5456 9833 - - 653 843 1156 1744 3423 5724 10205 - - - 844 1161 2849 2780 4683 10183 - - - 848 2011 1818 3171 5087 9329\\n\\n<table><thead><tr><th>d</th><th>M=8</th><th>M=16</th><th>M=32</th><th>M=48</th><th>M=64</th></tr></thead><tbody><tr><td>8</td><td>495</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>16</td><td>555</td><td>571</td><td>-</td><td>-</td><td>-</td></tr><tr><td>32</td><td>669</td><td>655</td><td>653</td><td>-</td><td>-</td></tr><tr><td>64</td><td>864</td><td>855</td><td>843</td><td>844</td><td>848</td></tr><tr><td>128</td><td>) 1182</td><td>= 1311</td><td>1156</td><td>1161</td><td>2011</td></tr><tr><td>256</td><td>| 1923</td><td>1779</td><td>1744</td><td>2849</td><td>1818</td></tr><tr><td>512</td><td>| 2802</td><td>3272</td><td>3423</td><td>2780</td><td>= 3171</td></tr><tr><td>1024</td><td>| 5127</td><td>5456</td><td>5724</td><td>4683</td><td>5087</td></tr><tr><td>2048</td><td>| 9907</td><td>9833</td><td>10205</td><td>10183</td><td>9329</td></tr></tbody></table>\\nAs with IVF, DiskANN is also well suited to the flexibility provided by AdANNS as we demonstrate on both ImageNet and NQ that the optimal PQ codebook for a given compute budget is learnt with a smaller em- bedding dimensionality d (see Fig- ures 6c and 7a). We demonstrate the capability of AdANNS-DiskANN with a compute budget of m   {32, 64} in Table 1. We tabulate the search time latency of AdANNS- DiskANN in microseconds ( s) in Table 6, which grows linearly with graph construction dimensionality d. We also examine DiskANN-MR with SSD graph indices on ImageNet-1K across OPQ budgets for distance com- putation mdc   {32, 48, 64}, as seen in Figure 12. With SSD indices, we store PQ-compressed vectors on disk with mdisk = mdc, which es- sentially disables DiskANN s implicit high-precision re-ranking. We ob- serve similar trends to other composite ANNS indices on ImageNet, where the optimal dim for fixed OPQ budget is not the highest dim (d = 1024 with fp32 embeddings is current highest dim supported by DiskANN which stores vectors in 4KB sectors on disk). This provides further motiva- tion for AdANNS-DiskANN, which leverages MRs to provide flexible access to the optimal dim for quantization and thus enables similar Top-1 accuracy to Rigid DiskANN for up to 1/4 the cost (Figure 6c).\\nG AdANNS on Natural Questions\\nIn addition to image retrieval on ImageNet, we also experiment with dense passage retrieval (DPR) on Natural Questions. As shown in Figure 6, MR representations are 1   10% more accurate than their\\n5https://github.com/microsoft/DiskANN\\nRR counterparts across PQ compute budgets with Exact Search + OPQ on NQ. We also demonstrate that IVF-MR is 1   2.5% better than IVF-RR for Precision@k, k   {1, 5, 20, 100, 200}. Note that on NQ, IVF loses   10% accuracy compared to exact search, even with the RR-768 baseline. We hypothesize the weak performance of IVF owing to poor clusterability of the BERT-Base embeddings fine-tuned on the NQ dataset. A more thorough exploration of AdANNS-IVF on NQ is an immediate future work and is in progress.\\nH Ablations\\nH.1 Recall Score Analysis\\nry S   o   Cy   g   NS 90 40-Recall@2048 (%) 1-Recall@1 (%) 88 1 2 4 10 1 2 4 10 IVF Search Probes (np) IVF Search Probes (n,) 100 100 95 _ 90 PS x = 90   d x d + 80 + g a 8 mS 85 a 8   a t |@ a 16 rs 70 m 32 8 80 m 32 @ = 6 |y m 64 x = 12 |< 75 m 128 $ 60 m 256 m 256 m= 512 70 m 812 50 m 1024 m 1024 m 2048 65 m 2048 1 2 4 10 20 1 2 4 10 20 HNSW Search Probes (efSearch) HNSW Search Probes (efSearch) 100 100 95 95 = 90 = 2 = 90 3 35 = q   ie S 75 = MR8 = 80 = MR8 v m MR-64 m MR-64 70 = MR-256 75 = MR-256 m= MR-2048 m MR-2048 85, 2 4 10 70, 2 4 10 IVF Search Probes (np) IVF Search Probes (np)\\n\\n\\n22.5 20.0 17.5 15.0 12.5 10.0 7.5 5.0 2.5 e MR @ RR Relative Contrast so) Xe} a7 ro S Vv bs xo) Coe et PP ES Ss vs\\n\\n\\nRepresentation Size\\nIn this section we also examine the variation of k-Recall@N with by probing a larger search space with IVF and HNSW indices. For IVF, search probes represent the number of clusters shortlisted for linear scan during inference. For HNSW, search quality is controlled by the ef Search parameter [38], which represents the closest neighbors to query q at level lc of the graph and is analogous to number of search probes in IVF. As seen in Figure 13, general trends show a) an intuitive increase in recall with increasing search probes np) for fixed search probes, b) a decrease in recall with increasing search dimensionality d c) similar trends in ImageNet-1K and 4  larger ImageNet-4K.\\nH.2 Relative Contrast\\nWe utilize Relative Contrast [18] to capture the difficulty of nearest neighbors search with IVF-MR compared to IVF-RR. For a given database X = {xi   Rd, i = 1, . . . , ND}, a query q   Rd, and a distance metric D(., .) we compute relative contrast Cr as a measure of the difficulty in finding the 1-nearest neighbor (1-NN) for a query q in database X as follows:\\n1. Compute Dq\\nmin = min i=1...n D(q, xi), i.e. the distance of query q to its nearest neighbor xq nn   X\\n2. Compute Dq mean = Ex[D(q, x)] as the average distance of query q from all database points x   X\\nDq mean Dq 3. Relative Contrast of a given query C q , which is a measure of how separable the r = min nn is from an average point in the database x query s nearest neighbor xq\\n4. Compute an expectation over all queries for Relative Contrast over the entire database as\\nCr = Eq[Dq Eq[Dq mean] min]\\nIt is evident that Cr captures the difficulty of Nearest Neighbor Search in database X, as a Cr   1 indicates that for an average query, its nearest neighbor is almost equidistant from a random point in the database. As demonstrated in Figure 14, MRs have higher Rc than RR Embeddings for an Exact Search on ImageNet-1K for all d   16. This result implies that a portion of MR s improvement over RR for 1-NN retrieval across all embedding dimensionalities d [31] is due to a higher average separability of the MR 1-NN from a random database point.\\nH.3 Generality across Encoders\\nWe perform an ablation over the representation function   : X   Rd learnt via a backbone neural network (primarily ResNet50 in this work), as detailed in Section 3. We also train MRL models [31]  M R(d) on ResNet18/34/101 [19] that are as accurate as their independently trained RR baseline models  RR(d), where d is the default max representation size of each architecture. We also train\\nMRL with a ConvNeXt-Tiny backbone with [d] = {48, 96, 192, 384, 786}. MR-768 has a top-1 accuracy of 79.45% compared to independently trained publicly available RR-768 baseline with top-1 accuracy 82.1% (Code and RR model available on the official repo6). We note that this training had no hyperparameter tuning whatsoever, and this gap can be closed with additional model training effort. We then compare clustering the MRs via IVF-MR with k = 2048, np = 1 on ImageNet-1K to Exact-MR, which is shown in Figure 15. IVF-MR shows similar trends across backbones compared to Exact-MR, i.e. a maximum top-1 accuracy drop of   1.6% for a single search probe. This suggests the clustering capabilities of MR extend beyond an inductive bias of  M R(d)   ResNet50, though we leave a detailed exploration for future work.\\n75 , a eee ee 3 70 > i)  65 (s) < ~ 60 & e@ ResNet18 r a ResNet32   @ = ResNet50   IVF-MR * ResNet101 == Exact-MR # ConvNeXt-Tiny Oe SS PP a Pp N % oOo WW a aN a ss Representation Size\\n\\n\\n6https://github.com/facebookresearch/ConvNeXt\\n', source='requests/doc_4842138e-4018-4ffa-b83d-3a317f5838a8', source_type=<SourceType.application_pdf: 'application/pdf'>, num_chunks=202, metadata={}, chunks=[ResponseChunk(id='chunk_c377b054-3128-4337-b06f-510e1e10dc56', content='3 2 0 2 t c O 8 1 ] G L . s c [ 2 v 5 3 4 9 1 . 5 0 3 2 : v i X r a AdANNS:', chunk_index=1, num_tokens=53, metadata={'section_titles': ['Introduction', ' Equal contribution.', 'Abstract', 'AdANNS: A Framework for Adaptive Semantic Search'], 'pages': [1]}), ResponseChunk(id='chunk_af92519a-d774-4703-9c14-ec2d682ccefc', content='A Framework for Adaptive Semantic Search Aniket Rege    Aditya Kusupati    Sharan Ranjit S  Alan Fan  Qingqing Cao , Sham Kakade  Prateek Jain  Ali Farhadi   University of Washington,  Google Research,  Harvard University {kusupati,ali}@cs.washington.edu, prajain@google.com Abstract Web-scale search systems learn an encoder to embed a given query which is then hooked into an approximate nearest neighbor search (ANNS) pipeline to retrieve similar data points.', chunk_index=2, num_tokens=111, metadata={'section_titles': ['Introduction', ' Equal contribution.', 'Abstract', 'AdANNS: A Framework for Adaptive Semantic Search'], 'pages': [1]}), ResponseChunk(id='chunk_cd3b8727-9381-47de-b53c-9ac806de84c7', content='To accurately capture tail queries and data points, learned representations typically are rigid, high-dimensional vectors that are generally used as-is in the entire ANNS pipeline and can lead to computationally expensive retrieval. In this paper, we argue that instead of rigid representations, different stages of ANNS can leverage adaptive representations of varying capacities to achieve significantly better accuracy-compute trade-offs, i.e., stages of ANNS that can get away with more approximate computation should use a lower-capacity representation of the same data point. To this end, we introduce AdANNS , a novel ANNS design framework that explicitly leverages the flexibility of Ma- tryoshka Representations [31]. We demonstrate state-of-the-art accuracy-compute trade-offs using novel AdANNS-based key ANNS building blocks like search data structures (AdANNS-IVF) and quantization (AdANNS-OPQ). For exam- ple on ImageNet retrieval, AdANNS-IVF is up to 1.5% more accurate than the rigid representations-based IVF [48] at the same compute budget; and matches accuracy while being up to 90  faster in wall-clock time.', chunk_index=3, num_tokens=236, metadata={'section_titles': ['Introduction', ' Equal contribution.', 'Abstract', 'AdANNS: A Framework for Adaptive Semantic Search'], 'pages': [1]}), ResponseChunk(id='chunk_c69e5d2e-949a-4100-b9e1-4e800d16e14a', content='For Natural Questions, 32-byte AdANNS-OPQ matches the accuracy of the 64-byte OPQ baseline [13] constructed using rigid representations   same accuracy at half the cost! We further show that the gains from AdANNS translate to modern-day composite ANNS indices that combine search structures and quantization. Finally, we demonstrate that AdANNS can enable inference-time adaptivity for compute-aware search on ANNS indices built non-adaptively on matryoshka representations.', chunk_index=4, num_tokens=99, metadata={'section_titles': ['Introduction', ' Equal contribution.', 'Abstract', 'AdANNS: A Framework for Adaptive Semantic Search'], 'pages': [1]}), ResponseChunk(id='chunk_baacf86c-d2fc-472e-8d8c-91d8b1caf118', content='Code is open-sourced at https://github.com/RAIVNLab/AdANNS. Introduction Semantic search [24] on learned representations [40, 41, 50] is a major component in retrieval pipelines [4, 9]. In its simplest form, semantic search methods learn a neural network to embed queries as well as a large number (N ) of data points in a d-dimensional vector space. For a given query, the nearest (in embedding space) point is retrieved using either an exact search or using approximate nearest neighbor search (ANNS) [21] which is now indispensable for real-time large-scale retrieval. Existing semantic search methods learn fixed or rigid representations (RRs) which are used as is in all the stages of ANNS (data structures for data pruning and quantization for cheaper distance computation; see Section 2). That is, while ANNS indices allow a variety of parameters for searching the design space to optimize the accuracy-compute trade-off, the provided data dimensionality is typically assumed to be an immutable parameter.', chunk_index=5, num_tokens=213, metadata={'section_titles': ['Introduction', ' Equal contribution.', 'Abstract', 'AdANNS: A Framework for Adaptive Semantic Search'], 'pages': [1]}), ResponseChunk(id='chunk_828fe295-bb73-4668-bb2b-49ebe3e2fa3f', content='To make it concrete, let us consider inverted file index (IVF) [48], a popular web-scale ANNS technique [16]. IVF has two stages (Section 3) during inference: (a) cluster mapping: mapping the query to a cluster of data points [36], and (b) linear Equal contribution. 37th Conference on Neural Information Processing Systems (NeurIPS 2023). scan: distance computation w.r.t all points in the retrieved cluster to find the nearest neighbor (NN).', chunk_index=6, num_tokens=105, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_be6b9dff-142f-4f10-a35a-98a8984a79d8', content='Standard IVF utilizes the same high-dimensional RR for both phases, which can be sub-optimal. Why the sub-optimality? Imagine one needs to partition a dataset into k clusters for IVF and the dimensionality of the data is d   IVF uses full d representation to partition into k clusters. However, suppose we have an alternate approach that somehow projects the data in d/2 dimensions and learns 2k clusters. Note that the storage and computation to find the nearest cluster remains the same in both cases, i.e., when we have k clusters of d dimensions or 2k clusters of d/2 dimensions. 2k clusters can provide significantly more refined partitioning, but the distances computed between queries and clusters could be significantly more inaccurate after projection to d/2 dimensions. So, if we can find a mechanism to obtain a d/2-dimensional representation of points that can accurately approximate the topology/distances of d-dimensional representation, then we can potentially build significantly better ANNS structure that utilizes different capacity representations for the cluster mapping and linear scan phases of IVF.', chunk_index=7, num_tokens=218, metadata={'section_titles': [' Equal contribution.'], 'pages': [2]}), ResponseChunk(id='chunk_6fb9916b-a5b6-4e3f-bd32-f3860f9be0b9', content='But how do we find such adaptive representations? These desired adaptive representations should be cheap to obtain and still ensure distance preservation across dimensionality. Post-hoc dimensionality reduction techniques like SVD [14] and random projections [25] on high-dimensional RRs are potential candidates, but our experiments indicate that in practice they are highly inaccurate and do not preserve distances well enough (Figure 2).', chunk_index=8, num_tokens=78, metadata={'section_titles': [' Equal contribution.'], 'pages': [2]}), ResponseChunk(id='chunk_d155f415-9ebe-411f-a550-0a592fffe77e', content='Instead, we identify that the recently proposed Matryoshka Representations (MRs) [31] satisfy the specifications for adaptive representations. Matryoshka representations pack information in a hierarchical nested manner, i.e., the first m-dimensions of the d-dimensional MR form an accurate low-dimensional representation while being aware of the information in the higher dimensions.', chunk_index=9, num_tokens=72, metadata={'section_titles': [' Equal contribution.'], 'pages': [2]}), ResponseChunk(id='chunk_eb9e3ad4-29aa-487b-9787-a75135841a91', content='This allows us to deploy MRs in two major and novel ways as part of ANNS: (a) low-dimensional representations for accuracy-compute optimal clustering and quantization, and (b) high-dimensional representations for precise re-ranking when feasible. To this effort, we introduce AdANNS , a novel design framework for semantic search that uses matryoshka representation-based adaptive representations across different stages of ANNS to ensure significantly better accuracy-compute trade-off than the state-of-the-art baselines. Typical ANNS systems have two key components: (a) search data structure to store datapoints, (b) distance computation to map a given query to points in the data structure.', chunk_index=10, num_tokens=137, metadata={'section_titles': [' Equal contribution.'], 'pages': [2]}), ResponseChunk(id='chunk_ee995e6f-c95e-4910-9d87-1db3353b382d', content='Through AdANNS, we address both these components and significantly improve their performance. In particular, we first propose AdANNS-IVF (Section 4.1) which tackles the first component of ANNS systems. AdANNS- IVF uses standard full-precision computations but uses adaptive representations for different IVF stages. On ImageNet 1-NN image retrieval (Figure 1a), AdANNS-IVF is up to 1.5% more accurate for the compute budget and 90  cheaper in deployment for the same accuracy as IVF.', chunk_index=11, num_tokens=117, metadata={'section_titles': [' Equal contribution.'], 'pages': [2]}), ResponseChunk(id='chunk_60c5d4d3-5efb-403e-be4a-ef3946896fe6', content='We then propose AdANNS-OPQ (Section 4.2) which addresses the second component by using AdANNS-based quantization (OPQ [13])   here we use exhaustive search overall points. AdANNS- OPQ is as accurate as the baseline OPQ on RRs while being at least 2  faster on Natural Ques- tions [32] 1-NN passage retrieval (Figure 1b). Finally, we combine the two techniques to obtain AdANNS-IVFOPQ (Section 4.3) which is more accurate while being much cheaper   up to 8    than the traditional IVFOPQ [24] index.', chunk_index=12, num_tokens=141, metadata={'section_titles': [' Equal contribution.', 'We make the following key contributions:', '2 Related Work'], 'pages': [3]}), ResponseChunk(id='chunk_634318c6-5193-419e-8e8b-3e8b6eaa9f2e', content='To demonstrate generality of our technique, we adapt AdANNS to DiskANN [22] which provides interesting accuracy-compute tradeoff; see Table 1.', chunk_index=13, num_tokens=33, metadata={'section_titles': [' Equal contribution.', 'We make the following key contributions:', '2 Related Work'], 'pages': [3]}), ResponseChunk(id='chunk_5cb5b93a-a59e-4a9f-98b6-4d85559a9988', content='While MR already has multi-granular representations, careful integration with ANNS building blocks is critical to obtain a practical method and is our main contribution.', chunk_index=14, num_tokens=30, metadata={'section_titles': [' Equal contribution.', 'We make the following key contributions:', '2 Related Work'], 'pages': [3]}), ResponseChunk(id='chunk_420ced87-fd6b-409a-a0da-ed4dcc6f6697', content='In fact, Kusupati et al. [31] proposed a simple adaptive retrieval setup that uses smaller-dimensional MR for shortlisting in re- trieval followed by precise re-ranking with a higher-dimensional MR. Such techniques, unfortunately, cannot be scaled to industrial systems as they require forming a new index for every shortlisting provided by low-dimensional MR.', chunk_index=15, num_tokens=71, metadata={'section_titles': [' Equal contribution.', 'We make the following key contributions:', '2 Related Work'], 'pages': [3]}), ResponseChunk(id='chunk_1df1d522-1ea6-443a-b2db-7ae0878d6ace', content='Ensuring that the method aligns well with the modern-day ANNS pipelines is important as they already have mechanisms to handle real-world constraints like load-balancing [16] and random access from disk [22].', chunk_index=16, num_tokens=42, metadata={'section_titles': [' Equal contribution.', 'We make the following key contributions:', '2 Related Work'], 'pages': [3]}), ResponseChunk(id='chunk_22438014-6bda-434a-966c-827c8a098c52', content='So, AdANNS is a step towards making the abstraction of adaptive search and retrieval feasible at the web-scale. Through extensive experimentation, we also show that AdANNS generalizes across search data structures, distance approximations, modalities (text & image), and encoders (CNNs & Transformers) while still translating the theoretical gains to latency reductions in deployment. While we have mainly focused on IVF and OPQ-based ANNS in this work, AdANNS also blends well with other ANNS pipelines. We also show that AdANNS can enable compute-aware elastic search on prebuilt indices without making any modifications (Section 5.1); note that this is in contrast to AdANNS-IVF that builds the index explicitly utilizing  adaptivity  in representations.', chunk_index=17, num_tokens=157, metadata={'section_titles': [' Equal contribution.', 'We make the following key contributions:', '2 Related Work'], 'pages': [3]}), ResponseChunk(id='chunk_55687a26-f480-4546-9ecf-23a28c80b776', content='Finally, we provide an extensive analysis on the alignment of matryoshka representation for better semantic search (Section 5.2). We make the following key contributions:', chunk_index=18, num_tokens=34, metadata={'section_titles': [' Equal contribution.', 'We make the following key contributions:', '2 Related Work'], 'pages': [3]}), ResponseChunk(id='chunk_d749e999-fc00-45d6-bc6e-1c3e684d4ac7', content='We introduce AdANNS , a novel framework for semantic search that leverages matryoshka representations for designing ANNS systems with better accuracy-compute trade-offs. AdANNS powered search data structure (AdANNS-IVF) and quantization (AdANNS-OPQ) show a significant improvement in accuracy-compute tradeoff compared to existing solutions. AdANNS generalizes to modern-day composite ANNS indices and can also enable compute-aware elastic search during inference with no modifications. 2 Related Work Approximate nearest neighbour search (ANNS) is a paradigm to come as close as possible [7] to retrieving the  true  nearest neighbor (NN) without the exorbitant search costs associated with exhaustive search [21, 52].', chunk_index=19, num_tokens=153, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_0b22b42c-7cd7-47da-9857-6fe3cd232c91', content='The  approximate  nature comes from data pruning as well as the cheaper distance computation that enable real-time web-scale search. In its naive form, NN-search has a complexity of O(dN ); d is the data dimensionality used for distance computation and N is the size of the database.', chunk_index=20, num_tokens=58, metadata={'section_titles': [' Equal contribution.', 'We make the following key contributions:', '2 Related Work'], 'pages': [3]}), ResponseChunk(id='chunk_3a1176b6-2d82-45da-8b16-f0cfc75e7e46', content='ANNS employs each of these approximations to reduce the linear dependence on the dimensionality (cheaper distance computation) and data points visited during search (data pruning).', chunk_index=21, num_tokens=33, metadata={'section_titles': [' Equal contribution.', 'We make the following key contributions:', '2 Related Work'], 'pages': [3]}), ResponseChunk(id='chunk_c4547589-607c-49e9-87b5-aeba06025e5a', content='Cheaper distance computation. From a bird s eye view, cheaper distance computation is always obtained through dimensionality reduction (quantization included). PCA and SVD [14, 26] can reduce dimensionality and preserve distances only to a limited extent without sacrificing accuracy.', chunk_index=22, num_tokens=53, metadata={'section_titles': [' Equal contribution.', 'We make the following key contributions:', '2 Related Work'], 'pages': [3]}), ResponseChunk(id='chunk_a30e9bef-13f4-49cf-b6f3-ce111657443a', content='On the other hand, quantization-based techniques [6, 15] like (optimized) product quantization ((O)PQ) [13, 23] have proved extremely crucial for relatively accurate yet cheap distance computation and simultaneously reduce the memory overhead significantly.', chunk_index=23, num_tokens=53, metadata={'section_titles': [' Equal contribution.', 'We make the following key contributions:', '2 Related Work'], 'pages': [3]}), ResponseChunk(id='chunk_f50e16fa-bcfc-40f6-83c2-e8dcb2d4f724', content='Another naive solution is to indepen- dently train the representation function with varying low-dimensional information bottlenecks [31] which is rarely used due to the costs of maintaining multiple models and databases.', chunk_index=24, num_tokens=40, metadata={'section_titles': [' Equal contribution.', 'We make the following key contributions:', '2 Related Work'], 'pages': [3]}), ResponseChunk(id='chunk_38df13f4-0feb-4f87-bff4-fed2f39e2510', content='Data pruning. Enabled by various data structures, data pruning reduces the number of data points visited as part of the search. This is often achieved through hashing [8, 46], trees [3, 12, 16, 48] and graphs [22, 38].', chunk_index=25, num_tokens=57, metadata={'section_titles': [' Equal contribution.', 'We make the following key contributions:', '2 Related Work'], 'pages': [3]}), ResponseChunk(id='chunk_277f7e5b-2444-4987-bd21-f7a7c0807692', content='More recently there have been efforts towards end-to-end learning of the search data structures [17, 29, 30]. However, web-scale ANNS indices are often constructed on rigid d-dimensional real vectors using the aforementioned data structures that assist with the real-time search.', chunk_index=26, num_tokens=54, metadata={'section_titles': [' Equal contribution.', 'We make the following key contributions:', '2 Related Work'], 'pages': [3]}), ResponseChunk(id='chunk_4765e64b-53f9-49af-8e8e-b7c7e35f91f3', content='For a more comprehensive review of ANNS structures please refer to [5, 34, 51]. Composite indices. ANNS pipelines often benefit from the complementary nature of various building blocks [24, 42].', chunk_index=27, num_tokens=43, metadata={'section_titles': [' Equal contribution.', 'We make the following key contributions:', '2 Related Work', '3 Problem Setup, Notation, and Preliminaries'], 'pages': [3, 4]}), ResponseChunk(id='chunk_574dc477-ed91-4c7f-9169-6af1452e1161', content='In practice, often the data structures (coarse-quantizer) like IVF [48] and HNSW [37] are combined with cheaper distance alternatives like PQ [23] (fine-quantizer) for massive speed-ups in web-scale search. While the data structures are built on d-dimensional real vectors, past works consistently show that PQ can be safely used for distance computation during search time. As evident in modern web-scale ANNS systems like DiskANN [22], the data structures are built on d-dimensional real vectors but work with PQ vectors (32   64-byte) for fast distance computations. ANNS benchmark datasets. Despite the Herculean advances in representation learning [19, 42], ANNS progress is often only benchmarked on fixed representation vectors provided for about a dozen million to billion scale datasets [1, 47] with limited access to the raw data.', chunk_index=28, num_tokens=180, metadata={'section_titles': ['2 Related Work', '3 Problem Setup, Notation, and Preliminaries'], 'pages': [4]}), ResponseChunk(id='chunk_3cbee9bf-7191-48b7-90c6-6f595bf36c05', content='This resulted in the improvement of algorithmic design for rigid representations (RRs) that are often not specifically designed for search. All the existing ANNS methods work with the assumption of using the provided d-dimensional representation which might not be Pareto-optimal for the accuracy-compute trade- off in the first place.', chunk_index=29, num_tokens=63, metadata={'section_titles': ['2 Related Work', '3 Problem Setup, Notation, and Preliminaries'], 'pages': [4]}), ResponseChunk(id='chunk_248ceb75-8c39-419a-bdab-cb829cc8af2b', content='Note that the lack of raw-image and text-based benchmarks led us to using ImageNet-1K [45] (1.3M images, 50K queries) and Natural Questions [32] (21M passages, 3.6K queries) for experimentation. While not billion-scale, the results observed on ImageNet often translate to real-world progress [28], and Natural Questions is one of the largest question answering datasets benchmarked for dense passage retrieval [27], making our results generalizable and widely applicable.', chunk_index=30, num_tokens=105, metadata={'section_titles': ['2 Related Work', '3 Problem Setup, Notation, and Preliminaries'], 'pages': [4]}), ResponseChunk(id='chunk_a5ef53d8-84e4-4c56-a7f8-43a58dd0db19', content='In this paper, we investigate the utility of adaptive representations   embeddings of different dimen- sionalities having similar semantic information   in improving the design of ANNS algorithms. This helps in transitioning out of restricted construction and inference on rigid representations for ANNS.', chunk_index=31, num_tokens=52, metadata={'section_titles': ['2 Related Work', '3 Problem Setup, Notation, and Preliminaries'], 'pages': [4]}), ResponseChunk(id='chunk_9cacb993-f0bd-4725-aa01-46bde7304dd0', content='To this end, we extensively use Matryoshka Representations (MRs) [31] which have desired adaptive properties in-built. To the best of our knowledge, this is the first work that improves accuracy-compute trade-off in ANNS by leveraging adaptive representations on different phases of construction and inference for ANNS data structures. 3 Problem Setup, Notation, and Preliminaries The problem setup of approximate nearest neighbor search (ANNS) [21] consists of a database of N data points, [x1, x2, . . . , xN ], and a query, q, where the goal is to  approximately  retrieve the nearest data point to the query.', chunk_index=32, num_tokens=140, metadata={'section_titles': ['2 Related Work', '3 Problem Setup, Notation, and Preliminaries'], 'pages': [4]}), ResponseChunk(id='chunk_c1fc6be3-22f1-451c-8392-45ae46a8dba4', content='Both the database and query are embedded to Rd using a representation function   : X   Rd, often a neural network that can be learned through various representation learning paradigms [2, 19, 20, 40, 42].', chunk_index=33, num_tokens=49, metadata={'section_titles': ['2 Related Work', '3 Problem Setup, Notation, and Preliminaries'], 'pages': [4]}), ResponseChunk(id='chunk_da3c845d-18d5-444d-a2b2-07585d11da18', content='Matryoshka Representations (MRs). The d-dimensional representations from   can have a nested structure like Matryoshka Representations (MRs) [31] in-built    MR(d). Matryoshka Representation Learning (MRL) learns these nested representations with a simple strategy of optimizing the same training objective at varying dimensionalities.', chunk_index=34, num_tokens=70, metadata={'section_titles': ['2 Related Work', '3 Problem Setup, Notation, and Preliminaries'], 'pages': [4]}), ResponseChunk(id='chunk_8f200be1-7f33-46b4-ae42-dbc7286adb2e', content='These granularities are ordered such that the lowest representation size forms a prefix for the higher-dimensional representations. So, high-dimensional MR inherently contains low-dimensional representations of varying granularities that can be accessed for free   first m-dimensions (m   [d]) ie.,  MR(d)[1 : m] from the d-dimensional MR form an m-dimensional representation which is as accurate as its independently trained rigid representation (RR) counterpart    RR(m).', chunk_index=35, num_tokens=92, metadata={'section_titles': ['2 Related Work', '3 Problem Setup, Notation, and Preliminaries'], 'pages': [4]}), ResponseChunk(id='chunk_baa0c38c-3914-42c5-94e2-5d67912b23f6', content='Training an encoder with MRL does not involve any overhead or hyperparameter tuning and works seamlessly across modalities, training objectives, and architectures. Inverted File Index (IVF). IVF [48] is an ANNS data structure used in web-scale search sys- tems [16] owing to its simplicity, minimal compute overhead, and high accuracy. IVF construction involves clustering (coarse quantization through k-means) [36] on d-dimensional representation that results in an inverted file list [53] of all the data points in each cluster.', chunk_index=36, num_tokens=113, metadata={'section_titles': ['2 Related Work', '3 Problem Setup, Notation, and Preliminaries'], 'pages': [4]}), ResponseChunk(id='chunk_a9f39238-27b4-40d1-ba1a-8072f4841c51', content='During search, d-dimensional query representation is assigned to the most relevant cluster (Ci; i   [k]) by finding the closest cen- troid ( i) using an appropriate distance metric (L2 or cosine). This is followed by an exhaustive linear search across all data points in the cluster which gives the closest NN (see Figure 5 in Appendix A for IVF overview). Lastly, IVF can scale to web-scale by utilizing a hierarchical IVF structure within each cluster [16]. Table 2 in Appendix A describes the retrieval formula for multiple variants of IVF.', chunk_index=37, num_tokens=116, metadata={'section_titles': ['2 Related Work', '3 Problem Setup, Notation, and Preliminaries'], 'pages': [4]}), ResponseChunk(id='chunk_8569c4f3-47bf-4df5-8bbf-7907ad1fd279', content='Optimized Product Quantization (OPQ). Product Quantization (PQ) [23] works by splitting a d-dimensional real vector into m sub-vectors and quantizing each sub-vector with an independent 2b length codebook across the database. After PQ, each d-dimensional vector can be represented by a compact m   b bit vector; we make each vector m bytes long by fixing b = 8. During search time, distance computation between the query vector and PQ database is extremely efficient with only m codebook lookups. The generality of PQ encompasses scalar/vector quantization [15, 36] as special cases. However, PQ can be further improved by rotating the d-dimensional space appropriately to maximize distance preservation after PQ. Optimized Product Quantization (OPQ) [13] achieves this by learning an orthonormal projection matrix R that rotates the d-dimensional space to be more amenable to PQ. OPQ shows consistent gains over PQ across a variety of ANNS tasks and has become the default choice in standard composite indices [22, 24].', chunk_index=38, num_tokens=216, metadata={'section_titles': ['2 Related Work', '3 Problem Setup, Notation, and Preliminaries', '4 AdANNS   Adaptive ANNS'], 'pages': [4, 5]}), ResponseChunk(id='chunk_3bd6459a-3e9a-43c1-a13c-3d5d2774e0af', content='Datasets. We evaluate the ANNS algorithms while changing the representations used for the search thus making it impossible to evaluate on the usual benchmarks [1].', chunk_index=39, num_tokens=30, metadata={'section_titles': ['3 Problem Setup, Notation, and Preliminaries', '4 AdANNS   Adaptive ANNS'], 'pages': [5]}), ResponseChunk(id='chunk_78148dda-8e01-4ac2-ab69-e308e597a81e', content='Hence we experiment with two public datasets: (a) ImageNet-1K [45] dataset on the task of image retrieval   where the goal is to retrieve images from a database (1.3M image train set) belonging to the same class as the query image (50K image validation set) and (b) Natural Questions (NQ) [32] dataset on the task of question answering through dense passage retrieval   where the goal is to retrieve the relevant passage from a database (21M Wikipedia passages) for a query (3.6K questions).', chunk_index=40, num_tokens=116, metadata={'section_titles': ['3 Problem Setup, Notation, and Preliminaries', '4 AdANNS   Adaptive ANNS'], 'pages': [5]}), ResponseChunk(id='chunk_c2099cb0-1248-4574-b0e1-7a00f4a833e5', content='Metrics Performance of ANNS is often measured using recall score [22], k-recall@N   recall of the exact NN across search complexities which denotes the recall of k  true  NN when N data points are retrieved.', chunk_index=41, num_tokens=45, metadata={'section_titles': ['3 Problem Setup, Notation, and Preliminaries', '4 AdANNS   Adaptive ANNS'], 'pages': [5]}), ResponseChunk(id='chunk_22b071ff-7c60-4349-9c6a-d9a97e268eec', content='However, the presence of labels allows us to compute 1-NN (top-1) accuracy. Top-1 accuracy is a harder and more fine-grained metric that correlates well with typical retrieval metrics like recall and mean average precision (mAP@k). Even though we report top-1 accuracy by default during experimentation, we discuss other metrics in Appendix C.', chunk_index=42, num_tokens=74, metadata={'section_titles': ['3 Problem Setup, Notation, and Preliminaries', '4 AdANNS   Adaptive ANNS'], 'pages': [5]}), ResponseChunk(id='chunk_f7a4b27f-5381-4e33-a145-2bbe36b9c595', content='Finally, we measure the compute overhead of ANNS using MFLOPS/query and also provide wall-clock times (see Appendix B.1). Encoders.', chunk_index=43, num_tokens=31, metadata={'section_titles': ['3 Problem Setup, Notation, and Preliminaries', '4 AdANNS   Adaptive ANNS'], 'pages': [5]}), ResponseChunk(id='chunk_6bd0e3ea-9d53-4f31-8733-519d3103ab28', content='For ImageNet, we encode both the database and query set using a ResNet50 ( I ) [19] trained on ImageNet-1K. For NQ, we encode both the passages in the database and the questions in the query set using a BERT-Base ( N ) [10] model fine-tuned on NQ for dense passage retrieval [27].', chunk_index=44, num_tokens=76, metadata={'section_titles': ['3 Problem Setup, Notation, and Preliminaries', '4 AdANNS   Adaptive ANNS'], 'pages': [5]}), ResponseChunk(id='chunk_f89354a4-b094-4afb-a826-b9f448b05a1f', content='We use the trained ResNet50 models with varying representation sizes (d = [8, 16, . . . , 2048]; default being 2048) as suggested by Kusupati et al. [31] alongside the MRL-ResNet50 models trained with MRL for the same dimensionalities. The RR and MR models are trained to ensure the supervised one-vs-all classification accuracy across all data dimensionalities is nearly the same   1-NN accuracy of 2048-d RR and MR models are 71.19% and 70.97% respectively on ImageNet-1K. Independently trained models,  RR(d) , output d = [8, 16 . . . , 2048] dimensional RRs while a single MRL-ResNet50 model,  MR(d) We also train BERT-Base models in a similar vein as the aforementioned ResNet50 models. The key difference is that we take a pre-trained BERT-Base model and fine-tune on NQ as suggested by Karpukhin et al. [27] with varying (5) representation sizes (bottlenecks) (d = [48, 96, . . . , 768]; default being 768) to obtain  RR(d) that creates RRs for the NQ dataset.', chunk_index=45, num_tokens=276, metadata={'section_titles': ['3 Problem Setup, Notation, and Preliminaries', '4 AdANNS   Adaptive ANNS'], 'pages': [5]}), ResponseChunk(id='chunk_857fcff4-e089-4e5a-8795-1e017b8d6532', content='To get the MRL-BERT- Base model, we fine-tune a pre-trained BERT-Base encoder on the NQ train dataset using the MRL objective with the same granularities as RRs to obtain  MR(d) which contains all five granularities. Akin to ResNet50 models, the RR and MR BERT-Base models on NQ are built to have similar 1-NN accuracy for 768-d of 52.2% and 51.5% respectively.', chunk_index=46, num_tokens=105, metadata={'section_titles': ['3 Problem Setup, Notation, and Preliminaries', '4 AdANNS   Adaptive ANNS'], 'pages': [5]}), ResponseChunk(id='chunk_68658fba-f92e-4652-bc4b-e62ddef721de', content='More implementation details can be found in Appendix B and additional experiment-specific information is provided at the appropriate places. 4 AdANNS   Adaptive ANNS In this section, we present our proposed AdANNS framework that exploits the inherent flexibility of matryoshka representations to improve the accuracy-compute trade-off for semantic search com- ponents. Standard ANNS pipeline can be split into two key components: (a) search data structure that indexes and stores data points, (b) query-point computation method that outputs (approximate) distance between a given query and data point.', chunk_index=47, num_tokens=116, metadata={'section_titles': ['3 Problem Setup, Notation, and Preliminaries', '4 AdANNS   Adaptive ANNS'], 'pages': [5]}), ResponseChunk(id='chunk_ef8b9a41-764a-42df-b4e8-b3947489658d', content='For example, standard IVFOPQ [24] method uses an IVF structure to index points on full-precision vectors and then relies on OPQ for more efficient distance computation between the query and the data points during the linear scan.', chunk_index=48, num_tokens=48, metadata={'section_titles': ['3 Problem Setup, Notation, and Preliminaries', '4 AdANNS   Adaptive ANNS'], 'pages': [5]}), ResponseChunk(id='chunk_e7c0730e-7ec5-48b8-b940-f70255d44c65', content='Below, we show that AdANNS can be applied to both the above-mentioned ANNS components and provides significant gains on the computation-accuracy tradeoff curve. In particular, we present AdANNS-IVF which is AdANNS version of the standard IVF index structure [48], and the closely related ScaNN structure [16].', chunk_index=49, num_tokens=71, metadata={'section_titles': ['4.1 AdANNS-IVF', '4 AdANNS   Adaptive ANNS'], 'pages': [6]}), ResponseChunk(id='chunk_a34b0fb9-571c-41b8-a5d0-babe78805e00', content='We also present AdANNS-OPQ which introduces representation adap- tivity in the OPQ, an industry-default quantization.', chunk_index=50, num_tokens=27, metadata={'section_titles': ['4.1 AdANNS-IVF', '4 AdANNS   Adaptive ANNS'], 'pages': [6]}), ResponseChunk(id='chunk_fd1828e7-1e98-4d17-b763-3e7bf65c9e69', content='Then, in Section 4.3 we further demonstrate the combination of the two techniques to get AdANNS-IVFOPQ   an AdANNS version of IVFOPQ [24]   and AdANNS-DiskANN, a similar variant of DiskANN [22]. Overall, our experiments show that AdANNS-IVF is significantly more accuracy-compute optimal compared to the IVF indices built on RRs and AdANNS-OPQ is as accurate as the OPQ on RRs while being significantly cheaper. 4.1 AdANNS-IVF Recall from Section 1 that IVF has a clustering and a linear scan phase, where both phase use same dimen- sional rigid representation. Now, AdANNS-IVF allows the clustering phase to use the first dc dimensions of the given matryoshka represen- tation (MR). Similarly, the linear scan within each cluster uses ds di- mensions, where again ds represents top ds coordinates from MR.', chunk_index=51, num_tokens=209, metadata={'section_titles': ['4.1 AdANNS-IVF', '4 AdANNS   Adaptive ANNS'], 'pages': [6]}), ResponseChunk(id='chunk_836f3bff-6f70-4c3b-9ddf-55d1d30e1c80', content='Note that setting dc = ds results in non- adaptive regular IVF. Intuitively, we would set dc   ds, so that instead of clustering with a high-dimensional representation, we can approximate it accurately with a low-dimensional em- bedding of size dc followed by a lin- ear scan with a higher ds-dimensional representation.', chunk_index=52, num_tokens=66, metadata={'section_titles': ['4.1 AdANNS-IVF', '4 AdANNS   Adaptive ANNS'], 'pages': [6]}), ResponseChunk(id='chunk_c0355980-503e-489e-a356-4d3c11bd9393', content='Intuitively, this helps in the smooth search of design space for state-of-the-art accuracy-compute trade-off. Furthermore, this can provide a precise operating point on accuracy-compute tradeoff curve which is critical in several practical settings.', chunk_index=53, num_tokens=48, metadata={'section_titles': ['4.1 AdANNS-IVF', '4 AdANNS   Adaptive ANNS'], 'pages': [6]}), ResponseChunk(id='chunk_d42c4149-cb7b-44c0-aba8-78667a555d2d', content='Our experiments on regular IVF with MRs and RRs (IVF-MR & IVF-RR) of varying dimensionali- ties and IVF configurations (# clusters, # probes) show that (Figure 2) matryoshka representations result in a significantly better accuracy-compute trade-off. We further studied and found that learned lower-dimensional representations offer better accuracy-compute trade-offs for IVF than higher- dimensional embeddings (see Appendix E for more results).', chunk_index=54, num_tokens=95, metadata={'section_titles': ['4.1 AdANNS-IVF', '4 AdANNS   Adaptive ANNS'], 'pages': [6]}), ResponseChunk(id='chunk_4c3d49c8-b924-44ff-9199-5eb48b7e16e2', content='AdANNS utilizes d-dimensional matryoshka representation to get accurate dc and ds dimensional vectors at no extra compute cost. The resulting AdANNS-IVF provides a much better accuracy- compute trade-off (Figure 2) on ImageNet-1K retrieval compared to IVF-MR, IVF-RR, and MG- IVF-RR   multi-granular IVF with rigid representations (akin to AdANNS without MR)   a strong baseline that uses dc and ds dimensional RRs.', chunk_index=55, num_tokens=105, metadata={'section_titles': ['4.1 AdANNS-IVF', '4 AdANNS   Adaptive ANNS'], 'pages': [6]}), ResponseChunk(id='chunk_06c94f2f-b806-43e7-8d16-3d97bdb05fe4', content='Finally, we exhaustively search the design space of IVF by varying dc, ds   [8, 16, . . . , 2048] and the number of clusters k   [8, 16, . . . , 2048].', chunk_index=56, num_tokens=53, metadata={'section_titles': ['4.1 AdANNS-IVF', '4 AdANNS   Adaptive ANNS'], 'pages': [6]}), ResponseChunk(id='chunk_5e2c1232-fe7e-44eb-9923-f9a1b972f2e4', content='Please see Appendix E for more details. For IVF experiments on the NQ dataset, please refer to Appendix G. Empirical results. Figure 2 shows that AdANNS-IVF outperforms the baselines across all accuracy-compute settings for ImageNet-1K retrieval. AdANNS-IVF results in 10  lower compute for the best accuracy of the extremely expensive MG-IVF-RR and non-adaptive IVF-MR. Specifi- cally, as shown in Figure 1a, AdANNS-IVF is up to 1.5% more accurate for the same compute and has up to 100  lesser FLOPS/query (90  real-world speed-up!) than the status quo ANNS on rigid representations (IVF-RR).', chunk_index=57, num_tokens=164, metadata={'section_titles': ['4.1 AdANNS-IVF', '4 AdANNS   Adaptive ANNS'], 'pages': [6]}), ResponseChunk(id='chunk_41024d4d-fc02-40f6-bc81-548c1b02bcde', content='We filter out points for the sake of presentation and encourage the reader to check out Figure 8 in Appendix E for an expansive plot of all the configurations searched.', chunk_index=58, num_tokens=32, metadata={'section_titles': ['4.1 AdANNS-IVF', '4 AdANNS   Adaptive ANNS'], 'pages': [6]}), ResponseChunk(id='chunk_2ebf68e2-e870-45fc-9f15-62ba606cc867', content='The advantage of AdANNS for construction of search structures is evident from the improvements in IVF (AdANNS-IVF) and can be easily extended to other ANNS structures like ScaNN [16] and HNSW [38].', chunk_index=59, num_tokens=51, metadata={'section_titles': ['4.1 AdANNS-IVF', '4.2 AdANNS-OPQ', '4 AdANNS   Adaptive ANNS'], 'pages': [6, 7]}), ResponseChunk(id='chunk_d5dd0188-7fb9-4795-a48b-0410e8f5eacd', content='For example, HNSW consists of multiple layers with graphs of NSW graphs [37] of increasing complexity. AdANNS can be adopted to HNSW, where the construction of each level can be powered by appropriate dimensionalities for an optimal accuracy-compute trade-off. In general, AdANNS provides fine-grained control over compute overhead (storage, working memory, inference, and construction cost) during construction and inference while providing the best possible accuracy. 4.2 AdANNS-OPQ Standard Product Quantization (PQ) essentially performs block-wise vector quantization via cluster- ing.', chunk_index=60, num_tokens=122, metadata={'section_titles': ['4.1 AdANNS-IVF', '4.2 AdANNS-OPQ'], 'pages': [7]}), ResponseChunk(id='chunk_50feba61-4a6a-419b-bb21-9297474bcc7f', content='For example, suppose we need 32-byte PQ compressed vectors from the given 2048 dimensional representations. Then, we can chunk the representations in m = 32 equal blocks/sub-vectors of 64-d each, and each sub-vector space is clustered into 28 = 256 partitions. That is, the representation of each point is essentially cluster-id for each block.', chunk_index=61, num_tokens=75, metadata={'section_titles': ['4.1 AdANNS-IVF', '4.2 AdANNS-OPQ'], 'pages': [7]}), ResponseChunk(id='chunk_c4c0cae0-fc9d-48f4-92ee-a33b334c31c1', content='Optimized PQ (OPQ) [13] further refines this idea, by first rotating the representations using a learned orthogonal matrix, and then applying PQ on top of the rotated representations.', chunk_index=62, num_tokens=38, metadata={'section_titles': ['4.1 AdANNS-IVF', '4.2 AdANNS-OPQ'], 'pages': [7]}), ResponseChunk(id='chunk_de1cd134-342b-4b50-b4c9-245201e5824a', content='In ANNS, OPQ is used extensively to compress vectors and improves approximate distance computation primarily due to significantly lower memory overhead than storing full-precision data points IVF. AdANNS-OPQ utilizes MR representations to apply OPQ on lower-dimensional representations. That is, for a given quantization budget, AdANNS allows using top ds   d dimensions from MR and then computing clusters with ds/m-dimensional blocks where m is the number of blocks. Depending on ds and m, we have further flexibility of trading-off dimensionality/capacity for increasing the number of clusters to meet the given quantization budget. AdANNS-OPQ tries multiple ds, m, and number of clusters for a fixed quantization budget to obtain the best performing configuration.', chunk_index=63, num_tokens=152, metadata={'section_titles': ['4.1 AdANNS-IVF', '4.2 AdANNS-OPQ'], 'pages': [7]}), ResponseChunk(id='chunk_60c8e718-0410-4ab7-b3fc-f52638e7d1c0', content='We experimented with 8   128 byte OPQ budgets for both ImageNet and Natural Questions retrieval with an exhaustive search on the quantized vectors.', chunk_index=64, num_tokens=30, metadata={'section_titles': ['4.1 AdANNS-IVF', '4.2 AdANNS-OPQ'], 'pages': [7]}), ResponseChunk(id='chunk_04faf3e3-f9d0-4ca9-9201-39c3bc3e05f7', content='We compare AdANNS-OPQ which uses MRs of varying granularities to the baseline OPQ built on the highest dimensional RRs. We also evaluate OPQ vectors obtained projection using SVD [14] on top of the highest-dimensional RRs.', chunk_index=65, num_tokens=53, metadata={'section_titles': ['4.1 AdANNS-IVF', '4.2 AdANNS-OPQ'], 'pages': [7]}), ResponseChunk(id='chunk_f49ba4b4-9f82-4584-89b5-3c1d4f666200', content='Empirical results. Figures 3 and 1b show that AdANNS-OPQ significantly outperforms   up to 4% accuracy gain   the baselines (OPQ on RRs) across compute budgets on both ImageNet and NQ. In particular, AdANNS-OPQ tends to match the accuracy of a 64-byte (a typical choice in ANNS) OPQ baseline with only a 32-byte budget.', chunk_index=66, num_tokens=91, metadata={'section_titles': ['4.1 AdANNS-IVF', '4.2 AdANNS-OPQ'], 'pages': [7]}), ResponseChunk(id='chunk_f2d05b89-b255-41b3-bd11-871920a30fd1', content='This results in a 2  reduction in both storage and compute FLOPS which translates to significant gains in real-world web-scale deployment (see Appendix D).', chunk_index=67, num_tokens=32, metadata={'section_titles': ['4.1 AdANNS-IVF', '4.2 AdANNS-OPQ'], 'pages': [7]}), ResponseChunk(id='chunk_498fbdca-b821-407b-a97f-392f0b12e0e8', content='We only report the best AdANNS-OPQ for each budget typically obtained through a much lower- dimensional MR (128 & 192; much faster to build as well) than the highest-dimensional MR (2048 & 768) for ImageNet and NQ respectively (see Appendix G for more details). At the same time, we note that building compressed OPQ vectors on projected RRs using SVD to the smaller dimensions (or using low-dimensional RRs, see Appendix D) as the optimal AdANNS-OPQ does not help in improving the accuracy. The significant gains we observe in AdANNS-OPQ are purely due to better information packing in MRs   we hypothesize that packing the most important information in the initial coordinates results in a better PQ quantization than RRs where the information is uniformly distributed across all the dimensions [31, 49]. See Appendix D for more details and experiments. 4.3 AdANNS for Composite Indices We now extend AdANNS to composite indices [24] which put together two main ANNS building blocks   search structures and quantization   together to obtain efficient web-scale ANNS indices used in practice.', chunk_index=68, num_tokens=238, metadata={'section_titles': ['4.1 AdANNS-IVF', '5 Further Analysis and Discussion', '5.1 Compute-aware Elastic Search During Inference', '4.3 AdANNS for Composite Indices', '4.2 AdANNS-OPQ'], 'pages': [7, 8]}), ResponseChunk(id='chunk_dd5bfd5e-afb4-4519-b51b-0585526cea3d', content='A simple instantiation of a composite index would be the combination of IVF and OPQ   IVFOPQ   where the clustering in IVF happens with full-precision real vectors but the linear scan within each cluster is approximated using OPQ-compressed variants of the representation   since often the full-precision vectors of the database cannot fit in RAM.', chunk_index=69, num_tokens=71, metadata={'section_titles': ['5 Further Analysis and Discussion', '4.3 AdANNS for Composite Indices', '4.2 AdANNS-OPQ', '5.1 Compute-aware Elastic Search During Inference'], 'pages': [8]}), ResponseChunk(id='chunk_fd18a327-6348-45c5-a5b8-1dac636cf9ef', content='Contemporary ANNS indices like DiskANN [22] make this a default choice where they build the search graph with a full-precision vector and approximate the distance computations during search with an OPQ-compressed vector to obtain a very small shortlist of retrieved datapoints. In DiskANN, the shortlist of data points is then re-ranked to form the final list using their full-precision vectors fetched from the disk. AdANNS is naturally suited to this shortlist-rerank framework: we use a low-d MR for forming index, where we could tune AdANNS parameters according to the accuracy-compute trade-off of the graph and OPQ vectors. We then use a high-d MR for re-ranking.', chunk_index=70, num_tokens=144, metadata={'section_titles': ['5 Further Analysis and Discussion', '4.3 AdANNS for Composite Indices', '4.2 AdANNS-OPQ', '5.1 Compute-aware Elastic Search During Inference'], 'pages': [8]}), ResponseChunk(id='chunk_e696398a-8a3d-40a0-b0ac-aabaae27ca34', content='Empirical results. Figure 4 shows that AdANNS-IVFOPQ is 1   4% better than the baseline at all the PQ compute budgets. Furthermore, AdANNS-IVFOPQ has the same ac- curacy as the baselines at 8  lower overhead.', chunk_index=71, num_tokens=63, metadata={'section_titles': ['5 Further Analysis and Discussion', '4.3 AdANNS for Composite Indices', '4.2 AdANNS-OPQ', '5.1 Compute-aware Elastic Search During Inference'], 'pages': [8]}), ResponseChunk(id='chunk_7519c428-0c52-44af-bbd2-7f261b97a9e6', content='With DiskANN, AdANNS accelerates shortlist generation by us- ing low-dimensional representations and recoups the accuracy by re- ranking with the highest-dimensional MR at negligible cost. Table 1 shows that AdANNS-DiskANN is more accurate than the baseline for both 1-NN and ranking performance at only half the cost. Using low-dimensional representations further speeds up inference in AdANNS-DiskANN (see Appendix F). These results show the generality of AdANNS and its broad applicability across a variety of ANNS indices built on top of the base building blocks. Currently, AdANNS piggybacks on typical ANNS pipelines for their inherent accounting of the real-world system constraints [16, 22, 25]. However, we believe that AdANNS s flexibility and significantly better accuracy-compute trade-off can be further informed by real-world deployment constraints. We leave this high-potential line of work that requires extensive study to future research. 5 Further Analysis and Discussion 5.1 Compute-aware Elastic Search During Inference AdANNS search structures cater to many specific large-scale use scenarios that need to satisfy precise resource constraints during construction as well as inference.', chunk_index=72, num_tokens=243, metadata={'section_titles': ['5 Further Analysis and Discussion', '4.3 AdANNS for Composite Indices', '4.2 AdANNS-OPQ', '5.1 Compute-aware Elastic Search During Inference'], 'pages': [8]}), ResponseChunk(id='chunk_9f4ffb23-3f8f-49cd-9350-44a663eca71f', content='However, in many cases, construction and storage of the indices are not the bottlenecks or the user is unable to search the design space. In these settings, AdANNS-D enables adaptive inference through accurate yet cheaper distance computation using the low-dimensional prefix of matryoshka representation. Akin to composite indices (Section 4.3) that use PQ vectors for cheaper distance computation, we can use the low- dimensional MR for faster distance computation on ANNS structure built non-adaptively with a high-dimensional MR without any modifications to the existing index.', chunk_index=73, num_tokens=113, metadata={'section_titles': ['5 Further Analysis and Discussion', '4.3 AdANNS for Composite Indices', '4.2 AdANNS-OPQ', '5.1 Compute-aware Elastic Search During Inference'], 'pages': [8]}), ResponseChunk(id='chunk_10d78478-4e7c-4b01-a394-409185f2c524', content='Empirical results. Figure 2 shows that for a given compute budget using IVF on ImageNet-1K retrieval, AdANNS-IVF is better than AdANNS-IVF-D due to the explicit control during the building of the ANNS structure which is expected.', chunk_index=74, num_tokens=58, metadata={'section_titles': ['5.3 Search for AdANNS Hyperparameters', '5 Further Analysis and Discussion', '5.2 Why MRs over RRs?', '5.1 Compute-aware Elastic Search During Inference', '4.3 AdANNS for Composite Indices', '4.2 AdANNS-OPQ'], 'pages': [8, 9]}), ResponseChunk(id='chunk_97a781c9-50d9-4699-8efe-5ab6653cdf7e', content='However, the interesting observation is that AdANNS-D matches or outperforms the IVF indices built with MRs of varying capacities for ImageNet retrieval.', chunk_index=75, num_tokens=32, metadata={'section_titles': ['5.1 Compute-aware Elastic Search During Inference', '5.2 Why MRs over RRs?', '5.3 Search for AdANNS Hyperparameters'], 'pages': [9]}), ResponseChunk(id='chunk_f67d950f-3453-4b2e-9e34-1e0145dbf8e9', content='However, these methods are applicable in specific scenarios of deployment. Obtaining optimal AdANNS search structure (highly accurate) or even the best IVF-MR index relies on a relatively expensive design search but delivers indices that fit the storage, memory, compute, and accuracy constraints all at once. On the other hand AdANNS-D does not require a precisely built ANNS index but can enable compute-aware search during inference. AdANNS-D is a great choice for setups that can afford only one single database/index but need to cater to varying deployment constraints, e.g., one task requires 70% accuracy while another task has a compute budget of 1 MFLOPS/query. 5.2 Why MRs over RRs? Quite a few of the gains from AdANNS are owing to the quality and capabilities of matryoshka representations. So, we conducted extensive analysis to understand why matryoshka representations seem to be more aligned for semantic search than the status-quo rigid representations.', chunk_index=76, num_tokens=203, metadata={'section_titles': ['5.1 Compute-aware Elastic Search During Inference', '5.2 Why MRs over RRs?', '5.3 Search for AdANNS Hyperparameters'], 'pages': [9]}), ResponseChunk(id='chunk_7b4975e1-9949-41e0-a899-122ef34b4792', content='Difficulty of NN search. Relative contrast (Cr) [18] is inversely proportional to the difficulty of nearest neighbor search on a given database. On ImageNet-1K, Figure 14 shows that MRs have better Cr than RRs across dimensionalities, further supporting that matryoshka representations are more aligned (easier) for NN search than existing rigid representations for the same accuracy.', chunk_index=77, num_tokens=80, metadata={'section_titles': ['5.1 Compute-aware Elastic Search During Inference', '5.2 Why MRs over RRs?', '5.3 Search for AdANNS Hyperparameters'], 'pages': [9]}), ResponseChunk(id='chunk_4ac805fa-8298-44af-ac4e-b1290c1a5f10', content='More details and analysis about this experiment can be found in Appendix H.2. Clustering distributions. We also investigate the potential deviation in clustering distributions for MRs across dimensionalities compared to RRs. Unlike the RRs where the information is uniformly diffused across dimensions [49], MRs have hierarchical information packing. Figure 11 in Appendix E.3 shows that matryoshka representations result in clusters similar (measured by total variation distance [33]) to that of rigid representations and do not result in any unusual artifacts.', chunk_index=78, num_tokens=106, metadata={'section_titles': ['5.1 Compute-aware Elastic Search During Inference', '5.2 Why MRs over RRs?', '5.3 Search for AdANNS Hyperparameters'], 'pages': [9]}), ResponseChunk(id='chunk_c07460fe-534f-4595-a6ca-850c5dcf9ba9', content='Robustness. Figure 9 in Appendix E shows that MRs continue to be better than RRs even for out-of- distribution (OOD) image queries (ImageNetV2 [44]) using ANNS.', chunk_index=79, num_tokens=44, metadata={'section_titles': ['5.1 Compute-aware Elastic Search During Inference', '5.2 Why MRs over RRs?', '5.3 Search for AdANNS Hyperparameters'], 'pages': [9]}), ResponseChunk(id='chunk_bb8d9a9a-fe06-467a-baad-b782e56c577c', content='It also shows that the highest data dimensionality need not always be the most robust which is further supported by the higher recall using lower dimensions. Further details about this experiment can be found in Appendix E.1.', chunk_index=80, num_tokens=42, metadata={'section_titles': ['5.1 Compute-aware Elastic Search During Inference', '5.2 Why MRs over RRs?', '5.3 Search for AdANNS Hyperparameters'], 'pages': [9]}), ResponseChunk(id='chunk_2f868c93-6e33-4f18-99fd-c35c135721cd', content='Generality across encoders. IVF-MR consistently has higher accuracy than IVF-RR across dimen- sionalities despite having similar accuracies with exact NN search (for ResNet50 on ImageNet and BERT-Base on NQ). We find that our observations on better alignment of MRs for NN search hold across neural network architectures, ResNet18/34/101 [19] and ConvNeXt-Tiny [35]. Appendix H.3 delves deep into the experimentation done using various neural architectures on ImageNet-1K.', chunk_index=81, num_tokens=114, metadata={'section_titles': ['5.1 Compute-aware Elastic Search During Inference', '5.2 Why MRs over RRs?', '5.3 Search for AdANNS Hyperparameters'], 'pages': [9]}), ResponseChunk(id='chunk_d940d515-9d2b-4db9-9b6d-3cc06205be1d', content='Recall score analysis. Analysis of recall score (see Appendix C) in Appendix H.1 shows that for a similar top-1 accuracy, lower-dimensional representations have better 1-Recall@1 across search complexities for IVF and HNSW on ImageNet-1K. Across the board, MRs have higher recall scores and top-1 accuracy pointing to easier  searchability  and thus suitability of matryoshka representations for ANNS.', chunk_index=82, num_tokens=93, metadata={'section_titles': ['5.1 Compute-aware Elastic Search During Inference', '5.2 Why MRs over RRs?', '5.3 Search for AdANNS Hyperparameters'], 'pages': [9]}), ResponseChunk(id='chunk_1f5d63fa-b825-41f3-902c-4f74768bdeff', content='Larger-scale experiments and further analysis can be found in Appendix H. Through these analyses, we argue that matryoshka representations are better suited for semantic search than rigid representations, thus making them an ideal choice for AdANNS. 5.3 Search for AdANNS Hyperparameters Choosing the optimal hyperparameters for AdANNS, such as dc, ds, m, # clusters, # probes, is an interesting and open problem that requires more rigorous examination. As the ANNS index is formed once and used for potentially billions of queries with massive implications for cost, latency and queries-per-second, a hyperparameter search for the best index is generally an acceptable industry practice [22, 38]. The Faiss library [24] provides guidelines2 to choose the appropriate index for a specific problem, including memory constraints, database size, and the need for exact results. There have been efforts at automating the search for optimal indexing parameters, such as Autofaiss3, which maximizes recall given compute constraints. 2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index 3https://github.com/criteo/autofaiss In case of AdANNS, we suggest starting at the best configurations of MRs followed by a local design space search to lead to near-optimal AdANNS configurations (e.g. use IVF-MR to bootstrap AdANNS-IVF).', chunk_index=83, num_tokens=289, metadata={'section_titles': ['5.3 Search for AdANNS Hyperparameters', '6 Conclusions', '5.2 Why MRs over RRs?', '5.1 Compute-aware Elastic Search During Inference', '5.4 Limitations', 'Acknowledgments'], 'pages': [9, 10]}), ResponseChunk(id='chunk_974832ba-78ed-48d1-bd77-6a6a7d7f869b', content='We also share some observations during the course of our experiments: 1. AdANNS-IVF: Top-1 accuracy generally improves (with diminishing returns after a point) with increasing dimensionality of clustering (d,.) and search (d,), as we show on ImageNet variants and with multiple encoders in the Appendix (Figures 9 and 15). Clustering with low-d MRs matches the performance of high-d MRs as they likely contain similar amounts of useful information, making the increased compute cost not worth the marginal gains.', chunk_index=84, num_tokens=111, metadata={'section_titles': ['5.3 Search for AdANNS Hyperparameters', 'Acknowledgments', '6 Conclusions', '5.4 Limitations'], 'pages': [10]}), ResponseChunk(id='chunk_17c1cd7f-4c82-43b3-8134-ba885e7cfe3e', content='Increasing # probes naturally boosts performance (Appendix, Figure 10a). Lastly, it is generally accepted that a good starting point for the # clusters k is V/Np/2, where Np is the number of indexable items [39]. k = Np is the optimal choice of k from a FLOPS computation perspective as can be seen in Appendix B.1. 2. AdANNS-OPQ: we observe that for a fixed compute budget in bytes (m), the top-1 accuracy reaches a peak at d < dmax (Appendix, Table 4). We hypothesize that the better performance of AdANNS-OPQ at d < dmax is due to the curse of dimensionality, i.e. it is easier to learn PQ codebooks on smaller embeddings with similar amounts of information. We find that using an MR with d = 4   m is a good starting point on ImageNet and NQ.', chunk_index=85, num_tokens=195, metadata={'section_titles': ['5.3 Search for AdANNS Hyperparameters', 'Acknowledgments', '6 Conclusions', '5.4 Limitations'], 'pages': [10]}), ResponseChunk(id='chunk_2fd69793-60ac-4b34-8e05-ac73ba934659', content='We also suggest using an 8-bit (256-length) codebook for OPQ as the default for each of the sub-block quantizer. 3.', chunk_index=86, num_tokens=32, metadata={'section_titles': ['5.3 Search for AdANNS Hyperparameters', 'Acknowledgments', '6 Conclusions', '5.4 Limitations'], 'pages': [10]}), ResponseChunk(id='chunk_efe1d5e9-88f0-4f4e-a2c9-a8a39366a984', content='AdANNS-DiskANN: Our observations with DiskANN are consistent with other indexing struc- tures, i.e. the optimal graph construction dimensionality d < dmax (Appendix, Figure 12). A careful study of DiskANN on different datasets is required for more general guidelines to choose graph construction and OPQ dimensionality d. 5.4 Limitations AdANNS s core focus is to improve the design of the existing ANNS pipelines. To use AdANNS on a corpus, we need to back-fill [43] the MRs of the data   a significant yet a one-time overhead. We also notice that high-dimensional MRs start to degrade in performance when optimizing also for an extremely low-dimensional granularity (e.g., < 24-d for NQ)   otherwise is it quite easy to have comparable accuracies with both RRs and MRs. Lastly, the existing dense representations can only in theory be converted to MRs with an auto-encoder-style non-linear transformation.', chunk_index=87, num_tokens=204, metadata={'section_titles': ['5.3 Search for AdANNS Hyperparameters', 'Acknowledgments', '6 Conclusions', '5.4 Limitations'], 'pages': [10]}), ResponseChunk(id='chunk_ff10d255-df54-4670-b4d9-8c8294954107', content='We believe most of these limitations form excellent future work to improve AdANNS further. 6 Conclusions We proposed a novel framework, AdANNS , that leverages adaptive representations for different phases of ANNS pipelines to improve the accuracy-compute tradeoff. AdANNS utilizes the inherent flexibility of matryoshka representations [31] to design better ANNS building blocks than the standard ones which use the rigid representation in each phase. AdANNS achieves SOTA accuracy-compute trade-off for the two main ANNS building blocks: search data structures (AdANNS-IVF) and quantization (AdANNS-OPQ). The combination of AdANNS-based building blocks leads to the construction of better real-world composite ANNS indices   with as much as 8  reduction in cost at the same accuracy as strong baselines   while also enabling compute-aware elastic search. Finally, we note that combining AdANNS with elastic encoders [11] enables truly adaptive large-scale retrieval.', chunk_index=88, num_tokens=202, metadata={'section_titles': ['5.3 Search for AdANNS Hyperparameters', 'Acknowledgments', '6 Conclusions', '5.4 Limitations'], 'pages': [10]}), ResponseChunk(id='chunk_982306a9-f3a2-4cac-be11-20970f52ea6b', content='Acknowledgments We are grateful to Kaifeng Chen, Venkata Sailesh Sanampudi, Sanjiv Kumar, Harsha Vardhan Simhadri, Gantavya Bhatt, Matthijs Douze and Matthew Wallingford for helpful discussions and feedback. Aditya Kusupati also thanks Tom Duerig and Rahul Sukthankar for their support.', chunk_index=89, num_tokens=80, metadata={'section_titles': ['5.3 Search for AdANNS Hyperparameters', 'Acknowledgments', '6 Conclusions', '5.4 Limitations'], 'pages': [10]}), ResponseChunk(id='chunk_fdbfc7a6-84df-4865-8c43-b7e61902744d', content='Part of the paper s large-scale experimentation is supported through a research GCP credit award from Google Cloud and Google Research. Sham Kakade acknowledges funding from the ONR award N00014-22-1-2377 and NSF award CCF-2212841. Ali Farhadi acknowledges funding from the NSF awards IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543, and gifts from Allen Institute for Artificial Intelligence and Google.', chunk_index=90, num_tokens=115, metadata={'section_titles': ['5.3 Search for AdANNS Hyperparameters', 'Acknowledgments', '6 Conclusions', '5.4 Limitations'], 'pages': [10]}), ResponseChunk(id='chunk_dbfa0512-3b44-4a91-bbd6-c880250562d6', content='References [1] M. Aum ller, E. Bernhardsson, and A. Faithfull. Ann-benchmarks: A benchmarking tool for approximate nearest neighbor algorithms.', chunk_index=91, num_tokens=38, metadata={'section_titles': ['References'], 'pages': [11]}), ResponseChunk(id='chunk_a288870e-87c9-4c07-9fd3-30326a909b27', content='Information Systems, 87:101374, 2020. [2] Y.', chunk_index=92, num_tokens=18, metadata={'section_titles': ['References'], 'pages': [11]}), ResponseChunk(id='chunk_6a919ab4-91b7-498e-a473-9ef601696fe7', content='Bengio. Deep learning of representations for unsupervised and transfer learning. In Proceed- ings of ICML workshop on unsupervised and transfer learning, pages 17 36. JMLR Workshop and Conference Proceedings, 2012. [3] E. Bernhardsson.', chunk_index=93, num_tokens=60, metadata={'section_titles': ['References'], 'pages': [11]}), ResponseChunk(id='chunk_dfaef07d-82a2-4d53-9744-fafd50a9707c', content='Annoy: Approximate Nearest Neighbors in C++/Python, 2018. URL https://pypi.org/project/annoy/. Python package version 1.13.0. [4] S.', chunk_index=94, num_tokens=46, metadata={'section_titles': ['References'], 'pages': [11]}), ResponseChunk(id='chunk_13f59b7c-2eeb-4690-a22d-f583b5c1655b', content='Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. Computer networks and ISDN systems, 30(1-7):107 117, 1998. IEEE Transactions on Knowledge and Data Engineering, 33(6):2337 2348, 2021. doi: 10.1109/ TKDE.2019.2953897. [6] T. Chen, L. Li, and Y. Sun.', chunk_index=95, num_tokens=99, metadata={'section_titles': ['References'], 'pages': [11]}), ResponseChunk(id='chunk_291830c6-d782-4770-a1cc-8ecfbab78796', content='Differentiable product quantization for end-to-end embedding compression. In International Conference on Machine Learning, pages 1617 1626. PMLR, 2020. [7] K. L. Clarkson.', chunk_index=96, num_tokens=44, metadata={'section_titles': ['References'], 'pages': [11]}), ResponseChunk(id='chunk_fb428bb1-19d2-4888-a409-c7f629e49e95', content='An algorithm for approximate closest-point queries. In Proceedings of the tenth annual symposium on Computational geometry, pages 160 164, 1994. [8] M. Datar, N. Immorlica, P. Indyk, and V. S.', chunk_index=97, num_tokens=54, metadata={'section_titles': ['References'], 'pages': [11]}), ResponseChunk(id='chunk_d77be52e-0746-414f-86d9-37e819ec4863', content='Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational geometry, pages 253 262, 2004. [9] J.', chunk_index=98, num_tokens=43, metadata={'section_titles': ['References'], 'pages': [11]}), ResponseChunk(id='chunk_8cde28c5-08ed-4c89-b94e-18b125083fdb', content='Dean. Challenges in building large-scale information retrieval systems. In Keynote of the 2nd ACM International Conference on Web Search and Data Mining (WSDM), volume 10, 2009. [10] J. Devlin, M.-W. Chang, K. Lee, and K.', chunk_index=99, num_tokens=62, metadata={'section_titles': ['References'], 'pages': [11]}), ResponseChunk(id='chunk_c67e5303-505d-442c-a304-4a0d28bb2692', content='Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [11] Devvrit, S.', chunk_index=100, num_tokens=45, metadata={'section_titles': ['References'], 'pages': [11]}), ResponseChunk(id='chunk_db713afc-5daf-4a4d-a6ac-06d424d4cf76', content='Kudugunta, A. Kusupati, T. Dettmers, K. Chen, I. Dhillon, Y. Tsvetkov, H. Hannaneh, S. Kakade, A. Farhadi, and P. Jain.', chunk_index=101, num_tokens=57, metadata={'section_titles': ['References'], 'pages': [11]}), ResponseChunk(id='chunk_f1e10c27-7cd9-45be-9094-8725cd90d4ed', content='Matformer: Nested transformer for elastic inference. arXiv preprint arxiv:2310.07707, 2023. [12] J.', chunk_index=102, num_tokens=33, metadata={'section_titles': ['References'], 'pages': [11]}), ResponseChunk(id='chunk_33160ca4-43be-439d-be98-49e449b7a871', content='H. Friedman, J. L. Bentley, and R. A. Finkel. An algorithm for finding best matches in logarithmic expected time. ACM Transactions on Mathematical Software (TOMS), 3(3):209  226, 1977. [13] T. Ge, K. He, Q. Ke, and J. Sun.', chunk_index=103, num_tokens=74, metadata={'section_titles': ['References'], 'pages': [11]}), ResponseChunk(id='chunk_84216922-ea69-40e2-b2fb-90ac492032ca', content='Optimized product quantization for approximate nearest neighbor search. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2946 2953, 2013. [14] G. Golub and W.', chunk_index=104, num_tokens=46, metadata={'section_titles': ['References'], 'pages': [11]}), ResponseChunk(id='chunk_bbf20c20-a7cf-4b4c-898b-d222345ad81b', content='Kahan. Calculating the singular values and pseudo-inverse of a matrix. Journal of the Society for Industrial and Applied Mathematics, Series B: Numerical Analysis, 2(2): 205 224, 1965. [15] R.', chunk_index=105, num_tokens=51, metadata={'section_titles': ['References'], 'pages': [11]}), ResponseChunk(id='chunk_73412197-ba2b-4e64-bd62-d8d98e6a378d', content='Gray. Vector quantization. IEEE Assp Magazine, 1(2):4 29, 1984. [16] R. Guo, P. Sun, E.', chunk_index=106, num_tokens=38, metadata={'section_titles': ['References'], 'pages': [11]}), ResponseChunk(id='chunk_2b727fbb-3c3c-4a9a-8a40-7a99203d6761', content='Lindgren, Q. Geng, D. Simcha, F. Chern, and S. Kumar. Accelerating large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning, pages 3887 3896. PMLR, 2020. [17] N. Gupta, P. H. Chen, H.-F. Yu, C.-J. Hsieh, and I. S. Dhillon.', chunk_index=107, num_tokens=95, metadata={'section_titles': ['References'], 'pages': [11]}), ResponseChunk(id='chunk_d20574b2-d45b-4541-b342-2c30f40f6f83', content='End-to-end learning to index and search in large output spaces. arXiv preprint arXiv:2210.08410, 2022. [18] J.', chunk_index=108, num_tokens=37, metadata={'section_titles': ['References'], 'pages': [11]}), ResponseChunk(id='chunk_f2b919e1-f3bd-4aac-bbc5-e46fb247b150', content='He, S. Kumar, and S.-F. Chang. On the difficulty of nearest neighbor search. In International Conference on Machine Learning (ICML), 2012. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770  778, 2016. [20] K.', chunk_index=109, num_tokens=64, metadata={'section_titles': ['References'], 'pages': [11, 12]}), ResponseChunk(id='chunk_81b27ea9-6ed6-48c6-ae18-a65ab520328f', content='He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729 9738, 2020. [21] P.', chunk_index=110, num_tokens=64, metadata={'section_titles': ['References'], 'pages': [12]}), ResponseChunk(id='chunk_0306b747-638c-4cfc-81c3-3c546f8c91d8', content='Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 604 613, 1998. [22] S.', chunk_index=111, num_tokens=54, metadata={'section_titles': ['References'], 'pages': [12]}), ResponseChunk(id='chunk_ac1d6c53-09ee-41f6-9d85-2a2d89b290fa', content='Jayaram Subramanya, F. Devvrit, H. V. Simhadri, R. Krishnawamy, and R. Kadekodi. Diskann:', chunk_index=112, num_tokens=38, metadata={'section_titles': ['References'], 'pages': [12]}), ResponseChunk(id='chunk_9a822ee2-baf8-4d65-b6b9-73fbf01f827d', content='Fast accurate billion-point nearest neighbor search on a single node. Advances in Neural Information Processing Systems, 32, 2019. [23] H. Jegou, M. Douze, and C. Schmid.', chunk_index=113, num_tokens=45, metadata={'section_titles': ['References'], 'pages': [12]}), ResponseChunk(id='chunk_791255ae-b82c-42e0-bce8-137ccead40e6', content='Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117 128, 2010. [24] J. Johnson, M. Douze, and H. J gou.', chunk_index=114, num_tokens=48, metadata={'section_titles': ['References'], 'pages': [12]}), ResponseChunk(id='chunk_50b8051e-4cf8-4ddb-8319-fe1720953cfb', content='Billion-scale similarity search with GPUs. Transactions on Big Data, 7(3):535 547, 2019.', chunk_index=115, num_tokens=26, metadata={'section_titles': ['References'], 'pages': [12]}), ResponseChunk(id='chunk_225ef5e9-6e60-4670-a865-4c347a8eab76', content='IEEE [25] W. B. Johnson. Extensions of lipschitz mappings into a hilbert space. Contemp. Math., 26: 189 206, 1984. [26] I. T. Jolliffe and J.', chunk_index=116, num_tokens=52, metadata={'section_titles': ['References'], 'pages': [12]}), ResponseChunk(id='chunk_817e8b59-f82c-469e-908c-b78a7b83c5bb', content='Cadima. Principal component analysis: a review and recent developments. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065):20150202, 2016. [27] V. Karpukhin, B.', chunk_index=117, num_tokens=55, metadata={'section_titles': ['References'], 'pages': [12]}), ResponseChunk(id='chunk_e198d13b-04ed-46b1-a486-8e403b91e1e8', content='O guz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih.', chunk_index=118, num_tokens=34, metadata={'section_titles': ['References'], 'pages': [12]}), ResponseChunk(id='chunk_ae755e38-095b-4cfe-8b62-6da5f559bf95', content='Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. [28] S.', chunk_index=119, num_tokens=34, metadata={'section_titles': ['References'], 'pages': [12]}), ResponseChunk(id='chunk_6c7b849f-7ec2-4bd3-9e57-19d2a5275f84', content='Kornblith, J. Shlens, and Q. V. Le. Do better imagenet models transfer better? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2661 2671, 2019. [29] T.', chunk_index=120, num_tokens=58, metadata={'section_titles': ['References'], 'pages': [12]}), ResponseChunk(id='chunk_468489e1-1e6a-4758-afd4-448d5c495b4b', content='Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned index structures. In Proceedings of the 2018 international conference on management of data, pages 489 504, 2018. [30] A. Kusupati, M. Wallingford, V. Ramanujan, R. Somani, J. S. Park, K. Pillutla, P. Jain, S. Kakade, and A.', chunk_index=121, num_tokens=110, metadata={'section_titles': ['References'], 'pages': [12]}), ResponseChunk(id='chunk_a5a72f52-a5f9-41e4-9a8c-87d6d6f8f71c', content='Farhadi. Llc: Accurate, multi-purpose learnt low-dimensional binary codes. Advances in Neural Information Processing Systems, 34:23900 23913, 2021. [31] A. Kusupati, G. Bhatt, A. Rege, M. Wallingford, A.', chunk_index=122, num_tokens=66, metadata={'section_titles': ['References'], 'pages': [12]}), ResponseChunk(id='chunk_06ac709f-d1b2-47cb-8a28-f16e8308871d', content='Sinha, V. Ramanujan, W. Howard-Snyder, K. Chen, S. Kakade, P. Jain, and A. Farhadi.', chunk_index=123, num_tokens=36, metadata={'section_titles': ['References'], 'pages': [12]}), ResponseChunk(id='chunk_a3e620dd-a625-4907-a64d-b4c28e9e81d0', content='Matryoshka representation learning. In Advances in Neural Information Processing Systems, December 2022. [32] T. Kwiatkowski, J. Palomaki, O. Redfield, M.', chunk_index=124, num_tokens=42, metadata={'section_titles': ['References'], 'pages': [12]}), ResponseChunk(id='chunk_266e259b-dbe1-4954-b931-f6c6c5324df0', content='Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, et al.', chunk_index=125, num_tokens=36, metadata={'section_titles': ['References'], 'pages': [12]}), ResponseChunk(id='chunk_bac56f3c-24e5-4795-8043-fba9f7b574f8', content='Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453 466, 2019. [33] D. A. Levin and Y.', chunk_index=126, num_tokens=41, metadata={'section_titles': ['References'], 'pages': [12]}), ResponseChunk(id='chunk_2b8bd566-0a22-4dc7-92d8-3ca5835e1b32', content='Peres. Markov chains and mixing times, volume 107. American Mathematical Soc., 2017. [34] W. Li, Y. Zhang, Y. Sun, W. Wang, W. Zhang, and X. Lin.', chunk_index=127, num_tokens=50, metadata={'section_titles': ['References'], 'pages': [12]}), ResponseChunk(id='chunk_b69d2db5-903e-4345-85a0-d6d54622f4a8', content='Approximate nearest neighbor search on high dimensional data experiments, analyses, and improvement. IEEE Transactions on Knowledge and Data Engineering, 2020. [35] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T.', chunk_index=128, num_tokens=54, metadata={'section_titles': ['References'], 'pages': [12]}), ResponseChunk(id='chunk_f3d0eb81-7305-4704-8a66-a0bbabac1355', content='Darrell, and S. Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11976 11986, 2022. [36] S.', chunk_index=129, num_tokens=51, metadata={'section_titles': ['References'], 'pages': [12]}), ResponseChunk(id='chunk_a4a32b32-d7ce-4f32-8949-519d1826ad7e', content='Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2): 129 137, 1982. [37] Y. Malkov, A. Ponomarenko, A. Logvinov, and V. Krylov. Approximate nearest neighbor algorithm based on navigable small world graphs.', chunk_index=130, num_tokens=71, metadata={'section_titles': ['References'], 'pages': [12, 13]}), ResponseChunk(id='chunk_edcfe3ca-3e44-40b5-8fe4-e6faeab882ab', content='Information Systems, 45:61 68, 2014. [38] Y. A. Malkov and D. Yashunin.', chunk_index=131, num_tokens=31, metadata={'section_titles': ['References'], 'pages': [13]}), ResponseChunk(id='chunk_63d3f97c-247c-4274-b990-61dcb5754f83', content='Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis & Machine Intelligence, 42(04):824 836, 2020. [39] K. Mardia, J. Kent, and J. Bibby.', chunk_index=132, num_tokens=57, metadata={'section_titles': ['References'], 'pages': [13]}), ResponseChunk(id='chunk_be69717d-3707-47f1-992a-39a804307290', content='Multivariate analysis. Probability and Mathematical Statistics, 1979. [40] P. Nayak. Understanding searches better than ever before. Google AI Blog, 2019. URL https: //blog.google/products/search/search-language-understanding-bert/. [41] A.', chunk_index=133, num_tokens=56, metadata={'section_titles': ['References'], 'pages': [13]}), ResponseChunk(id='chunk_dc422271-1b18-47ed-9c73-c62c5899239f', content='Neelakantan, T. Xu, R. Puri, A. Radford, J. M. Han, J. Tworek, Q. Yuan, N. Tezak, J. W. Kim, C.', chunk_index=134, num_tokens=49, metadata={'section_titles': ['References'], 'pages': [13]}), ResponseChunk(id='chunk_7327c9ec-02c0-4bb2-89ac-8786d94ccdf3', content='Hallacy, et al. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005, 2022. [42] A.', chunk_index=135, num_tokens=40, metadata={'section_titles': ['References'], 'pages': [13]}), ResponseChunk(id='chunk_40cf180a-cf53-4add-8955-f702c610179f', content='Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al.', chunk_index=136, num_tokens=53, metadata={'section_titles': ['References'], 'pages': [13]}), ResponseChunk(id='chunk_cb2b6842-59a0-4b99-9218-f4eb978c2669', content='Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748 8763. PMLR, 2021. [43] V. Ramanujan, P. K. A. Vasu, A.', chunk_index=137, num_tokens=54, metadata={'section_titles': ['References'], 'pages': [13]}), ResponseChunk(id='chunk_dbba0bde-8375-4be9-99aa-441f39b4cbab', content='Farhadi, O. Tuzel, and H. Pouransari. Forward compatible training for representation learning. arXiv preprint arXiv:2112.02805, 2021. [44] B.', chunk_index=138, num_tokens=48, metadata={'section_titles': ['References'], 'pages': [13]}), ResponseChunk(id='chunk_cff6939a-91e6-448f-ba5d-8027eddd109c', content='Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning, pages 5389 5400. PMLR, 2019. [45] O. Russakovsky, J.', chunk_index=139, num_tokens=62, metadata={'section_titles': ['References'], 'pages': [13]}), ResponseChunk(id='chunk_a5024934-6cd7-4df9-8ef4-37e4351d1e88', content='Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al.', chunk_index=140, num_tokens=45, metadata={'section_titles': ['References'], 'pages': [13]}), ResponseChunk(id='chunk_7601d6d0-e768-4a73-acf6-855a6cc00c1d', content='Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211 252, 2015. [46] R.', chunk_index=141, num_tokens=32, metadata={'section_titles': ['References'], 'pages': [13]}), ResponseChunk(id='chunk_c000da72-cf0b-4e60-853b-3fe2fe26ad30', content='Salakhutdinov and G. Hinton. Semantic hashing. International Journal of Approximate Reasoning, 50(7):969 978, 2009. [47] H.', chunk_index=142, num_tokens=40, metadata={'section_titles': ['References'], 'pages': [13]}), ResponseChunk(id='chunk_a2327058-6739-4512-9e4d-d99c9a32379c', content='V. Simhadri, G. Williams, M. Aum ller, M. Douze, A. Babenko, D. Baranchuk, Q. Chen, L. Hosseini, R. Krishnaswamy, G. Srinivasa, et al.', chunk_index=143, num_tokens=60, metadata={'section_titles': ['References'], 'pages': [13]}), ResponseChunk(id='chunk_a09519cc-79cb-4410-99dc-ceec6fb211d2', content='Results of the neurips 21 challenge on billion-scale approximate nearest neighbor search. arXiv preprint arXiv:2205.03763, 2022. [48] J.', chunk_index=144, num_tokens=40, metadata={'section_titles': ['References'], 'pages': [13]}), ResponseChunk(id='chunk_bade64c3-e3bc-446b-aee3-e411223e1806', content='Sivic and A. Zisserman. Video google: A text retrieval approach to object matching in videos. In Computer Vision, IEEE International Conference on, volume 3, pages 1470 1470.', chunk_index=145, num_tokens=43, metadata={'section_titles': ['References'], 'pages': [13]}), ResponseChunk(id='chunk_626ff07c-35f5-4a9f-9516-460214cd4430', content='IEEE Computer Society, 2003. [49] D. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N.', chunk_index=146, num_tokens=39, metadata={'section_titles': ['References'], 'pages': [13]}), ResponseChunk(id='chunk_4b56644f-877e-42d2-b0af-95f5d889fdf0', content='Srebro. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):2822 2878, 2018. [50] C.', chunk_index=147, num_tokens=42, metadata={'section_titles': ['References'], 'pages': [13]}), ResponseChunk(id='chunk_fba78176-87ad-4c0d-8592-6d596b8bf464', content='Waldburger. As search needs evolve, microsoft makes ai tools for better search available to researchers and developers. Microsoft AI Blog, 2019. URL https://blogs.microsoft. com/ai/bing-vector-search/. [51] M.', chunk_index=148, num_tokens=49, metadata={'section_titles': ['References'], 'pages': [13]}), ResponseChunk(id='chunk_1ab627f9-cb02-443d-878f-9a361b326e64', content='Wang, X. Xu, Q. Yue, and Y. Wang. A comprehensive survey and experimental comparison of graph-based approximate nearest neighbor search.', chunk_index=149, num_tokens=30, metadata={'section_titles': ['References'], 'pages': [13]}), ResponseChunk(id='chunk_e3dcc68e-80b6-4652-bf19-5d4bd2caa1cd', content='Proceedings of the VLDB Endowment, 14 (11):1964 1978, 2021. [52] R. Weber, H.-J. Schek, and S.', chunk_index=150, num_tokens=41, metadata={'section_titles': ['References'], 'pages': [13]}), ResponseChunk(id='chunk_4731eb00-9160-45c5-a42e-c56cac00acea', content='Blott. A quantitative analysis and performance study for similarity- search methods in high-dimensional spaces. In VLDB, volume 98, pages 194 205, 1998. [53] I.', chunk_index=151, num_tokens=42, metadata={'section_titles': ['References'], 'pages': [13]}), ResponseChunk(id='chunk_8f4c842a-027a-43d8-adfb-7cec7ee58fa5', content='H. Witten, I. H. Witten, A. Moffat, T. C. Bell, T. C. Bell, E. Fox, and T. C. Bell.', chunk_index=152, num_tokens=40, metadata={'section_titles': ['References'], 'pages': [13]}), ResponseChunk(id='chunk_b7bab632-5f0c-424c-994a-071b031134c0', content='Managing gigabytes: compressing and indexing documents and images. Morgan Kaufmann, 1999. A AdANNS Framework Algorithm 1 AdANNS-IVF Psuedocode # Index database to construct clusters and build inverted file system def adannsConstruction(database, d_cluster, num_clusters): # Slice database with cluster construction dim (d_cluster) xb = database[:d_cluster] cluster_centroids = constructClusters(xb, num_clusters) return cluster_centroids def adannsInference(queries, centroids, d_shortlist, d_search, num_probes, k): # Slice queries and centroids with cluster shortlist dim (d_shortlist) xq = queries[:d_shortlist] xc = centroids[:d_shortlist] for q in queries: # compute distance of query from each cluster centroid candidate_distances = computeDistances(q, xc) # sort cluster candidates by distance and choose small number to probe cluster_candidates = sortAscending(candidate_distances)[:num_probes] database_candidates = getClusterMembers(cluster_candidates) # Linear Scan all shortlisted clusters with search dim (d_search) k_nearest_neighbors[q] = linearScan(q, database_candidates, d_search, return k_nearest_neighbors Figure 5: The schematic of inverted file index (IVF) outlaying the construction and inference phases.', chunk_index=153, num_tokens=267, metadata={'section_titles': ['Algorithm 1 AdANNS-IVF Psuedocode', 'return k_nearest_neighbors', 'References', 'A AdANNS Framework'], 'pages': [13, 14]}), ResponseChunk(id='chunk_972cb23b-f732-46e3-bfca-61b8b1c57c6a', content='Adaptive representations can be utilized effectively in the decoupled components of clustering and searching for a better accuracy-compute trade-off (AdANNS-IVF). B Training and Compute Costs A bulk of our ANNS experimentation was written with Faiss [24], a library for efficient similarity search and clustering. AdANNS was implemented from scratch (Algorithm 1) due to difficulty in decoupling clustering and linear scan with Faiss, with code available at https://github.com/ RAIVNLab/AdANNS. We also provide a version of AdANNS with Faiss optimizations with the restriction that Dc   Ds as a limitation of the current implementation, which can be further optimized. All ANNS experiments (AdANNS-IVF, MG-IVF-RR, IVF-MR, IVF-RR, HNSW, HNSWOPQ, IVFOPQ) were run on an Intel Xeon 2.20GHz CPU with 12 cores. Exact Search (Flat L2, PQ, OPQ) and DiskANN experiments were run with CUDA 11.0 on a A100-SXM4 NVIDIA GPU with 40G RAM. The wall-clock inference times quoted in Figure 1a and Table 3 are reported on CPU with Faiss optimizations, and are averaged over three inference runs for ImageNet-1K retrieval.', chunk_index=154, num_tokens=283, metadata={'section_titles': ['B Training and Compute Costs', 'B.1 Inference Compute Cost', '4https://github.com/facebookresearch/DPR', 'return k_nearest_neighbors', 'Algorithm 1 AdANNS-IVF Psuedocode', 'A AdANNS Framework'], 'pages': [14, 15]}), ResponseChunk(id='chunk_d0839731-6a2a-447b-9077-9a5b07b258f3', content='Table 3: Comparison of AdANNS-IVF and Rigid-IVF wall-clock inference times for ImageNet-1K retrieval. AdANNS-IVF has up to   1.5% gain over Rigid-IVF for a fixed search latency per query.', chunk_index=155, num_tokens=60, metadata={'section_titles': ['4https://github.com/facebookresearch/DPR', 'B Training and Compute Costs', 'B.1 Inference Compute Cost'], 'pages': [15]}), ResponseChunk(id='chunk_a6f3b634-2fb5-49b3-bf2b-802695beed5c', content='DPR [27] on NQ [32]. We follow the setup on the DPR repo4: the Wikipedia corpus has 21 million passages and Natural Questions dataset for open-domain QA settings. The training set contains 79,168 question and answer pairs, the dev set has 8,757 pairs and the test set has 3,610 pairs.', chunk_index=156, num_tokens=72, metadata={'section_titles': ['4https://github.com/facebookresearch/DPR', 'B Training and Compute Costs', 'B.1 Inference Compute Cost'], 'pages': [15]}), ResponseChunk(id='chunk_6148058d-5948-4587-a31a-22a23a5fb813', content='B.1 Inference Compute Cost We evaluate inference compute costs for IVF in MegaFLOPS per query (MFLOPS/query) as shown in Figures 2, 10a, and 8 as follows:', chunk_index=157, num_tokens=45, metadata={'section_titles': ['4https://github.com/facebookresearch/DPR', 'B Training and Compute Costs', 'B.1 Inference Compute Cost'], 'pages': [15]}), ResponseChunk(id='chunk_142918ab-0646-4d11-9778-049693a7f2f7', content='C = dsk + npdsND k where dc is the cluster construction embedding dimensionality, ds is the embedding dim used for linear scan within each probed cluster, which is controlled by # of search probes np. Finally, k is the number of clusters |Ci| indexed over database of size ND. The default setting in this work, unless otherwise stated, is np = 1, k = 1024, ND = 1281167 (ImageNet-1K trainset).', chunk_index=158, num_tokens=100, metadata={'section_titles': ['4https://github.com/facebookresearch/DPR', 'B Training and Compute Costs', 'B.1 Inference Compute Cost'], 'pages': [15]}), ResponseChunk(id='chunk_ba452cb5-6854-497b-bc71-ea4f0a0cbd50', content='Vanilla IVF supports only dc = ds, while AdANNS-IVF provides flexibility via decoupling clustering and search (Section 4). AdANNS-IVF-D is a special case of AdANNS-IVF with the flexibility restricted to inference, i.e., dc is a fixed high-dimensional MR. 4https://github.com/facebookresearch/DPR C Evaluation Metrics In this work, we primarily use top-1 accuracy (i.e. 1-Nearest Neighbor), recall@k, corrected mean average precision (mAP@k) [30] and k-Recall@N (recall score), which are defined over all queries Q over indexed database of size ND as: top-1 = Q correct_pred@1 |Q| Recall@k = Q correct_pred@k |Q|   num_classes |ND| where correct_pred@k is the number of k-NN with correctly predicted labels for a given query. As noted in Section 3, k-Recall@N is the overlap between k exact search nearest neighbors (considered as ground truth) and the top N retrieved documents. As Faiss [24] supports a maximum of 2048- NN while searching the indexed database, we report 40-Recall@2048 in Figure 13.', chunk_index=159, num_tokens=268, metadata={'section_titles': ['B Training and Compute Costs', 'B.1 Inference Compute Cost', '4https://github.com/facebookresearch/DPR', '(c) DiskANN + OPQ on ImageNet-1K', 'C Evaluation Metrics', '(d) HNSW + OPQ on ImageNet-1K'], 'pages': [15, 16]}), ResponseChunk(id='chunk_020b450a-ef8d-4e01-9707-393a108e6182', content='Also note that for ImageNet-1K, which constitutes a bulk of the experimentation in this work, |Q| = 50000, |ND| = 1281167 and num_classes = 1000. For ImageNetv2 [44], |Q| = 10000 and num_classes = 1000, and for ImageNet-4K [31], |Q| = 210100, |ND| = 4202000 and num_classes = 4202. For NQ [32], |Q| = 3610 and |ND| = 21015324.', chunk_index=160, num_tokens=125, metadata={'section_titles': ['(c) DiskANN + OPQ on ImageNet-1K', '(d) HNSW + OPQ on ImageNet-1K', 'C Evaluation Metrics'], 'pages': [16]}), ResponseChunk(id='chunk_24d15e0f-6609-4ae4-b9ea-b9c54ac634dd', content='As NQ consists of question-answer pairs (instance- level), num_classes = 3610 for the test set. (c) DiskANN + OPQ on ImageNet-1K (d) HNSW + OPQ on ImageNet-1K D AdANNS-OPQ In this section, we take a deeper dive into the quantization characteristics of MR. In this work, we restrict our focus to optimized product quantization (OPQ) [13], which adds a learned space rotation and dimensionality permutation to PQ s sub-vector quantization to learn more optimal PQ codes. We compare OPQ to vanilla PQ on ImageNet in Table 4, and observe large gains at larger embedding dimensionalities, which agrees with the findings of Jayaram Subramanya et al. [22]. We perform a study of composite OPQ m   b indices on ImageNet-1K across compression compute budgets m (where b = 8, i.e. 1 Byte), i.e.', chunk_index=161, num_tokens=203, metadata={'section_titles': ['OPQ on NQ dataset', '(c) DiskANN + OPQ on ImageNet-1K', 'C Evaluation Metrics', '(d) HNSW + OPQ on ImageNet-1K', 'D AdANNS-OPQ', '(b) IVF + OPQ on Natural Questions'], 'pages': [16, 17]}), ResponseChunk(id='chunk_e1ae5664-ee66-4ba4-8c4a-68f183839a6b', content='Exact Search with OPQ, IVF+OPQ, HNSW+OPQ, and DiskANN+OPQ, as seen in Figure 6.', chunk_index=162, num_tokens=33, metadata={'section_titles': ['OPQ on NQ dataset', 'D AdANNS-OPQ', '(b) IVF + OPQ on Natural Questions'], 'pages': [17]}), ResponseChunk(id='chunk_037e4a18-d6a0-4385-b122-467f524e002f', content='It is evident from these results: 1. Learning OPQ codebooks with AdANNS (Figure 6a) provides a 1-5% gain in top-1 accuracy over rigid representations at low compute budgets (  32 Bytes). AdANNS-OPQ saturates to Rigid-OPQ performance at low compression (  64 Bytes). 2.', chunk_index=163, num_tokens=78, metadata={'section_titles': ['OPQ on NQ dataset', 'D AdANNS-OPQ', '(b) IVF + OPQ on Natural Questions'], 'pages': [17]}), ResponseChunk(id='chunk_5d40609c-be54-4378-9d8c-a99f6904062d', content='For IVF, learning clusters with MRs instead of RRs (Figure 6b) provides substantial gains (1- 4%). In contrast to Exact-OPQ, using AdANNS for learning OPQ codebooks does not provide substantial top-1 accuracy gains over MR with d = 2048 (highest), though it is still slightly better or equal to MR-2048 at all compute budgets.', chunk_index=164, num_tokens=84, metadata={'section_titles': ['OPQ on NQ dataset', 'D AdANNS-OPQ', '(b) IVF + OPQ on Natural Questions'], 'pages': [17]}), ResponseChunk(id='chunk_78f41cc9-7ee2-43f7-81b9-3d58dd233c09', content='This further supports that IVF performance generally scales with embedding dimensionality, which is consistent with our findings on ImageNet across robustness variants and encoders (See Figures 9 and 15 respectively). 3. Note that in contrast to Exact, IVF, and HNSW coarse quantizers, DiskANN inherently re-ranks the retrieved shortlist with high-precision embeddings (d = 2048), which is reflected in its high top-1 accuracy. We find that AdANNS with 8-byte OPQ (Figure 6c) matches the top-1 accuracy of rigid representations using 32-byte OPQ, for a 4  cost reduction for the same accuracy. Also note that using AdANNS provides large gains over using MR-2048 at high compression (1.5%), highlighting the necessity of AdANNS s flexibility for high-precision retrieval at low compute budgets. 4.', chunk_index=165, num_tokens=186, metadata={'section_titles': ['OPQ on NQ dataset', 'D AdANNS-OPQ', '(b) IVF + OPQ on Natural Questions'], 'pages': [17]}), ResponseChunk(id='chunk_13e005b0-b3c5-4383-8218-45c0a5803aeb', content='Our findings on the HNSW-OPQ composite index (Figure 6d) are consistent with all other indices, i.e. HNSW graphs constructed with AdANNS OPQ codebooks provide significant gains over RR and MR, especially at high compression (  32 Bytes).', chunk_index=166, num_tokens=59, metadata={'section_titles': ['OPQ on NQ dataset', 'D AdANNS-OPQ', '(b) IVF + OPQ on Natural Questions'], 'pages': [17]}), ResponseChunk(id='chunk_fdd85dd3-5d47-46bc-8e16-41a96acad48e', content='OPQ on NQ dataset (b) IVF + OPQ on Natural Questions Our observations on ImageNet with ResNet-50 MR across search structures also extend to the Natural Questions dataset with Dense Passage Retriever (DPR with BERT-Base MR embeddings). We note that AdANNS provides gains over RR-768 embeddings for both Exact Search and IVF with OPQ (Figure 7a and 7b).', chunk_index=167, num_tokens=88, metadata={'section_titles': ['OPQ on NQ dataset', 'D AdANNS-OPQ', '(b) IVF + OPQ on Natural Questions'], 'pages': [17]}), ResponseChunk(id='chunk_b10cdbe9-f803-417b-841a-4278441bcab0', content='We find that similar to ImageNet (Figure 15) IVF performance on Natural Questions generally scales with dimensionality. AdANNS thus reduces to MR-768 performance for M   16. See Appendix G for a more in-depth discussion of AdANNS with DPR on Natural Questions. 8 16 32 64 62.19 65.99 67.99 69.20 56.11 60.27 62.04 63.46 59.80 63.18 64.74 66.40 66.89 69.25 70.39 70.57 61.69 64.09 64.97 65.15 66.30 67.99 68.51 68.51 E AdANNS-IVF Inverted file index (IVF) [48] is a simple yet powerful ANNS data structure used in web-scale search systems [16].', chunk_index=168, num_tokens=199, metadata={'section_titles': ['OPQ on NQ dataset', 'D AdANNS-OPQ', '(b) IVF + OPQ on Natural Questions'], 'pages': [17, 18]}), ResponseChunk(id='chunk_30114b47-91c9-4548-bac6-28717f7e0033', content='IVF construction involves clustering (coarse quantization often through k-means) [36] on d-dimensional representation that results in an inverted file list [53] of all the data points in each cluster.', chunk_index=169, num_tokens=42, metadata={'section_titles': ['(b) IVF + OPQ on Natural Questions'], 'pages': [18]}), ResponseChunk(id='chunk_693e5ffa-1700-4571-a6f2-ee44d51713d0', content='During search, the d-dimensional query representation is first assigned to the closest clusters (# probes, typically set to 1) and then an exhaustive linear scan happens within each cluster to obtain the nearest neighbors.', chunk_index=170, num_tokens=40, metadata={'section_titles': ['(b) IVF + OPQ on Natural Questions'], 'pages': [18]}), ResponseChunk(id='chunk_06518beb-5008-4846-bc01-8c6068ad83ed', content='As seen in Figure 9, IVF top-1 accuracy scales logarithmically with increasing representation dimensionality d on ImageNet-1K/V2/4K. The learned low-d representations thus provide better accuracy-compute trade-offs compared to high-d representations, thus furthering the case for usage of AdANNS with IVF. Our proposed adaptive variant of IVF, AdANNS-IVF, decouples the clustering, with dc dimensions, and the linear scan within each cluster, with ds dimensions   setting dc = ds results in non- adaptive vanilla IVF.', chunk_index=171, num_tokens=118, metadata={'section_titles': ['(b) IVF + OPQ on Natural Questions'], 'pages': [18]}), ResponseChunk(id='chunk_69fe04d2-34aa-4694-bbe7-51aadf5e9e1b', content='This helps in the smooth search of design space for the optimal accuracy- compute trade-off. A naive instantiation yet strong baseline would be to use explicitly trained dc and ds dimensional rigid representations (called MG-IVF-RR, for multi-granular IVF with rigid representations).', chunk_index=172, num_tokens=56, metadata={'section_titles': ['E.1 Robustness', '(b) IVF + OPQ on Natural Questions'], 'pages': [18, 19]}), ResponseChunk(id='chunk_95e01e9d-4e5b-4d75-9c5a-fc7899d97a86', content='We also examine the setting of adaptively choosing low-dimensional MR to linear scan the shortlisted clusters built with high-dimensional MR, i.e. AdANNS-IVF-D, as seen in Table 5. As seen in Figure 8, AdANNS-IVF provides pareto-optimal accuracy-compute tradeoff across inference compute. This figure is a more exhaustive indication of AdANNS-IVF behavior compared to baselines than Figures 1a and 2.', chunk_index=173, num_tokens=98, metadata={'section_titles': ['E.1 Robustness', '(b) IVF + OPQ on Natural Questions'], 'pages': [19]}), ResponseChunk(id='chunk_85820108-7362-419e-9d19-bfc1b8dd83f3', content='AdANNS-IVF is evaluated for all possible tuples of dc, ds, k = |C|   {8, 16, . . . , 2048}. AdANNS-IVF-D is evaluated for a pre-built IVF index with dc = 2048 and ds   {8, . . . , 2048}. MG-IVF-RR configurations are evaluated for dc   {8, . . . , ds}, ds   {32, . . . , 2048} and k = 1024 clusters.', chunk_index=174, num_tokens=115, metadata={'section_titles': ['E.1 Robustness', '(b) IVF + OPQ on Natural Questions'], 'pages': [19]}), ResponseChunk(id='chunk_eaba92e0-f2f8-4c9d-8c88-9fcfc2438658', content='A study over additional k values is omitted due to high compute cost. Finally, IVF-MR and IVF-RR configurations are evaluated for dc = ds   {8, 16, . . . , 2048} and k   {256, . . . , 8192}.', chunk_index=175, num_tokens=61, metadata={'section_titles': ['E.1 Robustness', '(b) IVF + OPQ on Natural Questions'], 'pages': [19]}), ResponseChunk(id='chunk_43e0a991-4990-485d-b13f-ae50f652e9ac', content='Note that for a fair comparison, we use np = 1 across all configurations. We discuss the inference compute for these settings in Appendix B.1.', chunk_index=176, num_tokens=31, metadata={'section_titles': ['E.1 Robustness', '(b) IVF + OPQ on Natural Questions'], 'pages': [19]}), ResponseChunk(id='chunk_30b11d92-95df-437d-9ad1-f66e22194b9c', content='E.1 Robustness As shown in Figure 9, we examined the clustering capabilities of MRs on both in-distribution (ID) queries via ImageNet-1K and out-of-distribution (OOD) queries via ImageNetV2 [44], as well as on larger-scale ImageNet-4K [31]. For ID queries on ImageNet-1K (Figure 9a), IVF-MR is at least as accurate as Exact-RR for d   256 with a single search probe, demonstrating the quality of in- distribution low-d clustering with MR. On OOD queries (Figure 9b), we observe that IVF-MR is on average 2% more robust than IVF-RR across all cluster construction and linear scan dimensionalities d. It is also notable that clustering with MRs followed by linear scan with # probes = 1 is more robust than exact search with RR embeddings across all d   2048, indicating the adaptability of MRs to distribution shifts during inference.', chunk_index=177, num_tokens=209, metadata={'section_titles': ['E.1 Robustness', '(b) IVF + OPQ on Natural Questions'], 'pages': [19]}), ResponseChunk(id='chunk_566230b4-d515-4c4a-bd0c-06d9405ec73f', content='As seen in Table 5, on ImageNetV2 AdANNS-IVF-D is the best On 4-million scale ImageNet-4K (Figure 9c), we observe similar accuracy trends of IVF-MR compared to Exact-MR as in ImageNet-1K (Figure 9a) and ImageNetV2 (Figure 9b).', chunk_index=178, num_tokens=77, metadata={'section_titles': ['T & i=', 'E.1 Robustness', 'g  ', 'E.2 IVF-MR Ablations', '(b) IVF + OPQ on Natural Questions', 'E.3 Clustering Distribution'], 'pages': [19, 20]}), ResponseChunk(id='chunk_fe19648e-fa7d-49fc-85d1-8287d81ea532', content='We omit baseline IVF-RR and Exact-RR experiments due to high compute cost at larger scale. E.2 IVF-MR Ablations g T & i= As seen in Figure 10a, IVF-MR can match the accuracy of Exact Search on ImageNet-4K with   100  less compute. We also explored the capability of MRs at retrieving cluster centroids with low-d compared to a ground truth of 2048-d with k-Recall@N, as seen in Figure 10b. MRs were able to saturate to near-perfect 1-Recall@N for d   32 and N   4, indicating the potential of AdANNS at matching exact search performance with less than 10 search probes np.', chunk_index=179, num_tokens=160, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_5d5e34d6-be4b-4b32-958d-33192ecccac9', content='E.3 Clustering Distribution We examined the distribution of learnt clusters across embedding dimensionalities d for both MR and RR models, as seen in Figure 11. We observe IVF-MR to have less variance than IVF-RR at d   {8, 16}, and slightly higher variance for d   32, while IVF-MR outperforms IVF-RR in top-1 across all d (Figure 9a). This indicates that although MR learns clusters that are less uniformly distributed than RR at high d, the quality of learnt clustering is superior to RR across all d.', chunk_index=180, num_tokens=122, metadata={'section_titles': ['T & i=', 'E.1 Robustness', 'g  ', 'E.2 IVF-MR Ablations', 'E.3 Clustering Distribution'], 'pages': [20]}), ResponseChunk(id='chunk_108f1c92-fc1b-44ee-b43b-dffbb5c9e793', content='Note that a uniform distribution is N/k data points per cluster, i.e.   1250 for ImageNet-1K with k = 1024.', chunk_index=181, num_tokens=33, metadata={'section_titles': ['T & i=', 'E.1 Robustness', 'g  ', 'E.2 IVF-MR Ablations', 'E.3 Clustering Distribution'], 'pages': [20]}), ResponseChunk(id='chunk_2a20ba05-a409-40f0-9177-36adf2f4dc67', content='We quantitatively evaluate the proximity of the MR and RR clustering distributions with Total Variation Distance [33], which is defined over two discrete probability distributions p, q over [n] as follows: 1 drv (p,q) = 3 > \\\\pi   ai i [n] We also compute dT V,2048(MR-d) = dT V (MR-d, RR-2048), which evaluates the total variation dis- tance of a given low-d MR from high-d RR-2048. We observe a monotonically decreasing dT V,2048 with increasing d, which demonstrates that MR clustering distributions get closer to RR-2048 as we increase the embedding dimensionality d. We observe in Figure 11 that dT V (MR-d, RR-d)   7e   4 for d   {8, 256, . . . , 2048} and   3e   4 for d   {16, 32, 64}. These findings agree with the top-1 improvement of MR over RR as shown in Figure 9a, where there are smaller improvements for d   {16, 32, 64} (smaller dT V ) and larger improvements for d   {8, 256, . . . , 2048} (larger dT V ).', chunk_index=182, num_tokens=279, metadata={'section_titles': ['T & i=', 'E.1 Robustness', 'g  ', 'E.2 IVF-MR Ablations', 'F AdANNS-DiskANN', 'E.3 Clustering Distribution'], 'pages': [20, 21]}), ResponseChunk(id='chunk_4b66576f-0c0d-4598-a64c-11ec17b1e097', content='These results demonstrate a correlation between top-1 performance of IVF-MR and the quality of clusters learnt with MR. F AdANNS-DiskANN DiskANN is a state-of-the-art graph-based ANNS index capable of serving queries from both RAM and SSD. DiskANN builds a greedy best-first graph with OPQ distance computation, with compressed vectors stored in memory.', chunk_index=183, num_tokens=75, metadata={'section_titles': ['E.3 Clustering Distribution', 'F AdANNS-DiskANN'], 'pages': [21]}), ResponseChunk(id='chunk_532d55f2-8ecf-4c1d-b28e-122388142b90', content='The index and full-precision vectors are stored on the SSD. During search, As with IVF, DiskANN is also well suited to the flexibility provided by AdANNS as we demonstrate on both ImageNet and NQ that the optimal PQ codebook for a given compute budget is learnt with a smaller em- bedding dimensionality d (see Fig- ures 6c and 7a). We demonstrate the capability of AdANNS-DiskANN with a compute budget of m   {32, 64} in Table 1. We tabulate the search time latency of AdANNS- DiskANN in microseconds ( s) in Table 6, which grows linearly with graph construction dimensionality d. We also examine DiskANN-MR with SSD graph indices on ImageNet-1K across OPQ budgets for distance com- putation mdc   {32, 48, 64}, as seen in Figure 12.', chunk_index=184, num_tokens=191, metadata={'section_titles': ['G AdANNS on Natural Questions', 'E.3 Clustering Distribution', 'F AdANNS-DiskANN', '5https://github.com/microsoft/DiskANN'], 'pages': [21, 22]}), ResponseChunk(id='chunk_c2106abd-f2df-49c9-8cc5-2157ebc9e13d', content='With SSD indices, we store PQ-compressed vectors on disk with mdisk = mdc, which es- sentially disables DiskANN s implicit high-precision re-ranking. We ob- serve similar trends to other composite ANNS indices on ImageNet, where the optimal dim for fixed OPQ budget is not the highest dim (d = 1024 with fp32 embeddings is current highest dim supported by DiskANN which stores vectors in 4KB sectors on disk). This provides further motiva- tion for AdANNS-DiskANN, which leverages MRs to provide flexible access to the optimal dim for quantization and thus enables similar Top-1 accuracy to Rigid DiskANN for up to 1/4 the cost (Figure 6c).', chunk_index=185, num_tokens=153, metadata={'section_titles': ['G AdANNS on Natural Questions', 'F AdANNS-DiskANN', '5https://github.com/microsoft/DiskANN'], 'pages': [22]}), ResponseChunk(id='chunk_256f07c1-1195-4ee8-8786-d262c8794227', content='G AdANNS on Natural Questions In addition to image retrieval on ImageNet, we also experiment with dense passage retrieval (DPR) on Natural Questions.', chunk_index=186, num_tokens=31, metadata={'section_titles': ['G AdANNS on Natural Questions', 'F AdANNS-DiskANN', '5https://github.com/microsoft/DiskANN'], 'pages': [22]}), ResponseChunk(id='chunk_01038cc4-29e4-4389-9bb9-73f90918f33d', content='As shown in Figure 6, MR representations are 1   10% more accurate than their 5https://github.com/microsoft/DiskANN RR counterparts across PQ compute budgets with Exact Search + OPQ on NQ. We also demonstrate that IVF-MR is 1   2.5% better than IVF-RR for Precision@k, k   {1, 5, 20, 100, 200}. Note that on NQ, IVF loses   10% accuracy compared to exact search, even with the RR-768 baseline. We hypothesize the weak performance of IVF owing to poor clusterability of the BERT-Base embeddings fine-tuned on the NQ dataset. A more thorough exploration of AdANNS-IVF on NQ is an immediate future work and is in progress.', chunk_index=187, num_tokens=173, metadata={'section_titles': ['G AdANNS on Natural Questions', 'H Ablations', 'H.1 Recall Score Analysis', 'F AdANNS-DiskANN', '5https://github.com/microsoft/DiskANN'], 'pages': [22, 23]}), ResponseChunk(id='chunk_0b9b2420-4ff5-4e1d-b58c-d0d8c7f691c4', content='H Ablations H.1 Recall Score Analysis Representation Size In this section we also examine the variation of k-Recall@N with by probing a larger search space with IVF and HNSW indices. For IVF, search probes represent the number of clusters shortlisted for linear scan during inference. For HNSW, search quality is controlled by the ef Search parameter [38], which represents the closest neighbors to query q at level lc of the graph and is analogous to number of search probes in IVF.', chunk_index=188, num_tokens=105, metadata={'section_titles': ['H.3 Generality across Encoders', 'Representation Size', 'H Ablations', 'H.1 Recall Score Analysis', '5https://github.com/microsoft/DiskANN', 'H.2 Relative Contrast'], 'pages': [23, 24]}), ResponseChunk(id='chunk_da1af0d2-1d44-4f89-be36-8177e1597bc6', content='As seen in Figure 13, general trends show a) an intuitive increase in recall with increasing search probes np) for fixed search probes, b) a decrease in recall with increasing search dimensionality d c) similar trends in ImageNet-1K and 4  larger ImageNet-4K.', chunk_index=189, num_tokens=61, metadata={'section_titles': ['H.3 Generality across Encoders', 'Representation Size', 'H.2 Relative Contrast'], 'pages': [24]}), ResponseChunk(id='chunk_38e7a0a1-ab8f-4a5b-9b86-15378f80a9d1', content='H.2 Relative Contrast We utilize Relative Contrast [18] to capture the difficulty of nearest neighbors search with IVF-MR compared to IVF-RR. For a given database X = {xi   Rd, i = 1, . . . , ND}, a query q   Rd, and a distance metric D(., .) we compute relative contrast Cr as a measure of the difficulty in finding the 1-nearest neighbor (1-NN) for a query q in database X as follows: 1.', chunk_index=190, num_tokens=106, metadata={'section_titles': ['H.3 Generality across Encoders', 'Representation Size', 'H.2 Relative Contrast'], 'pages': [24]}), ResponseChunk(id='chunk_ce047c27-b576-43f8-b583-a1f6798d6085', content='Compute Dq min = min i=1. . .n D(q, xi), i.e. the distance of query q to its nearest neighbor xq nn   X 2. Compute Dq mean = Ex[D(q, x)] as the average distance of query q from all database points x   X Dq mean Dq 3. Relative Contrast of a given query C q , which is a measure of how separable the r = min nn is from an average point in the database x query s nearest neighbor xq 4. Compute an expectation over all queries for Relative Contrast over the entire database as Cr = Eq[Dq Eq[Dq mean] min] It is evident that Cr captures the difficulty of Nearest Neighbor Search in database X, as a Cr   1 indicates that for an average query, its nearest neighbor is almost equidistant from a random point in the database.', chunk_index=191, num_tokens=183, metadata={'section_titles': [], 'pages': []}), ResponseChunk(id='chunk_b04da71b-b8b7-422d-87f9-c06be7048464', content='As demonstrated in Figure 14, MRs have higher Rc than RR Embeddings for an Exact Search on ImageNet-1K for all d   16. This result implies that a portion of MR s improvement over RR for 1-NN retrieval across all embedding dimensionalities d [31] is due to a higher average separability of the MR 1-NN from a random database point.', chunk_index=192, num_tokens=81, metadata={'section_titles': ['H.3 Generality across Encoders', 'Representation Size', 'H.2 Relative Contrast'], 'pages': [24]}), ResponseChunk(id='chunk_ee840e84-5364-4004-85d7-7b24e208771e', content='H.3 Generality across Encoders We perform an ablation over the representation function   : X   Rd learnt via a backbone neural network (primarily ResNet50 in this work), as detailed in Section 3. We also train MRL models [31]  M R(d) on ResNet18/34/101 [19] that are as accurate as their independently trained RR baseline models  RR(d), where d is the default max representation size of each architecture. We also train MRL with a ConvNeXt-Tiny backbone with [d] = {48, 96, 192, 384, 786}.', chunk_index=193, num_tokens=131, metadata={'section_titles': ['H.3 Generality across Encoders', 'Representation Size', 'H.2 Relative Contrast'], 'pages': [24, 25]}), ResponseChunk(id='chunk_0ac5fbc3-5734-42c2-b1f1-c625aaaa0f40', content='MR-768 has a top-1 accuracy of 79.45% compared to independently trained publicly available RR-768 baseline with top-1 accuracy 82.1% (Code and RR model available on the official repo6).', chunk_index=194, num_tokens=47, metadata={'section_titles': ['H.3 Generality across Encoders'], 'pages': [25]}), ResponseChunk(id='chunk_1f33d66f-af3d-4407-b1c7-1d3fb67eeae2', content='We note that this training had no hyperparameter tuning whatsoever, and this gap can be closed with additional model training effort. We then compare clustering the MRs via IVF-MR with k = 2048, np = 1 on ImageNet-1K to Exact-MR, which is shown in Figure 15. IVF-MR shows similar trends across backbones compared to Exact-MR, i.e. a maximum top-1 accuracy drop of   1.6% for a single search probe.', chunk_index=195, num_tokens=105, metadata={'section_titles': ['H.3 Generality across Encoders'], 'pages': [25]}), ResponseChunk(id='chunk_69a1aaed-4bf3-41a5-a237-73d791c66de1', content='This suggests the clustering capabilities of MR extend beyond an inductive bias of  M R(d)   ResNet50, though we leave a detailed exploration for future work. 6https://github.com/facebookresearch/ConvNeXt', chunk_index=196, num_tokens=47, metadata={'section_titles': ['H.3 Generality across Encoders'], 'pages': [25]}), ResponseChunk(id='chunk_c2da876d-85a4-4c04-b880-3cb5339af15e', content=\"\\nThe table compares the performance of two indexing methods, RR-2048 and AdANNS, in the context of composite indices. Key metrics include the PQ budget, top-1 accuracy, and precision at a specified threshold. RR-2048 utilizes a PQ budget of 32 bytes, while AdANNS is more efficient with a budget of 16 bytes. In terms of top-1 accuracy, AdANNS outperforms RR-2048, achieving 64.70% compared to 62.46%. Additionally, AdANNS demonstrates superior precision at 40%, with a score of 68.25% versus RR-2048's 65.65%. Overall, AdANNS shows enhanced efficiency and accuracy in composite index applications, making it a preferable choice for optimized performance.\", chunk_index=1, num_tokens=162, metadata={'section_titles': ['4.3 AdANNS for Composite Indices'], 'pages': [8], 'text_as_markdown': '\\n\\n|  | RR\\\\-2048 | AdANNS |\\n| --- | --- | --- |\\n| PQ Budget (Bytes) | 32 | 16 |\\n| Top\\\\-1 Accuracy (%) mAP@10 (%) 62\\\\.46 64\\\\.70 | | |\\n| Precision @40 (%) | 65\\\\.65 | 68\\\\.25 |\\n\\n', 'caption': '', 'summary': \"The table compares the performance of two indexing methods, RR-2048 and AdANNS, in the context of composite indices. Key metrics include the PQ budget, top-1 accuracy, and precision at a specified threshold. RR-2048 utilizes a PQ budget of 32 bytes, while AdANNS is more efficient with a budget of 16 bytes. In terms of top-1 accuracy, AdANNS outperforms RR-2048, achieving 64.70% compared to 62.46%. Additionally, AdANNS demonstrates superior precision at 40%, with a score of 68.25% versus RR-2048's 65.65%. Overall, AdANNS shows enhanced efficiency and accuracy in composite index applications, making it a preferable choice for optimized performance.\"}), ResponseChunk(id='chunk_4302289f-bcf6-46fc-a5a0-2deb8590a832', content='\\nThe table outlines various methods for retrieving k-nearest neighbors during inference, highlighting their respective retrieval formulas. \\n\\n1. **IVF-RR**: Utilizes a formula involving the arg min function to minimize the distance between query vector \\\\( q \\\\) and reference points, denoted as \\\\( o^* \\\\).\\n\\n2. **AdANNS-IVF and MG-IVF-RR**: These methods also employ an arg min approach, focusing on minimizing the distance between the query \\\\( q \\\\) and the reference points \\\\( OMR \\\\) and \\\\( OR \\\\), respectively. The retrieval process is conditioned on specific constraints.\\n\\n3. **AdANNS-IVF-D and IVFOPQ**: These methods introduce additional complexity with multiple arg min calculations, emphasizing the minimization of distances across various dimensions and conditions, including the use of specific indices and weights.\\n\\nOverall, the table presents a comparative view of advanced retrieval techniques, emphasizing the importance of distance minimization in efficient k-nearest neighbor searches.', chunk_index=2, num_tokens=202, metadata={'section_titles': ['return k_nearest_neighbors'], 'pages': [15], 'text_as_markdown': '\\n\\n| Method \\\\| | Retrieval Formula during Inference |\\n| --- | --- |\\n| IVF\\\\-RR | . d d . d pk arg minjecy,,, \\\\| (g)  oR (w,)\\\\|\\\\|, st. h(q) \\\\= arg min, \\\\|\\\\|o\\\\*\\\\* (q)  \\\\\\\\ mn |\\n| AdANNS\\\\-IVF \\\\| MG\\\\-IVF\\\\-RR | arg minjecy,, \\\\|OMR (q)  OR) (x5\\\\)\\\\|], st. h(q) \\\\= arg min), \\\\|]oM) (q) \\\\~ Hi Ride) \\\\| Ride) arg minjec,,,, \\\\|O\\\\&)(q)  dR) (a)\\\\|\\\\|, st. h(g) \\\\= arg miny, \\\\|\\\\|OR) (q)  |\\n| AdANNS\\\\-IVF\\\\-D IVFOPQ | i d jl d jl i MR (d d) jl arg minjec, ., \\\\[OM (@)\\\\[L \\\\= d]  OM (\\\\#;)\\\\[1 : d]ll, st. h(q) \\\\= argmin,, \\\\|\\\\|\" (@)\\\\[I : 3  wR: dll arg minjecy,,) \\\\[]G2 (g)  GPR (a), 8\\\\. h(q) \\\\= arg min), \\\\|\\\\|9(q) \\\\~ pall |\\n\\n', 'caption': '', 'summary': 'The table outlines various methods for retrieving k-nearest neighbors during inference, highlighting their respective retrieval formulas. \\n\\n1. **IVF-RR**: Utilizes a formula involving the arg min function to minimize the distance between query vector \\\\( q \\\\) and reference points, denoted as \\\\( o^* \\\\).\\n\\n2. **AdANNS-IVF and MG-IVF-RR**: These methods also employ an arg min approach, focusing on minimizing the distance between the query \\\\( q \\\\) and the reference points \\\\( OMR \\\\) and \\\\( OR \\\\), respectively. The retrieval process is conditioned on specific constraints.\\n\\n3. **AdANNS-IVF-D and IVFOPQ**: These methods introduce additional complexity with multiple arg min calculations, emphasizing the minimization of distances across various dimensions and conditions, including the use of specific indices and weights.\\n\\nOverall, the table presents a comparative view of advanced retrieval techniques, emphasizing the importance of distance minimization in efficient k-nearest neighbor searches.'}), ResponseChunk(id='chunk_6cf9fabb-942a-4172-b329-7b98f4e00555', content='Table 3: Comparison of AdANNS-IVF and Rigid-IVF wall-clock inference times for ImageNet-1K retrieval. AdANNS-IVF has up to   1.5% gain over Rigid-IVF for a fixed search latency per query.\\nTable 3 compares the wall-clock inference times of AdANNS-IVF and Rigid-IVF for ImageNet-1K retrieval, highlighting the performance of each method in terms of search latency per query. AdANNS-IVF demonstrates a maximum gain of 1.5% in Top-1 accuracy over Rigid-IVF while maintaining comparable search latencies. The search latency for AdANNS-IVF ranges from 0.03 ms to 5.57 ms, with Top-1 accuracy values of 70.02% to 70.60%. In contrast, Rigid-IVF shows search latencies from 0.02 ms to 5.67 ms, with Top-1 accuracy values between 68.51% and 70.13%. Notably, both methods exhibit similar latencies at higher accuracy levels, with AdANNS-IVF slightly outperforming Rigid-IVF in accuracy at fixed latencies. This analysis underscores the efficiency and effectiveness of AdANNS-IVF in image retrieval tasks, making it a competitive choice for applications requiring rapid and accurate search capabilities.', chunk_index=3, num_tokens=293, metadata={'section_titles': ['B Training and Compute Costs'], 'pages': [15], 'text_as_markdown': '\\n\\n|  | AdANNS\\\\-IVF |  | Rigid\\\\-IVF |\\n| --- | --- | --- | --- |\\n| Top\\\\-1 | Search Latency/Query (ms) | \\\\| Top\\\\-1 | Search Latency/Query (ms) |\\n| 70\\\\.02 | 0\\\\.03 | 68\\\\.51 | 0\\\\.02 |\\n| 70\\\\.08 | 0\\\\.06 | 68\\\\.54 | 0\\\\.05 |\\n| 70\\\\.19 | 0\\\\.06 | 68\\\\.74 | 0\\\\.08 |\\n| 70\\\\.36 | 0\\\\.88 | 69\\\\.20 | 0\\\\.86 |\\n| 70\\\\.60 | 5\\\\.57 | 70\\\\.13 | 5\\\\.67 |\\n\\n', 'caption': 'Table 3: Comparison of AdANNS-IVF and Rigid-IVF wall-clock inference times for ImageNet-1K retrieval. AdANNS-IVF has up to   1.5% gain over Rigid-IVF for a fixed search latency per query.', 'summary': 'Table 3 compares the wall-clock inference times of AdANNS-IVF and Rigid-IVF for ImageNet-1K retrieval, highlighting the performance of each method in terms of search latency per query. AdANNS-IVF demonstrates a maximum gain of 1.5% in Top-1 accuracy over Rigid-IVF while maintaining comparable search latencies. The search latency for AdANNS-IVF ranges from 0.03 ms to 5.57 ms, with Top-1 accuracy values of 70.02% to 70.60%. In contrast, Rigid-IVF shows search latencies from 0.02 ms to 5.67 ms, with Top-1 accuracy values between 68.51% and 70.13%. Notably, both methods exhibit similar latencies at higher accuracy levels, with AdANNS-IVF slightly outperforming Rigid-IVF in accuracy at fixed latencies. This analysis underscores the efficiency and effectiveness of AdANNS-IVF in image retrieval tasks, making it a competitive choice for applications requiring rapid and accurate search capabilities.'}), ResponseChunk(id='chunk_89a26a5a-40db-45d8-bdea-490d70f54ffb', content='\\nThe table presents performance metrics for various configurations of IVF (Inverted File) and OPQ (Optimized Product Quantization) on the Natural Questions dataset. Key metrics include Top-l1 accuracy, mean Average Precision at 10 (mAP@10), and Precision at 100 (P@100) across different dimensions (d) and quantization parameters (m). \\n\\nConfigurations vary from 8 to 2048 dimensions, with PQ and OPQ showing distinct performance trends. Notably, the highest Top-l1 accuracy of 70.61 is achieved with a 128-dimensional OPQ configuration. The mAP@10 and P@100 metrics also peak at this configuration, indicating strong retrieval performance. \\n\\nAs dimensions increase, performance generally improves, particularly with OPQ, which consistently outperforms PQ across most configurations. The results suggest that higher dimensionality and optimized quantization techniques enhance retrieval effectiveness in natural language processing tasks. This analysis is crucial for optimizing search embeddings in machine learning applications.', chunk_index=4, num_tokens=203, metadata={'section_titles': ['(b) IVF + OPQ on Natural Questions'], 'pages': [18], 'text_as_markdown': '\\n\\n| Config | | PQ | | | OPQ | | |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n| d | m \\\\| | Top\\\\-l1 | mAP@10 | P@100 | \\\\| Top\\\\-l1 | mAP@10 | P@100 |\\n|  | 8 \\\\| | 62\\\\.18 | 56\\\\.71 | 61\\\\.23 | \\\\| 62\\\\.22 | 56\\\\.70 | 61\\\\.23 |\\n| 16 | 8 \\\\| 6 \\\\| | 67\\\\.91 67\\\\.85 | 62\\\\.85 62\\\\.95 | 67\\\\.21 67\\\\.21 | \\\\| 67\\\\.88 \\\\| 67\\\\.96 | 62\\\\.96 62\\\\.94 | 67\\\\.21 67\\\\.21 |\\n| 32 | 8 \\\\| | 68\\\\.80 | 63\\\\.62 | 67\\\\.86 | \\\\| 68\\\\.91 | 63\\\\.63 | 67\\\\.86 |\\n| 6 \\\\| 32 \\\\| | 69\\\\.57 69\\\\.44 | 64\\\\.22 64\\\\.20 | 68\\\\.12 68\\\\.12 | \\\\| 69\\\\.47 \\\\| 69\\\\.47 | 64\\\\.20 64\\\\.23 | 68\\\\.12 68\\\\.12 |\\n| 64 | 8 \\\\| 6 \\\\| 32 \\\\| 64 \\\\| | 68\\\\.39 69\\\\.77 70\\\\.13 70\\\\.12 | 63\\\\.40 64\\\\.43 64\\\\.67 64\\\\.69 | 67\\\\.47 68\\\\.25 68\\\\.38 68\\\\.42 | \\\\| 68\\\\.38 \\\\| 69\\\\.95 \\\\| 70\\\\.05 \\\\| 70\\\\.18 | 63\\\\.42 64\\\\.55 64\\\\.65 64\\\\.70 | 67\\\\.60 68\\\\.38 68\\\\.38 68\\\\.38 |\\n| 128 | 8 \\\\| 6 \\\\| 32 \\\\| 64 \\\\| | 67\\\\.27 69\\\\.51 70\\\\.27 70\\\\.61 | 61\\\\.99 64\\\\.32 64\\\\.72 64\\\\.93 | 65\\\\.78 68\\\\.12 68\\\\.51 68\\\\.49 | \\\\| 68\\\\.40 \\\\| 69\\\\.78 \\\\| 70\\\\.60 \\\\| 70\\\\.65 | 63\\\\.11 64\\\\.56 64\\\\.97 64\\\\.98 | 67\\\\.34 68\\\\.38 68\\\\.51 68\\\\.51 |\\n|  |  |  |  |  |  |  |\\n| 256 | 8 \\\\| | 66\\\\.06 | 60\\\\.44 | 64\\\\.09 | \\\\| 67\\\\.90 | 62\\\\.69 | 66\\\\.95 |\\n| 6 \\\\| | 68\\\\.56 | 63\\\\.33 | 66\\\\.95 | \\\\| 69\\\\.92 | 64\\\\.71 | 68\\\\.51 |\\n| 32 \\\\| | 70\\\\.08 | 64\\\\.83 | 68\\\\.38 | \\\\| 70\\\\.59 | 65\\\\.15 | 68\\\\.64 |\\n| 64 \\\\| | 70\\\\.48 | 64\\\\.98 | 68\\\\.55 | \\\\| 70\\\\.69 | 65\\\\.09 | 68\\\\.64 |\\n| 512 | 8 \\\\| | 65\\\\.09 | 59\\\\.03 | 62\\\\.53 | \\\\| 67\\\\.51 | 62\\\\.12 | 66\\\\.56 |\\n| 6 \\\\| | 67\\\\.68 | 62\\\\.11 | 65\\\\.39 | \\\\| 69\\\\.67 | 64\\\\.53 | 68\\\\.38 |\\n| 32 \\\\| | 69\\\\.51 | 64\\\\.01 | 67\\\\.34 | \\\\| 70\\\\.44 | 65\\\\.11 | 68\\\\.64 |\\n| 64 \\\\| | 70\\\\.53 | 65\\\\.02 | 68\\\\.52 | \\\\| 70\\\\.72 | 65\\\\.17 | 68\\\\.64 |\\n| 1024 | 8 \\\\| | 64\\\\.58 | 58\\\\.26 | 61\\\\.75 | \\\\| 67\\\\.26 | 62\\\\.07 | 66\\\\.56 |\\n| 6 \\\\| | 66\\\\.84 | 61\\\\.07 | 64\\\\.09 | \\\\| 69\\\\.34 | 64\\\\.23 | 68\\\\.12 |\\n| 32 \\\\| | 68\\\\.71 | 62\\\\.92 | 66\\\\.04 | \\\\| 70\\\\.43 | 65\\\\.03 | 68\\\\.64 |\\n| 64 \\\\| | 69\\\\.88 | 64\\\\.35 | 67\\\\.68 | \\\\| 70\\\\.81 | 65\\\\.19 | 68\\\\.64 |\\n| 2048 | 8 \\\\| | 62\\\\.19 | 56\\\\.11 | 59\\\\.80 | \\\\| 66\\\\.89 | 61\\\\.69 | 66\\\\.30 |\\n| 6 \\\\| | 65\\\\.99 | 60\\\\.27 | 63\\\\.18 | \\\\| 69\\\\.25 | 64\\\\.09 | 67\\\\.99 |\\n| 32 \\\\| | 67\\\\.99 | 62\\\\.04 | 64\\\\.74 | \\\\| 70\\\\.39 | 64\\\\.97 | 68\\\\.51 |\\n\\n', 'caption': '', 'summary': 'The table presents performance metrics for various configurations of IVF (Inverted File) and OPQ (Optimized Product Quantization) on the Natural Questions dataset. Key metrics include Top-l1 accuracy, mean Average Precision at 10 (mAP@10), and Precision at 100 (P@100) across different dimensions (d) and quantization parameters (m). \\n\\nConfigurations vary from 8 to 2048 dimensions, with PQ and OPQ showing distinct performance trends. Notably, the highest Top-l1 accuracy of 70.61 is achieved with a 128-dimensional OPQ configuration. The mAP@10 and P@100 metrics also peak at this configuration, indicating strong retrieval performance. \\n\\nAs dimensions increase, performance generally improves, particularly with OPQ, which consistently outperforms PQ across most configurations. The results suggest that higher dimensionality and optimized quantization techniques enhance retrieval effectiveness in natural language processing tasks. This analysis is crucial for optimizing search embeddings in machine learning applications.'}), ResponseChunk(id='chunk_3115e149-7e44-464d-aefd-37e510f34736', content='\\nThe table presents robustness metrics for various algorithms in approximate nearest neighbor search, specifically comparing AdANNS-IVF-D, IVF-MR, Exact-MR, IVF-RR, and Exact-RR across different dimensions (d). The results indicate performance variations as the dimensionality increases from 8 to 2048. \\n\\nFor lower dimensions (8 and 16), AdANNS-IVF-D shows competitive performance, peaking at 57.32 for d=32. As dimensions increase, Exact-MR consistently outperforms other methods, achieving a maximum score of 59.63 at d=2048. IVF-MR and Exact-RR also demonstrate strong performance, particularly at higher dimensions, while AdANNS-IVF-D maintains relatively stable results. \\n\\nOverall, Exact-MR emerges as the most robust method across all dimensions, while AdANNS-IVF-D provides a viable alternative, especially in lower dimensions. The data highlights the importance of algorithm selection based on dimensionality for optimal performance in nearest neighbor searches.', chunk_index=5, num_tokens=215, metadata={'section_titles': ['E.1 Robustness'], 'pages': [20], 'text_as_markdown': '\\n\\n| d | AdANNS\\\\-IVF\\\\-D | \\\\| IVF\\\\-MR\\\\_ | Exact\\\\-MR | \\\\| IVF\\\\-RR\\\\_ | Exact\\\\-RR |\\n| --- | --- | --- | --- | --- | --- |\\n| 8 16 | 53\\\\.51 57,32 | 50\\\\.44 56\\\\.35 | 50\\\\.41 56\\\\.64 | 49\\\\.03 55\\\\.04 | 48\\\\.79 55\\\\.08 |\\n| 32 | 57\\\\.32 | 57\\\\.64 | 57\\\\.96 | 56\\\\.06 | 56\\\\.69 |\\n| 64 | 57\\\\.85 | 58\\\\.01 | 58\\\\.94 | 56\\\\.84 | 57\\\\.37 |\\n| 128 | 58\\\\.02 | 58\\\\.09 | 59\\\\.13 | 56\\\\.14 | 57\\\\.17 |\\n| 256 | 58\\\\.01 | 58\\\\.33 | 59\\\\.18 | 55\\\\.60 | 57\\\\.09 |\\n| 512 | 58\\\\.03 | 57\\\\.84 | 59\\\\.40 | 55\\\\.46 | 57\\\\.12 |\\n| 1024 | 57\\\\.66 | 57\\\\.58 | 59\\\\.11 | 54\\\\.80 | 57\\\\.53 |\\n| 2048 | 58\\\\.04 | 58\\\\.04 | 59\\\\.63 | 56\\\\.17 | 57\\\\.84 |\\n\\n', 'caption': '', 'summary': 'The table presents robustness metrics for various algorithms in approximate nearest neighbor search, specifically comparing AdANNS-IVF-D, IVF-MR, Exact-MR, IVF-RR, and Exact-RR across different dimensions (d). The results indicate performance variations as the dimensionality increases from 8 to 2048. \\n\\nFor lower dimensions (8 and 16), AdANNS-IVF-D shows competitive performance, peaking at 57.32 for d=32. As dimensions increase, Exact-MR consistently outperforms other methods, achieving a maximum score of 59.63 at d=2048. IVF-MR and Exact-RR also demonstrate strong performance, particularly at higher dimensions, while AdANNS-IVF-D maintains relatively stable results. \\n\\nOverall, Exact-MR emerges as the most robust method across all dimensions, while AdANNS-IVF-D provides a viable alternative, especially in lower dimensions. The data highlights the importance of algorithm selection based on dimensionality for optimal performance in nearest neighbor searches.'}), ResponseChunk(id='chunk_06e951f3-e716-4f24-9d15-784078565302', content='\\nThe table presents performance metrics for the F AdANNS-DiskANN algorithm across various dimensions (d) and parameter settings (M). The values indicate the efficiency of the algorithm in terms of processing time or resource usage. \\n\\nFor dimension 8, only M=8 is available with a value of 495. As dimensions increase, performance metrics vary significantly. At dimension 16, M=8 shows 555, while M=16 improves slightly to 571. Dimension 32 exhibits a peak performance at M=8 with 669, while higher M values show diminishing returns. \\n\\nDimension 64 maintains relatively stable performance across M values, ranging from 843 to 864. Notably, dimension 128 shows a significant jump at M=64 with a value of 2011, indicating a potential trade-off in efficiency. \\n\\nAs dimensions increase to 256 and beyond, values escalate, with dimension 2048 reaching a maximum of 9907 at M=8. Overall, the data suggests that while increasing M generally improves performance, the benefits may plateau or vary at higher dimensions.', chunk_index=6, num_tokens=221, metadata={'section_titles': ['F AdANNS-DiskANN'], 'pages': [22], 'text_as_markdown': '\\n\\n| d | M\\\\=8 | M\\\\=16 | M\\\\=32 | M\\\\=48 | M\\\\=64 |\\n| --- | --- | --- | --- | --- | --- |\\n| 8 | 495 | \\\\- | \\\\- | \\\\- | \\\\- |\\n| 16 | 555 | 571 | \\\\- | \\\\- | \\\\- |\\n| 32 | 669 | 655 | 653 | \\\\- | \\\\- |\\n| 64 | 864 | 855 | 843 | 844 | 848 |\\n| 128 | ) 1182 | \\\\= 1311 | 1156 | 1161 | 2011 |\\n| 256 | \\\\| 1923 | 1779 | 1744 | 2849 | 1818 |\\n| 512 | \\\\| 2802 | 3272 | 3423 | 2780 | \\\\= 3171 |\\n| 1024 | \\\\| 5127 | 5456 | 5724 | 4683 | 5087 |\\n| 2048 | \\\\| 9907 | 9833 | 10205 | 10183 | 9329 |\\n\\n', 'caption': '', 'summary': 'The table presents performance metrics for the F AdANNS-DiskANN algorithm across various dimensions (d) and parameter settings (M). The values indicate the efficiency of the algorithm in terms of processing time or resource usage. \\n\\nFor dimension 8, only M=8 is available with a value of 495. As dimensions increase, performance metrics vary significantly. At dimension 16, M=8 shows 555, while M=16 improves slightly to 571. Dimension 32 exhibits a peak performance at M=8 with 669, while higher M values show diminishing returns. \\n\\nDimension 64 maintains relatively stable performance across M values, ranging from 843 to 864. Notably, dimension 128 shows a significant jump at M=64 with a value of 2011, indicating a potential trade-off in efficiency. \\n\\nAs dimensions increase to 256 and beyond, values escalate, with dimension 2048 reaching a maximum of 9907 at M=8. Overall, the data suggests that while increasing M generally improves performance, the benefits may plateau or vary at higher dimensions.'})]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
